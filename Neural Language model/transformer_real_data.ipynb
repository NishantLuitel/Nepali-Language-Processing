{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8527adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a6ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    if torch.cuda.device_count() >= i+1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "device = try_gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba983f7",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18adf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "with open('../data/ne_dedup.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    text = re.sub(r'\\s*[\\u0964]\\s*', r'\\u0020\\u0964\\u0020', text)\n",
    "    text = re.sub(r'\\s*[\\u003f]\\s*', r'\\u0020\\u003f\\u0020', text)\n",
    "    text = re.sub(r'\\s*[\\u002c]\\s*', r'\\u0020\\u003f\\u0020', text)\n",
    "    text = re.sub(r'\\s*\\n\\s*','\\n', text)\n",
    "    #text = re.sub(r'\\s*[\\u0966-\\u0976]+\\s*','\\u0020[\\u0966-\\u0976]\\u0020', text)\n",
    "    #text = re.sub(r'\\s+?\\s+', r'\\u0020?\\u0020', text)\n",
    "    text = re.sub(r'[^\\u0900-\\u097F,?\\s+]','', text)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2ec7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?नमस्ते,'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = re.sub(r'[^\\u0900-\\u097F,?]','','?नमस्तेnishant,')\n",
    "#c = re.sub(r'\\s*[\\u002c]\\s*','\\u0020\\u002c\\u0020','?नमस्तेnishant,')\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8492ecaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x966'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(ord('०'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b02d3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341961"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b4a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = text.split('\\n')[:30000]\n",
    "test_iter = text.split('\\n')[30000:35000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f050e4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['बर्दिबास नगरपालिकाको तेस्रो नगर परिषदबाट पारित आव२०७३ । ७४ को संशोधित र २०७४ । ७५ को प्रस्तावित नीति ? कार्यक्रम तथा बजेट',\n",
       " 'अार्थिक वर्ष २०७५७६ काे नदिजन्य पदार्थकाे उत्खनन् गरी बिक्रि वितरण तथा अान्तरिक निकासी गर्ने कार्यकाे बाेलपत्र सम्बन्धी सुचना',\n",
       " 'सक्षार सप्तरी अभियानमा सप्तरीबासी सम्पूर्ण सरोकारवालाहरुको सहयोग र सहभागिताकाो लागि अनुराोध छ ।  सामुदायिक अध्ययन केन्द्रहरूको नविकरण सम्बन्धमा । ',\n",
       " 'काठमाडौं ? १२ कातिक । राष्ट्रपति विद्यादेवी भण्डारी मित्रराष्ट्र कतारको चार दिवसीय औपचारिक भ्रमणमा आज त्यसतर्फ प्रस्थान गरेकी छन् । राष्ट्रपति विद्यादेवी भण्डारी कतारका अमिर शेख हमाद बीन खालिदा अल थानीको मैत्रीपूर्ण निमन्त्रणामा चार दिवसीय औपचारिक',\n",
       " 'काठमाडौँ ? २६ कात्तिक । सरकारले सङ्घ ? प्रदेश र स्थानीय तहमा कर्मचारी समायोजन गर्नका लागि कर्मचारी समायोजन अध्यादेश२०७५ ल्याउने तयारी गरेको छ । सरकारले यसअघि ल्याएको',\n",
       " 'काठमाडौं ? २६ कातिक । महानायक राजेश हमाल अहिले चलचित्र क्षेत्रमा पातलिए पनि उनको सिने जगतमा नामै काफी छ । कुनै समय बलिउड सुपरस्टार अमिताभ वच्चनसँग',\n",
       " 'काठमाडौं ? २६ कातिक । यमनको प्रमुख शहर होडेडामा सरकार समर्थक र विद्रोहीबीच भएको पछिल्लो युद्धमा एक सय ४९ जनाको मृत्यु भएको चिकित्सक र सैनिक स्रोतले',\n",
       " 'काठमाडौं ? २६ कातिक । निजी क्षेत्रबाट निर्माण भएको पहिलो चलचित्र माइतीघरका छाँयाकार गेहेन्द्रप्रसाद धिमालको ७२',\n",
       " 'नेपाल कम्युनिष्ट पार्टी नेकपाको एकीकरण प्रक्रिया जारी छ । तर ? ३ जेठ २०७५ मा पार्टी एकताको घोषणा हुदै गर्दा जुन कार्यतालिका बनाइएको थियो । त्यो',\n",
       " 'सुर्खेत ? १ कात्तिक । गत शुक्रबार आठबीस नगरपालिका१ डाव दैलेखका कर्णबहादुर रावत र उनकी श्रीमतीलाई चिया चाउचाउ नास्ता खुवाउन भ्याइ नभ्याइ थियो । खासै यसअघि दुईचार स्थानीय ग्राहक आउने त्यस चिया पसलमा उक्त दिनको भीडले भने रावत दम्पती निकै दङ्ग थिए । त्यहाँ कर्णाली प्रदेश सरकार ? आन्तरिक तथा विदेशी पाहुना पुगेका थिए । आजभोलि रावतझैँ',\n",
       " 'काठमाडौं ? २२ कात्तिक । सशस्त्र प्रहरी बलको मुख्यालयमा मंगलबार कुकुर तिहारका दिन देउसीभैली कार्यक्रम गरिएको थियो । सशस्त्र प्रहरी परिवार महिला संघको ब्यानरमा गरिएको उक्त दिनको देउसी भैली कार्यक्रम रमाइलोका लागि र स्वेच्छिक रुपमा दान दिने उद्देश्यले गरिएको नभई जबर्जस्ती असुलीका लागि गरिएको सन्देश गएको',\n",
       " 'काठमाडौं ? २६ कातिक । नेपाल स्टक एक्सचेञ्जले कारोवार रकम तलमाथि परेको भन्दै आएको गुनासो हल्ला मात्रै भएको जनाएको छ । कारोवार रकमलगायतका विषयमा अन्योलता बढेपछि केही लगानीकर्ताले एउटा दलालको देखिरहेको मूल्य र अर्को दलालमा देखिरहेको',\n",
       " 'काठमाडौं ? २२ कात्तिक । सशस्त्र प्रहरी बलको मुख्यालयमा मंगलबार कुकुर तिहारका दिन देउसीभैली कार्यक्रम गरिएको थियो । सशस्त्र प्रहरी परिवार महिला संघको ब्यानरमा गरिएको उक्त दिनको देउसी भैली कार्यक्रम रमाइलोका लागि र स्वेच्छिक रुपमा दान दिने उद्देश्यले गरिएको नभई जबर्जस्ती असुलीका लागि गरिएको सन्देश गएको छ । पदको आडमा रकम उठाउन गरिएको थियो भन्ने कुरा',\n",
       " 'पेरिस ? २५ कार्तिक । पहिलो विश्वयुद्ध समाप्तिको एक सय वर्ष अवसर पारेर फ्रान्सको पेरिसमा विशेष कार्यक्रम आयोजना गरिएको छ । प्रथम विश्वयुद्ध अन्त्य भएको सम्झौतामा हस्ताक्षर गरिएको एकसय वर्ष पुगेको सम्झनामा पेरिसमा आयोजित एउटा समारोहमा विश्वका',\n",
       " 'हङकङ  चीनले संसारलाई चकित तुल्याउने गरी छोटो समयमै विश्वकै लामो समुन्द्री पुलको निर्माण कार्य सम्पन्न गरेको छ । हङकङमकाउचुहाई जोड्न ७ किलोमिटर सामुद्रिक सुरङसहित ५५ किलोमिटर लामो सामुद्रिक पुल चीनले निर्माण गरेर उद्घाटन गरेको हो । तस्बिरहरु ऐजेन्सी चिनियाँ सहर जुहाईको हुँदै मकाउ जोड्ने',\n",
       " 'काठमाडौं ? १३ कात्तिक । नेपाल एयर होस्टेस एकाडमीद्धारा सञ्चालित एयर होस्टेस तालिम लिएका विद्यार्थी दीक्षित भएका छन् । मकवानपुरमा एक कार्यक्रमको विच मंगलवार १८ जना एयरहोस्टेस दीक्षित भए । तालिम लिएका युवायुवतीहरुलाई उक्त एयरहोस्टेस एकाडमीका शिक्षकहरु नितामनि',\n",
       " 'दमौली । तनँहुको पर्यटकीय नगरी बन्दीपुरमा कृत्रिम ताल बनाइने भएको छ । पहाडकी रानी नामले परिचित बन्दीपुरको पर्यटन प्रवद्र्धन गर्न मानवनिर्मित ताल बनाउन लागिएको हो । सोमबार प्रसिद्ध अर्थशास्त्री कार्लो कोटेरेलीलाई नयाँ सरकारको नेतृत्व गर्न आग्रह गर्दै तत्काल यहाँको मन्त्रीमण्डल विस्तार गर्ने जनादेश प्रदान गरेका छन् राष्ट्रपतिको कार्यालयबाट जारी एक विज्ञप्तिमा जनाइएको छ । प्रधानमन्त्रीमा आफ्नो नाम प्रस्तावित भएपछि कोटेरेलीले देशवासीलाई सम्बोधन गर्दै भने ? यदि मेरो नेतृत्वको सरकारले संसदबाट विश्वासको मत पाएमा सार्वजनिक वित्त सुधारका लागि सावधानीपूर्वक नेतृत्व प्रदान गर्नेछ । ',\n",
       " 'उहाँले कुशल नेतृत्व प्रदान गर्दै आगामी वर्ष सन् २०१९ को बजेटलाई अनुमोदन गराउँदै इटालीलाई यूरो समूहमा नै रहने सुनिश्चितता गर्ने पनि सम्बोधनमा बताए । लण्डन । उपसभामुख डा शिवमाया तुम्बाहाङ्फेले लण्डनमा सम्पन्न विश्वभरका महिला सांसदहरुको एक अन्तर्राष्ट्रिय सम्मेलनलाई सम्बोधन गरेकी छन् । आइतबार लण्डनको फेल्थाममा १६ औँ राष्ट्रिय विभुती ? समाज सुधारक महागुरु फल्गुनन्दको १३४ औँ जन्म जयन्ती मनाइएको छ । हनोई । भियतनामले यसै वर्षको अन्त्यदेखि लागू हुने बहुचर्चित प्रशान्त व्यापार सम्झौतालाई अनुमोदन गरेको छ । अमेरिकी राष्ट्रपति डोनाल्ड ट्रम्पले यो सम्झौताबाट अलग हुने घोषणा गरे पनि सम्झौता अनुमोदन गर्ने',\n",
       " 'काठमाडौँ । उमेर पुगेका हरेक व्यक्तिलाई यौनको चाहाना हुनु स्वाभाविक नै मानिन्छ तर यदि कोहि महिला वा पुरुषलाई यौन चाहानामा गडबढी हुनु पनि सामान्य हुँदै गएको छ । धेरै विद्यार्थीहरुको मनमा ग्य्राजुएट गरेपछि पढु कि नपढु भन्ने दुविधा हुन्छ । यो विषय पढौं कि त्यो विषय पढौं भन्ने प्रश्न पनि धेरै ग्य्राजुएटहरुको दिमागमा खेलिरहेको हुन्छ । यो फाइल   बाट हो र अन्य परियोजनाहरू द्वारा पनि प्रयोग गर्न सकिन्छ । त्यहाँ नेर यसको फाइल विवरण पृष्ठमा रहेको विवरण तल दिइएको छ । उजुरी कर्ताहरुलाई आइतबार सिंहदरबार वोलाइएको छ । उजुरीका लागि दिइएको १० दिन मध्ये वुधवार अन्तिम समयसम्म मिश्रलाई अनुमोदन गर्न नहुने दाबीसहित चारवटा उजुरी दर्ता भएको थियो । मिश्र बिरुद्ध परेका चार वटा उजुरी मध्ये एक जना उजुरीकर्ताले उजुरी फिर्ता लिएको समितीले जनाएको छ । मिश्रविरुद्ध ऐनसेलको मुद्दामा राज्यलाई करोडौं घाटा हुने गरी पैसा विदेश लैजान अन्तरिम आदेश दिएको ? सूर्य नेपालको मुद्दामा कर छली गर्न आदेश मार्फत सहयोग गरेको ? हाँडीगाउँको सार्वजनिक जग्गा व्यक्तिका नाउमा नामसारी गर्न बाटो खोलिदिएको ? विभिन्न मुद्दामा पैसा लिएर अभियुक्तलाई छाडेको लगायतका उजुरी परेका छन् । प्रधानन्यायाधीशमा प्रस्तावित दीपकराज जोशीलाई संसदीय सुनुवाई विशेष समितिले अस्विकृत गरेपछि संवैधानिक परिषद्ले सर्वोच्च अदालतका वरिष्ठ न्यायाधीश मिश्रलाई प्रधानन्यायाधीशमा सिफारिस गरेको हो । यसैबीच समितिको बैठकले संवैधानिक परिषद्बाट अख्तियार दुरुपयोग अनुसन्धान आयोगको प्रमुख आयुक्तमा सिफारिस भएका नविनकुमार घिमिरेमाथि सार्वजनिक उजुरीको आव्हान गरिएको छ । आज शनिवारदेखि लागु हुने गरी १० दिनभित्र उजुरी दर्ता गराउन सार्वजनिक सूचना प्रकाशित गर्ने निर्णय वैठकले गरेको छ । भदौ २० गते बुधबार वसेको संवैधानिक परिषदको बैठकले कार्यवाहक प्रमुख आयुक्त घिमिरेलाई प्रमुख आयुक्तमा सिफारिस गर्ने निर्णय गरेको थियो । घिमिरे २०७१ सालमा अख्तियारको आयुक्तमा नियुक्त हुनुभएको थियो ।          ?                   ',\n",
       " 'उक्त तरकारी बेच्न उनी दैनिक ६० किलोमिटर यात्रा गर्छन् । बिक्री गर्ने तरकारी अघिल्लो दिन नै तयार गरेर बुढा बिहान झिसमिसेमै हिँड्छन् । उज्यालो हुँदासम्म महेन्द्रनगर पुग्छन् । दैनिक एकडेढ क्विन्टल तरकारी महेन्द्रनगर पुर्याउँछु बुढाले भने ? महेन्द्रनगरमा तरकारीको माग पनि बढी छ ? मूल्य पनि राम्रो पाइन्छ ।  महाकाली नदीमा निर्मित झोलुंगे पुल भएर महेन्द्रनगर आउने गरेका छन् । कुतियाकभरदेखि महेन्द्रनगरसम्मको दूरी ३० किलोमिटरभन्दा बढी छ । कञ्चनपुर  कञ्चनपुरको महाकाली नगरपालिका १० कुतियाकभरमा खड्के बुढा तरकारी खेतीका लागि चिनिएका किसान हुन् । उनको बारीमा अहिले पनि मुला ? परबल ? भण्टा ? तरुल लगायतको तरकारी छ । कुतियाकभर तरकारी खेतीका लागि चिनिएको क्षेत्र हो । कुतियाकभरसँगै जोडिएको भारतको बंगाली बस्तीभरि तरकारी खेती गरिने भएकाले त्यहींको सिको नेपालीले पनि गरेका छन् । सिजन अनुसारका तरकारी लगाएर दोधारा चांँदनी र महेन्द्रनगरसम्म पुर्याउने गरिन्छ । यहाँको माटो तरकारीका लागि निकै उर्बर छ लामो समयदेखि व्यावसायिक रूपमा तरकारी खेती गर्दै आएका खड्केले भने ? तर बजारको अभावका कारण कि त आफैंले बोकेर टाढासम्म लैजानुपर्छ ? कि सस्तोमै बेच्नुपर्छ । ',\n",
       " 'उनका अनुसार अस्पतालमा उपचाररत कतिपय बिरामीले रगत प्रोसेसिङको पनि रकम तिर्न नसक्ने अवस्था हुँदा गाह्रो हुने भएकोले नगरपालिकाको सहयोगमा बिरामीले सहज रगत पाउन सक्नेछन् । रगतको जोहो रेडक्रसले नै गर्नेछ । कुतियाकभरमा करिब ४० परिवारको बसोबास छ । यहांँ अधिकांश तरकारी उत्पादनमै लागेका छन् । तर यहांँका कृषकहरूले न त कुनै बीउ बीजन पाउँछन् न कुनै प्राविधिक सुविधा नै । छिमेकी बंगालीहरूबाट बीउबीजन लिएर तरकारी लगाउने गरेको उनीहरू बताउँछन् । बंगालीबाटै खेतीपाती गर्ने तरिकासमेत सिक्नुपर्ने बाध्यता रहेको स्थानीयको गुनासो छ । टीकापुर  टीकापुर नगरपालिकाका बासिन्दाले निशुल्क रगत पाउन थालेका छन् । रगत निकाल्दा लाग्ने प्रोसेसिङ चार्ज नगरपालिकाले व्यहोरिदिने भएपछि बिरामीले रगत निशुल्क पाउन थालेका हुन् । नेपाल रेडक्रस सोसाइटी शाखा टीकापुरका सचिव धर्मराज शर्माले टीकापुर नगरपालिकाभित्रका बिरामीलाई आवश्यक रगत जाँचको खर्च रकम नगरपालिकाले व्यहोर्ने रेडक्रससित सम्झौता भएको बताए । उनले भने ?  रगत त पहिले पनि निशुल्क नै हो तर एक पिन्ट रगत प्रोसिसिङका लागि करिब ८ सय रुपैयाँ लाग्छ । त्यो रकम नगरपालिकाले दिने भएपछि बिरामीले त्यो रकम तिर्न नपर्ने भएको छ । कुतियाकभरकै मानवीर सुनारले पनि मुला ? बोडी र परबर उत्पादन गरेका छन् । उनी पनि खड्केसँगै दैनिक महेन्द्रनगर पुगेर आफ्नो तरकारी बिक्री गर्दै आएका छन् । पारि भारतीय बजारभन्दा महेन्द्रनगरमा प्रतिकिलो ५१० रुपैयाँ बढी पाइन्छ मानवीरले भने ? टाढा भए पनि तरकारीको मूल्य हामीले सोचेभन्दा राम्रो पाइन्छ । ',\n",
       " 'नेपालभारत सीमा क्षेत्रको बस्ती भएकाले बुढाका लागि भारतीय बजार नजिक छ । तर त्यहॉँ तरकारीको उचित मूल्य पाइँदैन । त्यही भएर धेरै टाढा भए पनि महेन्द्रनगर पुग्नुपरेको उनले बताए । उनले आधा बिघा आफ्नो र आधा बिघा वार्षिक ५० हजार भाडामा लिएको जमिनमा तरकारी खेती गरेका छन् । खुर्सानी ? तरुल ? मुला ? परबर ? भण्टासँगै बेसार र अदुवा समेत उनको बारीमा पाइन्छ । मानवीर पनि दैनिक एकडेढ क्विन्टल तरकारी महेन्द्रनगर पुर्याउँछन् । महेन्द्रनगरबाट फर्किंदा पनि सानो तिनो सामान लिएर आउँछन् । अहिले महेन्द्रनगरमा मुला प्रतिकिलो २० ? परबर ४० ? भण्टा ४० र बोडी ६० मा बिक्री हुने गरेको उनले बताए । भारतमा यसको आधा मूल्य पनि पाइँदैन उनले भने ? नजिक भएर मात्रै के गर्नु ? फाइदा हँुदैन । ',\n",
       " 'नगरपालिकाले यसका लागि आर्थिक वर्ष २०७५०७६ का लागि करिब १० लाख रुपैयाँ बजेट विनियोजन गरेको जनाएको छ । रेडक्रससँगको सहकार्यमा निशुल्क रगत उपलब्ध गराउने व्यवस्था मिलाएको नगरपालिकाले जनाएको छ । टीकापुर नगरपालिकाबाहेक अन्य ठाउँका बिरामीले भने टीकापुर अस्पतालमा उपचार गराउँदा आवश्यक रगत परीक्षणको शुल्क आफैं व्यहोर्नुपर्ने नेपाल रेडक्रस टीकापुर शाखाले जनाएको छ । प्रदेश १ का मुख्यमन्त्री शेरधन राई दुखेको गर्धन इलाज गर्न बैंकक गएका के थिए ? केही दिनमै न्युयोर्क पुगेको खबर आयो । सँगै आयोउनको अस्वाभाविक अमेरिकी',\n",
       " 'संघ र प्रदेशबीच अधिकारको विवादबारे चर्चापरिचर्चा सुरु भएको छ । सार्वजनिक सञ्चार माध्यम र प्रदेश तथा संघीय सरकारका मन्त्रीहरूको सार्वजनिक अभिव्यक्तिमा समेत यो विषयले प्राथमिकता पाउन',\n",
       " 'कात्तिक ८ गते राति सिकागो अमेरिका मा सुत्नै लाग्दा मेरो मोबाइल फोन बज्यो । विराटनगरमा जाग्राम बसेकी मेरी श्रीमती डाँको छाडेर रुन थालिन् । किन रोएको भनेर',\n",
       " 'संघीय संसदबाट मातृभाषा शिक्षाको पक्षमा विधेयक पारित भएको छ । अनिवार्य तथा निशुल्क शिक्षाको सम्बन्धमा व्यवस्था गर्न बनेको विधेयकको दफा २५ मा विद्यालयले प्रदान गर्ने शिक्षणको',\n",
       " 'केटाकेटीलाई अनुशासित बनाउन पिट्ने प्रवृत्ति घट्दो छ । तर कराउने ? प्राय सबैले आफ्ना बच्चालाई कहिलेकाहीं कराएर गाली गर्छन् । यसले कुनै काम गर्दैन भन्ने थाहा पाउँदापाउँदै पनि',\n",
       " 'गतवर्ष सिरहाबाट बिहानै एक साथीको फोन आयो । उनी निकै हतासिएकी थिइन् । कसैले उनकी छोरीको फोटो तोडमोड गरेर सामाजिक सञ्जालमा राखिदिएछ । पुरानो रिस साध्न कसैले उनकी',\n",
       " 'पञ्चायतकालको आखिरीतिर रानी ऐश्वर्य समाजसेवाका नाममा गैरसरकारी संस्थाहरूको संरक्षकत्व गर्दै थिइन् । गैससहरूको समन्वय गर्ने समाज कल्याण परिषद्को नेतृत्व गर्थिन् । परिषद्को लैनचौरस्थित मुख्यालय निर्माण गर्न दाताहरूसँग',\n",
       " 'लेनिनले रूसका विख्यात लेखक लियो टल्सटायलाई कति पढे थाहा छैन\\u202fतर अहिले नेपालमा राज्य सञ्चालनको जिम्मेवारीमा रहेका धेरै वामपन्थी नेताहरूले टल्सटायलाई भन्दा लेनिन र सोभियत समाजवादलाई',\n",
       " 'आज कोही निरीह छ भने त्यो हो प्रहरी । प्रहरी चौतर्फी प्रहारको सिकार हुनुपरेको छ । अपराधीलाई कठघरामा उभ्याउने प्रहरी आफैं कठघरामा शिर निहुराएर उभिएको छ । १३',\n",
       " 'युवा पुस्ता राजनीति मन पराउँदैन । राजनीतिको कुरा आउँदा उनीहरू नाक खुम्च्याउँदै भन्छन् ? यो फोहोरी खेल हो ? स्वार्थी व्यापार हो । लुटतन्त्र ? बेथिति अनि पथभ्रष्ट र भ्रष्टाचारको',\n",
       " 'यो वेबसाइट कान्तिपुर राष्ट्रिय दैनिकको आधिकारिक न्युज पोर्टल हो । नेपाली भाषाको यो पोर्टलले समाचार ? विचार ? मनोरञ्जन ? खेल ? विश्व ? सूचना प्रविधि ? भिडियो तथा जीवनका विभिन्न आयामका समाचार र विश्लेषणलाई समेट्छ । पूरा पढ्नुहोस् ',\n",
       " 'नोट हरेक शुक्रवार बेलुका ९ बजे  मा कार्यक्रम द सिनेमा टाइम्स ? हेर्न नभुल्नु होला ? हाम्रो यस आधिकारिक  मार्फत पनि   द सिनेमा टाइम्स ?  हेर्न सकिने छ  साथै एभिन्युज टेलिभिजन मा बिहान ९३० बाट बेलुकी ९३० सम्म प्रतेक घण्टा प्रोमो प्रसारण भैरेहेको छ । फ्यानले काटे आकाशको सरप्राइज बर्थ डे केक फोटो फिचर           ',\n",
       " '२६ कार्तिक २०७५ ? सोमबार १९४० ४ वर्षमा ६ हजार किलोमिटर सडक र २ हजार ८ सय किलोमिटरसम्म रेलमार्ग विस्तार गरिने',\n",
       " 'काठमाण्डौ ? २ जेष्ठ । प्रमुख प्रतिपक्षी दल नेपाली कांग्रेसले सहमतिका आधारमा राष्ट्रियसभा र प्रतिनिधिसभाको नियमावली मस्यौदा पारित गर्नुपर्ने निष्कर्ष निकालेको छ । सिंहदरबारस्थित संसदीय दलको कार्यालयमा बुधबार बसेको काँग्रेस संसदीय दलको बैठकले मतविभाजनका आधारमा नभई सहमतीय आधारमा सदनबाट नियमावली मस्यौदा पारित गर्नुपर्ने निष्कर्ष निकालेको हो । मस्यौदा समितिमा भएको सहमति अनुसार नै जिम्मेवार भएर नियमावली मस्यौदालाई अघि बढाउन कांग्रेसले सत्तारुढ दलको ध्यानाकर्षण पनि गराएको छ । कांग्रेसले नियमावलीका विषयमा सदनमा सहभागी सबै दलहरूबीच अधिकतम सहमति खोज्नुपर्नेमा समेत जोड दिएको छ । त्यस्तै बैठकले सरकारको नीति तथा कार्यक्रममा सम्बन्धमा पार्टीको धारणा ठोस रूपमा राख्नका लागि विषयगत समिति बनाउने निर्णय पनि गरेको छ । टोलीमा विज्ञहरूलाई पनि सहभागी गराइने र छोटो समयमा पनि महत्वपूर्ण धारणा राख्न सक्ने गरी सांसदहरूलाई प्रशिक्षण गराइने मुख्य सचेतक बालकृष्ण खाँणले जानकारी दिए । बैठकले बुधबार सम्पन्न संसदीय दलको निर्वाचनमा सहमति जुटाउन सफल भएकोमा निर्वाचन समितिलाई धन्यवाद दिने निर्णय पनि गरेको खाँडले जानकारी दिए । काठमाडौँ  प्रमिला गिरीको बारेमा पढेर मलाई बस्तीपुरको अनायास याद आयो । गिरी परिवारमा प्रदीप गिरीले लेखेको मलाई मन पर्छ ? वानीरा गिरीको लेखन मलाई मन पर्छ । प्रमिला गिरीको आर्ट चाहिं त्यति बुझ्ने क्षमता नभए पनि उनको व्यक्तित्वको म प्रशंसक छु । गिरी परिवार र बस्तीपुरको नाता छ भनेर दस वर्षअघि थाहा पाउँदा म छक्क परेकी थिएँ । मधेसको कुनै पनि आन्दोलनमा कुनै पनि डोम ? चमारलाई सँगै लिएर हिँड्नुपर्ने कुरालाई बल दिएका छैन । कुनै दलितका किशोरीहरू कलिलो उमेरमा आमा नबनुन् र स्कुल जाउन् भनेर कुनै मधेसी नेता प्राय बोल्दैनन् । कुनै पनि मधेस आन्दोलनले मधेसका जिल्ला अस्पताल राम्रो व्यवस्थापन होस् भनेर आवाज उठाउँदैन । तराईका केही भागको मातृ मृत्यु दर कर्णालीमा उच्च छ भनरे तथ्यांक आउँछ ? तर त्यहाँक नेताहरू प्राय यो विषयमा बोल्न रुचाउँदैनन् कि आन्दोलन जीवनसँग जोडिंदैन भन्ने नेताको विचार छ ? प्रदीप गिरीको विचार छ ? राष्ट्रिय र अन्तर्राष्ट्रिय रूपमा चिनिएको बस्तीपुरको गिरी परिवारका र प्रमिला गिरी ? वानीता गिरी र प्रदीप गिरीजस्ता हस्तीहरूले दलित जीवन ? शिक्षा ? स्वास्थ र उनीहरूको गरिबीबारे बोलिदिए र केही काम गरिदिए बस्तीपुरका चमार र पासवान का जीवनमा केही उज्यालो आउँयो कि भन्ने आशाचाहिं मलाई भइरहन्छ । यो आशा गर्नुहुन्न भन्ने थाहा पाउँदापाउँदै । मधेसका महिलाको आन्दोलनले कुनचाहिं यस्तो मानवीय मुद्दा उठाएको छ जुन मुद्दा सफल भयो भने मधेसका महिला र मुसहर मूल प्रभावमा लाग्न सक्नेछन् ? कुनचाहिं मधेसका नेताले तराईमा भएका हिंसा र बलात्कारको सिकार भएका किशोरीको न्यायका लागि मुद्दा उठाएको छ र मैले प्रदीपजीसँग आशा गर्नु । कुनचाहिं मधेस र थारूहरूको आन्दोलनले कैलाली र दाङका पूर्वकमलरीहरूको जीवनमा परिर्वतन ल्याउन कोसिस गरेको छ । मैले गिरी परिवारसँग आशा गर्नु । कागजमा हटिसकेको कमलरी र कमैया प्रथा वास्तविक जीवनमा अझै छ र १५ वर्षकी कमलरी स्कुलमा जान खोज्दा कनै सहयोग नपाएर घरेलु काममा फर्केको उदाहरण मधेसका नेता र थारू नेतालाई भए पनि मौन छन् । त्यसैले बस्तीपुरका मुसहरका लागि आवाज उठाउन गिरी परिवारले किन बोल्नुपर्यो र ? घरभित्र शौचालय नभएर राति मात्र महिला खेतमा जानुपर्ने बाध्यता कम गर्न कुन नेताले ध्यान दिएका छन् ? दिउसो खेत जान नसक्ने भएकाले आधा पेट मात्र खाएर बस्नुपर्ने मधेसी महिलाको कुपोषण के मधेसी नेताले देखेका छन् ? नेताका परिवारका आमा ? बुबा ? दिदीबहिनीलाई खेतमा जान पर्दैन ? मधेसमा रहेका अस्पताल जानु पर्दैन ? कुपोषणको सिकार भएर अकालमा मर्नुपर्दैन त्यसैले त उहाँहरूलाई किन वास्ता हुन्थ्यो त मुसहर महिला समूहको जीवन सुधार्ने वास्ता हुँदैन । मधेसको अहिलेको आन्दोलनमा आएमा नेतृत्व वर्गमा कति जना मुसहरपासवानडोम महिलाहरू छन् ? कुनचाहिं मुसहर महिलाले यो आन्दोलनमा मलाई उमेर पुगेपछि मेरो नै नागरिकता पाउनु मेरो अधिकार हो । र ? म १६ वर्ष पुगेपछि नागरिकता पाउन सक्छु मेरो खातामा मैले कमाएको पैसा राख्न सक्छु भनेर आवाज उठाउन सकेका छन् । उनीहरूले यो आवाज उठाउन सक्दैनन् किनभने उनीहरूको आवाज दबिएको छ । जसले आवाजमा शक्ति भर्ने हो ऊ चुप छ । विकासका लागि यस्ता कुरा उठाएर भनिदिए हुन्थो । कति डोमको घरमा शौचालय छ र आन्दोलनले उनीहरूलाई के दिनेर भनेर हेरे ? उनीहरूलाई सोधौँ कसरी उनीहरूले चुलोमा आगो बलेको छ बुझिदिए त हुन्थ्यो । काठमाडौँ  निर्मला  तिमी मरेकी छैनौकसले भन्छ तिमी मर्यौ ? तिमी त करोडौं करोड विवेकशील मान्छेको आाखामाअमर भएर बााचिरहेकी छ्यौ ',\n",
       " 'दार्चुला  हिमाली क्षेत्रमा बसोबास गर्दै आएका शौका समुदाय पछिल्लो पुस्तादेखि दसैंतिहार मनाउन थालेका छन् । पछिल्ला १०१५ वर्षयता अन्य समुदायको देखासिकीका कारण चाडपर्व मनाउन थालिएको उनीहरू बताउँछन् । शौका जनजाति समुदायको बसोबास नेपालको दार्चुला जिल्लामा मात्र छ । कपिलवस्तुको तौलिहवास्थित तौलेश्वरनाथ मन्दिर परिसरमा माटोका भाँडाकुँडा बेच्न बसेका व्यवसायी । तिहारमा विशेषगरी पाला बढी बिक्री हुने गरेका छन् । तस्बिर  मनोज पौडेलकान्तिपुर',\n",
       " 'एकडेढ दशक अघिसम्म दसैंतिहारको समयमा आफूहरू ब्याँसमै हुन्थ्यौं ?  तिंकर टोलका स्थानीय चमकसिंह तिंकरीले भने ? बिस्तारै अन्य समुदायसित सदरमुकाम खलंगामै बसिन थालेपछि रमाइलोका लागि दसैंतिहार मनाउँछौं ।  तिहारमा भने यमपञ्चकका पाँचै दिनमा तिहार मनाउँदैनाैं । लक्ष्मीपूजा अर्थात् औंसीको दिन घरभित्र साँझको समयमा आफ्नो कुल देवतालाई सम्झेर पूजा गरिने उनको भनाइ छ । अन्दाजी ४५ वर्ष उमेर बताउने तिंकरी दसैं त आफूले हाईस्कुल पढ्ने बेलादेखि मनाउन थालेको बताए । तिहार भने उक्त समयमा मनाउँदैनथ्यौं ?  उनले भने ? आफूभन्दा पाको पुस्तालाई सोध्दा दसैंतिहारका बारेमा जानकारी नभएको बताउँछन् । ',\n",
       " 'तिहारमा मनाइने काग ? कुकुर ? गाई ? गोरु र भाइटीकामध्ये पछिल्ला समय क्याम्पस र कलेज पढ्ने विद्यार्थी उमेरसम्मकाले बिस्तारै मनाउँदै आएका छन् । यहाँका स्थानीय अन्य समुदायमा भाइटीकालाई महत्त्वसाथ मनाउने हुनाले उनको साथी सर्कलका भाइबहिनी टीका लगाउँछन् । तर ? हाम्रो पुस्तामा भने भाइबहिनीमा तिहारको टीकासमेत लगाउने चलन छैन ?  तिंकरीले भने । उनका अनुसार आफूभन्दा पाको पुस्तालाई तिहार भनेपछि जुवातास मात्र खेल्ने सम्झन्छन् । आफ्ना बालबच्चाको अन्य समुदायसँग हिंडाइ बसाइ बाक्लिँदै गएपछि उनीहरूको संस्कार भित्रिनु स्वाभाविकै भएको महेन्द्र माध्यमिक विद्यालयका शिक्षक प्रेमसिंह बोहराको भनाइ छ । दसैंतिहार मान्ने आफ्नो परम्परागत संस्कार नभए पनि अन्य समुदायका परिवार छिमेकी हुँदा त्यसको प्रभाव बढेको छ ?  उनले भने ? वल्लोपल्लो घरका मानिसले बिस्तारै दसैंतिहार मान्दै आएपछि आफूले पनि बाध्य भएर मनाउनुपर्ने स्थिति छ । ',\n",
       " 'आफ्नो समुदायको परम्परागत रूपमा भव्यतासाथ मनाउँदै आएको चार्ड एवं मुख्य पर्व घोबला हो । उनीहरूको मुख्य चाड घोबला माघ महिनामा मनाउने उनले बताए । पहिला यो समयमा ताक्लाकोटको व्यापारमा व्यक्त भइने बेला भएको हुनाले सबै मानिस कोही ब्याँसमा त कोही व्यापारको सिलसिलामा ताक्लाकोट भइन्थ्यो ?  बोहराले भने ? परिवारका सबै सदस्य आफ्ना काममा व्यस्त हुन्थे । त्यसका कारण पनि दसैंतिहार तथा अन्य चाडको खासै मतलबै हुँदैनथ्यो ।  अहिले पनि यो समुदायका धेरैजसो मानिस व्यापारका सिलसिलामा तांलाकोट गएको उनी बताउँछन् । अधिकांशका घरमा केटाकेटी र वृद्धवृद्धा भेटिन्छन् । यो समुदाय परम्परागत रूपमा व्यापार गर्दै आएको हुनाले महिला पुरुष व्यापारमै व्यस्त हुने शिक्षक बोहराको भनाइ छ । ब्याँस क्षेत्रमा बसोबास रहेका शौका समुदायका मानिस यो महिना ताक्लाकोटको व्यापार सकेर मंसिरको सुरुमै बेंसी अर्थात् सदरमुकाम खलंगा झर्ने गर्छन् । परम्परागत आफ्ना लालाबालासहित बसाइँ सर्ने हुँदा हिउँदमा लगाइने न्यानो कपडा र ऊन ताक्लाकोटबाट ल्याएर यहाँ व्यापार गर्छन् । ',\n",
       " '६ महिनासम्मलाई व्यापार गर्नका लागि ताक्लाकोटबाट रेडिमेट लत्ताकपडा ? श्रृगारका सामग्री ? इलोक्ट्रोनिक सामान बिक्रीका लागि बेंसी ल्याउँछौं ?  ब्याँस र खलंगामा व्यापार गर्ने स्थानीय व्यापारी जितसिंह बोहराले भने ? ६ महिनापछि ब्याँस उक्लँदा यहाँबाट ताक्लाकोटका लागि गुड ? मिश्री ? शृंगारलगायत सामान लानुपर्छ । ',\n",
       " 'सिमरा  तराईको प्रसिद्ध लोक पर्व समा चकेवा मनाउन बारा सदरमुकाम कलैया लगायत जिल्लाको ग्रामिण भेकमा अहिले समा चकेवाको कलात्मक मुर्ती घर घरमा भित्रियाउन थालिएको छ । माटोले बनाईएको उक्त मुर्ती लिएर गाउ गाउमा आईपुगेका कुम्हालबाट किशोरीहरुले प्रतिगोटा नगद २५ रुपैयाँ वा सो रकम बराबर अन्न दिएर खरीद गरीरहेका छन । एका बिहानै कांधमा समा चकेवाको भरिया बोकेर कलैया उपमहानगरपालिका १६ उत्तरझिटकैयामा पत्रहट्टीबाट एक कुम्हाल आई पुगेका थिए । उनको कांधमा रहेको भरिया समा चकेवाको स साना मुर्तिले भरिएको थियो । उनी गाउमा प्रवेस गरेसंगै स साना बालबालिकाहरु समा चकेवा आयो भन्दै खुशीले हौसिएका देखिन्थे । गाउका अधिकांसलाई पायक पर्ने स्थानमा उनी भरिया राखेर पहिला सुस्ताए । आफ्नो घरबाट उनी झन्डै ३ घण्टाको पैदल दुरी तय गरेर मुर्ती बेच्न परको गाउमा आई पुगेका थिए । सिजनको बेलामा यस्तो मुर्ती निर्माण गर्छौ ?  मुर्ति बेच्न आएका विश्वनाथ कुम्हालले भने ? पर्व सुरु हुनु अगावै बिक्री गरीसक्नु पर्छ ।  समयमा बिक्री गर्न नसकिए लगानी खेर जाने उनले बताए । उनी लामो समय देखी सिजनलाई लक्षित गरी समाचकेवाको मुर्ती बनाएर बिक्री गर्दै आएको बताउछन । माटोको भाडा कुडा ? मुर्ती ? दियो लगायत निर्माण गर्ने उनको पुख्र्यौली पेसा हो । पुख्र्यौली पेसाबाट जिवन निर्वाह गर्न सक्ने अवस्था नरहे पनि माग अनुसारको सामा निर्माण गरी पुर्याउदै आएको उनले बताए । आफ्नो उत्पादनलाई छिटो भन्दा छिटो बिक्री गर्न समा चकेवाको कलात्मक माटोको मुर्ति घुमाएर बिक्री गर्न थालेको उनले बताए । उनले उत्तर झिटकैयामा मात्र ३२ वटा भन्दा बढी समा चकेवाको मुर्ति एकै छिनमा बिक्री गरेको जानकारी दिए । पहिलाको तुलनामा मुर्तिको लगायत माटोको भाडोको व्यपार घटेको उनले जानकारी दिए । प्रत्येक वर्ष गोवद्र्धन पुजा देखी कार्तिक पुर्णिमा सम्म स्थानीय किशोरीहरु समाचकेवा उक्त मुर्ति राखेर समा चकेवा लोक पर्व मनाउछन । समा चकेवा लोक पर्व मनाउन उत्तरझिटकैयाकी १४ वर्षिया किशोरी मुनिया कुमारी ठाकुरले समा चकेवाको २ वटा मुर्ति एक्लैले खरीद गरीन । उनको समुहमा अन्य ४ जना किशोरीहरु सामेल रहेका छन । उनीहरुले पनि एकएक वटा मुर्ति किनेको उनले जानकारी दिईन । पहिला जस्तो धेरै समा चकेवाको मुर्ति किन्दैनन ?  उनले भनिन ? पहिला धेरैले सामचकेवाको मुर्ती किन्थे ।  अहिले १६ वर्ष मुनिका किशोरीले मात्र समा चकेवा खेल्छन । ठुलाहरु खेल्दैनन । त्यसैले अहिले आएर मुर्तीको माग कम हुदै गएको उनले जानकारी दिईन । सोही ठाउकै अर्का किशोरी रंजिता कुमारी मण्डलले परम्परागत रुपमा मनाईदै आएका धेरै लोक पर्व लोप हुदै गएको बताईन । समा चकेवा प्रति पनि धेरैको चासो छैन ?  उनले भनिन ? पहिला बढी उमेरका किशोरी र महिलाले पनि मनाउथे ? अहिले मनाउदैनन ।  हामी कलिाला किशोरीहरुले यो मौलिक पर्वलाई धानेका छौं । गाउका युवा पुस्ताले ध्यान नदिए यो पर्व पनि अरु लोक पर्व जस्तो लोप भएर जाने उनको बुझाई छ । लोक पर्व हाम्रो पहिचान हो ? त्यसैले यसलाई जिवन्त राख्नु पर्छ ?  उनले भनिन ? पहिचानको लागी सबैले आन्दोलन गर्छन तर पहिचान झल्काउने लोक पर्वको जगेर्णा गर्दैनन ।  मधेशको पहिचान र संस्कृति झल्काउने लोक पर्वलाई जोगाउनै पर्नेमा उनको जोड छ । लोक पर्वलाई मनाउन अहिले देखी नै किशोरीहरु तयारीमा लागेको गाउका बृद्धा ७५ वर्षिय भुवन रात अहिरले बताए । पहिला पहिला त अन्नबाट नै समा चकेवाको मुर्ती किन्थे ?  उनले भने ? नगद हेर्नै मुश्किल पथ्र्यो ।  तर अहिले सबैसंग पैसा सहज उपलब्ध भएकोले अन्न भन्दा पैसाले नै मुर्ती खरीद गर्ने गरेको उनले बताए । पहिला पहिला जोडतोडका साथ श्रद्धा पुर्वक समा चकेवा मनाउने गिरिन्थ्यो । अहिले यो पर्व मनाउन कसैले चासो नदेखाएको उनले गुनासो गरे । युग परिवर्तनसंगै रिती संस्कृती समेत परिवर्त हुन थालेकोमा उनले चिन्ता व्यक्त गरे । तराई मधेशमा खासगरी मधेशी र थारु समुदायका किशोरीहरु समा चकेवा लोक पर्व मनाउने गर्दछन् । त्यहाँ तर अहिले औपचारिक रूपमा स्कुल निषेधित त छैन तर धेरै दलित बालिका स्कुल जाँदैनन् । सानैमा गौना गर्छन् । उनीहरूको स्कुल जाने सपनालाई कुनै पनि मधेसका नेताहरूले मुद्दा बनाएजस्तो लाग्दैन ? त्यसैले गिरी परिवारले किन वास्ता गरेनन् भनेर चित्त दुखाउनुपर्ने त कारण सायद छैन । तर ? मानवीय व्यवहार नै हो ? जो शक्तिशाली छ ऊसँग सायद बढी नै आशा गरिन्छ । मैले भन्ठानेकी छु सायद हामी टाढाबाट बस्तीपुर हेर्ने मानिसले गिरी जति शक्तिशाली परिवार छ ? त्योभन्दा बढी नै अपेक्षा गरेको हो कि ? ',\n",
       " 'किनभने प्रदीप गिरीले त्यहींबाट आफ्नो राजनीति बनाए ? उनी अनेक दार्शनिक लेख लेख्छन् ? गाँधीका ? अनुयायी हुन् भन्ने थाहा थियो । तर ? उनको परिवारले बनारसम गएर उच्च शिक्षा हासिल गरे पनि सायद उनले बस्तीपुर को बस्तीलाई चाहिं बिर्सेजस्तै छ । किनभने म बस्तीपुर पुग्दा कोही पनि दलित ? मुसहर ? चमार ? राय ? पासवान ? डोम । स्कुलमा आउँदैनथे । डोम त सायद अझै पनि स्कुल जाँदैनन् । बस्तीपुरको स्कुल २००४ सालमा खुलेको थियो । त्यहाँ पढ्न भारतबाट पनि आउँथे रे । तर ? वरिपरि भएका दलितहरूका लागि त्यो स्कुल प्राय निषेधित क्षेत्र नै रहेको थियो ?  एक ७० वर्षीय शिक्षकले भनेका थिए । यो त गिरीहरूको राजनितीक क्षेत्र हो । किन सानो रूपमा भए पनि यहाँका दलितहरूलाई शक्तिशाली परिवारले केही प्रेरणा नदिएको होला ।  मैले प्रमिला गिरीको बारेमा पढेपछि सम्झें । तर ? सायद उहाँहरूले धेरै काम गरे पनि ? मुसहर बस्तीसम्म पुगेको पनि हुन सक्छ । बस्तीपुर बारम्बार जाँदा मैले अनुभव गरेकी हुँ कि कति मुसहर महिला नागरिकता नपाएर सरकारले दलितलाई दिएको सुविधा पाएका छैनन् । सायद ती दलित परिवारले आफ्नो व्यथा गिरी परिवारलाई सुनाउन भ्याएका छैनन् । मधेसमा छोरीहरूलाई नागरिकता दियो भने बाबुको सम्पत्तिमा आफ्नो अधिकारको दाबा गर्छन् भन्ने विचार हुन्छ । लोग्नेले पनि विवाह गरेपछि नागरिकता दिन चाहँदैनन् ? सम्बन्धविच्छेद भए लोग्नेको सम्पत्ति माग गर्छन् भनेर ।  यो वास्तविकता त पक्कै पनि गिरी परिवारलाई थाहा नै छ होला । प्रदीप गिरीले कांग्रेसका नेताको हैसियतले यी समस्या उठाउँदा पनि कतै हामीले नसुनेका त होइनौं ? सायद स्थानीय नेताले राष्ट्रिय स्तरका नेता गिरीलाई यो समस्या भन्न नभ्याएका हुन् कि ? सायद कति नेता भन्छन् ? हामीले सम्पूर्ण मधेसको मुद्दा उठाइरहेछौं । यति जाबो एउटा बस्तीपुरको त्यो पनि मुसहर ? डोम र चमारको मुदा मात्र उठाउने हो र ?  तर मलाई लाग्छ साधारण समस्या समाधान नभए कसरी जीवन चल्छ ? द्वन्द्व विशेषज्ञ डा विण्णु उप्रेतीले एक छलफलको कार्यक्रममा भन्नुभएको थियो ? मधेसी दलले मधेसी मुद्दा उठाएका नै छैनन् । भूमिहीन मुसहर ? हलियाको जीवनको स्तर अति नराम्रो छ तर मधेसका नेताहरूले कुनै पनि नीति ? माग र आन्दोलनले भूमिसुधार गरेर भूमिहीनको जीवन बदल्नका लागि आवाज उठाउँदैनन् ? उनीहरू मधेसका महिलाप्रति गरिने मानसिक र शारीरिक हिंसाविरुद्धमा बोल्दैनन् । दाइजो प्रथा र बोक्सीको नाममा मधेसका महिलाप्रति गरिने अति अमानवीय कार्यको विरुद्धमा मधेसीमा नेताहरूले आवाज उठाउँदैन ।  द्वन्द्व विशेषज्ञको विश्लेषण र विचार पनि सायद मधेसका नेताको कानमा पुग्दैन । दर्शनको बारेमा गान्धीको विचार लेख्ने कुनै सिरहाका नेताले आफ्नो लेखमा बस्तीपुर महिलाले ज्ञानको लाठी लिएर गलत सांस्कृतिक मान्यतालाई हटाउन प्रेरित गरेको देखिएको छैन । कहिले उनीहरूले मुसहरको आवाजलाई स्थान दिएको पाइएको छैन । ना॰ खेतीपाती गर्ने वर्षायाममा वा पानी पर्नुपर्ने समयमा पानी नपरी सुक्खा लागेको अवस्था लामो समयदेखि पानी नपरेको अवस्था सुक्खा अनावृष्टि । पाठ क्रिएटिभ कमन्स एट्रिब्युसनसेयरअलाइक लाइसेन्सअन्तर्गत उपलब्ध छ अतिरिक्त सर्तहरू लागू हुन सक्छन् । अधिक जानकारीको लागि उपयोगका सर्तहरू हेर्नुहोला । गाउँमा जनप्रतिनिधि निवार्चित भएपछि स्थानीय तहहरुमा केही न केही नौला कामहरु भइरहेका छन । रोल्पा जिल्लाको सुकिदह गाउँपालिका ३ नं वडापालिका पुरै गाउँनै हरियाली देखिएको छ । रोल्पा ? १५ मंसिर । सदरदमुकाम लिबाङदेखि केही कोष टाढा रहेको सुकिदह गाउँ । पहिले पुग्दा हरेक रङले गाउँ रङगी ? बिरङगी देखिन्थ्यो । अहिले गाउँ पुरै हरियो रङले सजाइएको छ । सरसफाइको प्रतिक भनेर चिनिने हरियो रंङले गाउँ हरियाली बनेको छ । सुकिदह गाउँपालिका वडा नं ३ मा करिब ७ सय घरहरु रहेका छन । ती घरमध्ये ८७ प्रतिशत घरमा हरियो रङ लगाइसकेको वडा सचिब कमलराज वलीले जानकारी दिए । गाउँको फरक परिचान बनाउनको लागि यो अभियानको सुरुवात गरेको सुकिदह ३ का वडाअध्यक्ष जलेशकुमार केसीले बताए । पूर्ण सरसफाइमा सहयोग पुयाउने उद्देश्यका साथ यो अभियानको सुरुवात गरिएको हो । गाउँका पुरै घर एउटै रङ लगाउँदा निकै सुन्दर देखिएको स्थानीयहरुले बताएका छन । उनीहरु वडापालिकामा एउटै रङ लगाउने कुरामा सहमत भएका थिए । सरकारकै इशारामा चल्ने किसिमको कर्मचारी संयन्त्र बनाउन आफूहरुले छलफल गरिरहेको संकेत गर्दै उनले कर्मचारीहरु सिंहदरबारबाट प्रदेश र गाउँपालिकामा जान नमान्ने नियति छिट्टै अन्त्य हुने बताए । पोखरा । नेपाल कम्युनिष्ट पार्टी नेकपा अध्यक्ष पुष्पकमल दाहालले कर्मचारीकै कारण सरकारले जनता सामु गरेका बाचा पुरा गर्न नसकेको बताएका छन् । प्रेस सेन्टर र प्रेस चौतारी गण्डकी प्रदेशले पोखरामा बुधबार आयोजना गरेको पत्रकार सम्मेलनमा दाहालले राजनीतिक परिवर्तनका लागि दलहरु सफल भए पनि कर्मचारी संयन्त्रका कारण सरकार असफल भएको आरोप लगाए । पूर्वप्रधानमन्त्री समेत रहेका दाहालले अन्तर्राष्ट्रिय जगतमा राजनीतिक परिवर्तनपछि पुराना कर्मचारीको नेतृत्व तहलाई पूरै हटाइने र नयाँ प्रणालीलाई आत्मसात गर्ने कर्मचारी ल्याउने परम्परा रहेको बताए । ब्यूरोक्रेसीले राजनीतिक नेतृत्वलाई बिस्तारै आफूतर्फ लैजाँदा दलहरु असफल हुने गरेको उनले बताए । प्रदेश सरकारलाई प्रभावकारी नबनाउने हो भने परिवर्तन असफल हुने र सबै उपलब्धी चेतावनीसमेत उनले दिए । उनले भने ? प्रदेश सरकार असफल हुनु भनेको गणतन्त्र ? संघीयता ? धर्मनिरपेक्षतालगायत सबै परिवर्तनहरु असफल हुनु हो । ',\n",
       " 'अध्यक्ष दाहालले ओली सरकार अन्तर्राष्ट्रिय शक्ति सन्तुलन ? राष्ट्रियता ? ऐतिहासिक र रणनीतिक रुपमा सफल भएको बताए । जनताले देख्ने गरि विकासको काम हुने ठोकुवा गर्दै उनले सरकारले गियर बदलेर जनताका चाहना पुरा गर्ने बताए । असार ९ काठमाडौँ क्रोयसियासँग बिहीबार भएको विश्वकप खेलमा अर्जेन्टिनाले लज्जास्पद हार ब्यहोरेको र यसका कारण खेलाडीले प्रशिक्षक जर्ज सामपाओलीलाई हटाउन चाहेका भन्ने समाचार आफ्नो समूहविरुद्ध अफवाह फैलाउन रचिएको प्रपञ्चबाहेक केही नभएको अर्जेन्टिनाले जनाएको छ । यो विल्कुल गलत हो ?  अर्जेन्टिनी फुटबल समूहका अधिकारीले भने ? खेलाडीको बैठक र अन्य केहीबारे पनि आएका नकारात्मक खबरहरू भ्रम सिर्जना गर्न गरिएका अनर्गल प्रचारमात्र हुन् । ',\n",
       " 'बिहीबारको खेलपछि अर्जेन्टिनाका स्थानीय तथा अन्तर्राष्ट्रिय सञ्चारमाध्यममा अर्जेन्टिनाका खेलाडी तथा प्रशिक्षक र प्रशिक्षण समूहका अन्य सदस्यबीच तनाब बढेको र खेलाडीले प्रशिक्षक सामपाओलीलाई हटाउन चाहेको भन्ने समाचारहरू बाहिरिएका थिए । शनि ? असार ९ ? २०७५ मा प्रकाशित सुनौलो नेपाल अनलाइनद्वारा संचालित सुनौलो नेपाल टी भीसंग प्रत्यक्ष जोडिन यहाँ क्लिक गर्नुहोस',\n",
       " 'हलिउड अभिनेता फिलिप सिमोर हफम्यानको निधन भएको छ । ओस्कार अवार्ड विजेता फिलिपको ४६ बर्षको उमेरमा न्युयोर्कस्थित आफ्नै अपार्टमेन्टमा निधन भएको हो । उनी केही समयदेखि लागूऔषधीको दुव्र्यसन थिए । अपार्टमेन्टबाट लागूऔषधका सिरिन्ज र हेरोइन फेला परेको बताइएको छ । सन् १९९० पनि हलिउडमा देखिएका फिलिपको बग्गी नाइट ? द बिग लेबोस्की जस्ता चर्चित फिल्म रहेका छन् । रंगमन्चबाट अभिनय थालेका फिलिपले चलचित्रको निर्देशन समेत गरेका छन् । उनलाइ २००५ मा कोपेट फिल्मको लागि ओस्कार अबार्ड दिइएको थियो । उनले आफ्नो अन्तिम अभिनय हङ्गर गेम्समार्फत गरेका थिए । असंख्य शुभचिन्तकहरुले उनको निधनमा शोक व्यक्त गरेका छन् । उनलाई महत्वकांक्षी र निकै प्रशन्न अभिनेताका रुपमा हलिउडमा सम्झने गरिन्छ । काठमाडौं  एभिन्यूज खबरले हरेक वर्ष सञ्चालन गर्दै आएको देउसी भैलो प्रतियोगिता यस वर्ष पनि सञ्चालन गर्ने भएको छ । ',\n",
       " 'प्रधानमन्त्री शुसिल कोइराला तेस्रो बिमस्टेक शिखर सम्मेलनमा भाग लिएर स्वदेश फर्केका छन् । फर्कने क्रममा त्रिभुवन अन्तराष्ट्रिय बिमानस्थलमा संचारकर्मीहरुसंग कुरा गर्दै प्रधानमन्त्री कोइरालाले बिमस्टेक सम्मेलन नेपालका लागि फलदायी भएको दाबी गरेका छन् । उनले सम्मेलनमा भुटानी शरणार्थी समस्या समाधान लागि भुटानी पक्षसंग निकै महत्वपुर्ण कुराकानी भएको बताएका छन् । नेपालले बिमस्टेकको अध्यक्षता ग्रहण गर्न',\n",
       " 'काठमाडौं  सरकारले भारत ? संयुक्त अरब इमिरेट्स र मलेसियाका लागि राजदूत सिफारिस गरेको छ । गएराति भएको मन्त्रीपरिषद् बैठकले तीन ',\n",
       " 'पाउनु ठुलो उपलब्धी भएको भन्दै प्रधानमन्त्री कोइरालाले सबै सदस्य राष्ट्रहरु मिलेर यस क्षेत्रको गरिबी घटाउने लगायतका साझा चुनौती सामना गर्नुपर्नेमा जोड दिए । उनले भारतिय प्रधनमन्त्री डा मनमोहन सिंह र म्यानमारकी प्रजातन्त्रवादी नेत्री आङसाङ सुकीलाइ नेपाल भ्रमणको निमन्त्रणा दिएको जानकारी गराएका छन् । पत्रकार सम्मेलनमा प्रधानमन्त्री कोइरालाले प्रशंगबस आफुले उठाएको टनकपुरबाँधको बिषयमा बिबाद गर्नुपर्ने आबश्यकता नरहेको पनि बताएका छन् । उनले सम्मेलनमा नेपालमा जारी शान्ति प्रक्रियामा संबिधान निर्माण तथा लोकतन्त्र प्राप्तिका लागि भएका प्रयासका बारेमा पनि सम्मेलनमा सहभागी बिभिन्न देशका राष्ट्र प्रमुखहरुले चासो दिएको पनि बताए । आल्डरशट ? ग्रेटर रशमॊर नेपाली कम्युनिटीले यहि आगामी १ जुन २०१७ ? बेलायतकी महारानी एलिजाबेथ द्वीतियको जन्म दिन भिक्टोरिया डे मनाउने काम भब्य रुपमा सम्पन्न गर्ने भएको छ । यस वर्ष ग्रेटर रशमॊर नेपाली कम्युनिटीको सक्रिय पहलमा स्थानिय काउन्सिल एवं ब्रिटिश समुदायसंग प्रत्यक्ष रुपमा साझेदारी सहित सहकार्य गरि प्रथम नौमती बाजा लगायत प्राय सबैजसो',\n",
       " 'कोरियामा प्रवासी मजदुर स्वयमले प्रवासी मजदुरहरुकै समानता र हक अधिकार प्राप्तिको निम्ति गठन भएको एक मात्र मजदुर युनियन प्रवासी मजदुर युनियन एमटियुले निय',\n",
       " 'कोरियामा रहेको प्रवासी श्रमिकहरुको हक अधिकारको निमित्त संघर्ष गर्दै आइरहेको प्रवासी श्रमिकहरुको एक मात्र स्वतन्त्र युनियन प्रवासी मजदुर युनियन एमटियुले ',\n",
       " '२७ वषर्ीय जहाँगिरविरुद्ध वीरगन्जकै एक युवती रमा नाम परिवर्तन ले असार २ गते जिल्ला प्रहरी कार्यालय पर्सामा साइबर अपराध मुद्दाको किटानी जाहेरी दर्ता गरेकी थिइन् । ती युवतीले आफ्नो सेक्स टेपलाई जहाँगिरले इन्टरनेटका विभिन्न साइटमा राखेको तथा त्यसको सिडीसमेत बनाई बजारमा सार्वजनिक गरेको आरोप लगाएकी छिन् । गत जेठ २३ गते युट्युबलगायत विभिन्न अश्लील पोर्नसाइटमा ती युवतीसमेत सरिक सेक्सक्लिप राखिएको थियो । जहाँगिरले आफूसँग पटकपटक शारीरिक सम्पर्क राख्न धम्की र दबाब दिएको तर आफूले नमान्दा उक्त मुभीको सिडी आफ्ना श्रीमान्को घर तथा माइतीमा पठाई आफ्नो बेइज्जती गरेको दाबी जाहेरीमा गरिएको छ । उक्त सेक्स टेप प्रकरणपछि आफ्ना बुबा पक्षघात भै हिडडुल गर्न नसकी अपाहिज भैसकेको तथा बहिनीहरू घरबाट बाहिर निस्कन नसकेको उनले आफ्नो जाहेरीमा उल्लेख गरेकी छिन् । हाल श्रीमान्ले आफूलाई बहिष्कार गरी माइतीमा छाडी गएको अवस्थामा समेत जहाँगिरले आफूलाई पटकपटक धम्की दिइरहेको उनले आरोप लगाएकी छिन् । रमाले जहाँगिरसँग आफ्नो मित्रता एवं प्रेमसम्बन्ध एक कम्प्युटर इन्स्िटच्युटमा कम्प्युटर सिक्ने क्रममा भएको बताएकी छिन् । जहाँगिरले त्यस क्रममा आफूलाई नशालु पदार्थ ख्वाएर शारीरिक सम्पर्क राखेको तथा त्यसको तस्बिर खिचेको दाबी गरेकी छिन् । उक्त मुभी वीरगन्जकै छपकैयातिर खिचिएको हुन सक्ने उनको अनुमान छ । चरणबद्ध रूपमा दुई थरीका क्लिप इन्टरनेट साइटमा राखिएका थिए । तेस्रो क्लिप भने सिडीका रूपमा बजारमा आएको थियो । आगोझैं फैलिएको वीरगन्जसँग सम्बन्धित उक्त सेक्स टेप प्रकरण पछि जहाँगिर फरार भएका थिए । उनी एकैपटक अदालतमा उपस्थित भएका हुन् । मुलुकमा साइबर अपराधसम्बन्धी कानुन निर्माण भएपछि उक्त कानुनलाई टेकेर न्यायालयसमक्ष पुगेको वीरगन्जको एउटा मुद्दा अहिले चर्चाको बिषय बनेको छ । साइबर अपराधका आरोपी एक युवक भदौ २३ गते पर्सा जिल्ला अदालतमा १ लाख ३० हजार रुपैयाँ धरौटी राखेर सामान्य तारेखमा छुटेका छन् । मुलुककै नौलो खालको तथा सम्भवतः पहिलो यो मुद्दाका आरोपी वीरगन्ज उपमहानगरपालिका२ ? छपकैंयाका शैफ भनिने जहाँगिर आलम पर्सा जिल्ला न्यायाधीश विष्णुप्रसाद कोइरालाको आदेशमा धरौटी लिई तारेखमा रिहा भएका हुन् । युवतीले विवाहपूर्व आफ्नो प्रेमसम्बन्ध जहाँगिरसँग रहेको र त्यसै क्रममा उनीसँग राखिएको शारीरिक सम्बन्धको तस्बिर मोबाइलको क्यामेराबाट छायांकन गरी इन्टरनेट वेबसाइट तथा सिडीमार्फत सावर् जनिक गरिएको दाबी गरेकी छिन् । उक्त मुभीलाई जहाँगिरले एमएमएस तथा सिडी बनाई सार्वजनिक रूपमा बिक्रीवितरणसमेत गरेर आफूलाई हदैसम्मको ब्ल्याकमेलिङ गर्ने काम गरेको रमाले बताएकी छिन् । युवतीको माइती तथा घर दुवै पक्षका व्यक्तिहरूको नामै उल्लेख गरेर उक्त सेक्स प्रकरण सार्वजनिक भएपछि उनको माइती र घर दुवै पक्षले सामाजिक रूपमा ठूलो अपहेलना भोग्नुपर्यो । भारतमा उनको घर पक्ष आफू बस्दै आएको सहरबाट बसाइँ सरिसकेको छ । न्यायाधीश कोइरालाको इजलासले थुनछेक प्रयोजनको बहसपछिको आदेशमा आरोपित जहाँगिर कसुरदार देखिए पनि निजलाई अदालती बन्दोबस्त ११८२ अनुसार थुनामा राख्नुपर्ने अवस्था नदेखिएकाले पछि प्रमाण बुझ्दै जादा ठहरेबमोजिम हुने गरी निज प्रतिवादीबाट अदालती बन्दोबस्तको ११८५ समेतको आधारमा धरौटी लिई मुद्दाको पुर्पक्ष गर्ने भनी उल्लेख छ । अविवाहित जहाँगिरले अदालतमा बयानका क्रममा आफूले हालसम्म रमासँग भेटघाट नगरेको र उनीसँग आफ्नो कुनै पनि खालको मित्रता तथा सम्बन्ध नरहेको बताएका छन् । रमाले जाहेरीमा आफूउपर लगाएका आरोप पूर्ण रूपले झूटो र निराधार भएको उनको जिकिर छ । जहाँगिरले आफूले कक्षा ८ सम्म मात्र पढेको र रमासँग कुनै सम्बन्ध र सरोकार नभएकाले उनी संलग्न सिडी तथा सेक्स क्लिप आफूले सार्वजनिक नगरेको जिकिर गरेका छन् । जिल्ला सरकारी वकिलको कार्यालयले जिल्ला अदालतमा उक्त मुद्दा प्रस्तुत गर्दै जहाँगिरले विद्युतिय इलेक्ट्रोनिक कारोबार ऐन २०६३ को दफा ४७१ अनुसारको कसुर अपराध गरेकाले उनलाई हदैसम्मको सजायको माग गरेको छ । सरकारी वकिलको कार्यालयको अभियोगपत्रमा जहाँगिरले सामाजिक जातीय सद्भाव बिगार्ने ? कसैप्रति घृणा वा द्वेष फैल्याउने वा विभिन्न जातजाति तथा साम्प्रदायिक सुमधुर सम्बन्ध खलल पार्ने कार्य गरेकाले उनलाई सजायको माग गरेको छ । रमाका कानुन व्यवसायी अधिवक्ता सुधीर कर्णले अधिराज्यमै सम्भवतः कुनै पनि न्यायिक निकायमा दर्ता गरिएको सबैभन्दा पहिलो यस्तो खालको मुद्दामा आफूले चाहेर पनि बहस पैरवी गर्न नपाएको बताए । आफू कामविशेषले हेटौंडामा रहेको र रमालाई समेत उक्त मुद्दाको पेसी बुधबार परेको कुराको जानकारी नहुँदा अत्यन्त कम समयमा अदालती कारबाही सम्पन्न भएको उनले बताए । उनले सूचना र प्रविधिको वर्तमान युगमा यस्ता खाले मुद्दामा इजलासमै भिडियो ? कम्प्युटर आदि राखेर न्याय निरूपण गर्न सकिने अवस्था विद्यमान हुँदाहँुदै पनि त्यसो नगरिएको गुनासो गरे । उनले प्रहरीले यो मुद्दामा गम्भीर रूपमा अनुसन्धान गर्न सक्ने अवस्था हुँदाहँुदै पनि त्यसो नभएको बताए । आरोपी एक्लैले उक्त टेप बनाउने ? एडिटिङ गर्ने र वेबसाइटमा हाल्नेसम्मको काम गर्न नसक्ने अवस्था रहेको र उक्त कार्यमा कम्तीमा तीनचार जनाको समूह लागिपरेको निश्चित हुँदाहँुदै पनि जहाँगिर एक्लैलाई अभियुक्त बनाउनु प्रहरी अनुसन्धानमा रहेको कमजोरीको प्रमाण भएको उनको आरोप छ । प्रहरी उपरीक्षक राजेन्द्रमान श्रेष्ठ भने यो मुद्दा हालसम्मकै अत्यन्त नौलो र जटिलसमेत भएकाले प्रहरीले अत्यन्त गम्भीरतापूर्वक लिएको जिकिर गर्छन् । उनले व्यापक अनुसन्धानपछि मात्र प्रहरीले सरकारी वकिलको कार्यालयमा मुद्दा पेस गरेको बताए । यता जिल्ला सरकारी वकिलको कायर्ालय जहाँगिरलाई धरौटीमा छाडने जिल्ला अदालतको आदेशविरुद्ध पुनरावेदन अदालत हेटौंडामा पुनरावेदन गर्ने तयारीमा जुटेको कार्यालय स्रोतले बताएको छ । विवाहअघिको प्रेमसम्बन्ध र त्यस क्रममा भएका गतिविधिका कारण अनपेक्षित नतिजा बेहोरेकी रमा हाल बिस्तारै उक्त घटनाजनित सामाजिक अपहेलना ? लाञ्छनाबाट आफूलाई सम्हाल्ने क्रममा छिन् । घरबाट बहिष्कृत भैसकेकी उनले अकल्पनीय घटना बेहोरेको र आफ्ना कारण माइती पक्ष पनि ठूलो पीडाको सिकार भएकाले आफू अत्यन्त दुःखित भएको बताएकी छिन् । यद्यपि रमाले पहाडजस्तो लामो जीवनलाई त्यत्तिकै घरमा बसेर खेर नफाल्ने मनस्थिति बनाएकी छिन् । विवाहका कारण आफ्नो उच्च शिक्षा अवरुद्ध भएकाले अब उच्च शिक्षालाई निरन्तरता दिने र स्वावलम्बी बन्न चाहेको उनी बताउँछिन् ? तर यो कुरा समाजले सकारात्मक सहयोग र समर्थन गरिदिए मात्र सम्भव भएको कुरा उनलाई थाहा छ । कुनै सहृदयी संघसंस्थाले आफूलाई पुनर्जीवन दिने पहल गरे आफू त्यसका लागि तयार रहेको पनि उनको भनाइ छ । टेप प्रकरणको पात्र आफू एक्लै नभई आफूलाई बदनाम गर्ने युवक पनि उत्तिकै जिम्मेवार भएकाले पितृसत्तात्मक समाजले आफूलाई मात्र दोषी बनाइरहेको उनको गुनासो छ । समाजमा यस्ता क्रियाकलाप गर्ने पुरुष पनि उत्तिकै दोषी हुनुपर्ने र उसको सामाजिक बहिष्कार गर्न सक्नुपर्ने उनको भनाइ छ । नेपाली हिरो त्यसै बलियो भएको होइन । ऊ सुत्नअघि नियमित दूध पिउँछ । उसलाई कि आमाले दूध पिलाउँछ कि हिरोइनले । उसले दूध सधैं पारदर्शी गिलासमा पिउँछ । हिरोइन वा आमा पात्रले पारदर्शी गिलासमा दूध लिएको दृश्य छ भने ? त्यो हिरोकै लागि हो भने बुझे हुन्छ । यसअघि एक गायक तथा हास्यकलाकारको यस्तै खाले टेप सार्वजनिक हुँदा हालको ऐन बनिनसकेकाले उनलाई गाली बेइज्जती ऐनअनुसार कारबाही चलाइएको अधिवक्ता कर्ण बताउँछन् । त्यसैगरी चर्चित दुई मोडल तथा नायिका यस्तै खालका दुई फरक प्रकरणमा चर्चामा आए पनि उनीहरूले कुनै कानुनी उपचार खोजेनन् । वीरगन्जका लागि पनि यस्तो घटना पहिलो भने होइन । यसअघि पनि वीरगन्ज स्थानीय घर भएका एक अन्र्तराष्ट्रिय खेलाडीको भिडियो क्लिप्स सार्वजनिक भएको थियो । स्थानीय एक महिलासँगको उनको उक्त टेपले पनि आगाको झिल्कासरह चर्चा बटुलेको थियो । आधुनिकतासँगै बढ्दो साइबर अपराध यहाँका युवा पुस्ता र उनीहरूका अभिभावकका लागि पनि चिन्ताको विषय बन्दै गएको छ । फलस्वरूप साइबर अपराधसम्बन्धी कानुनलाई अझ बढी प्रभावकारी र बलियो बनाउनुपर्ने धारणा अधिकांश अभिभावकको छ । दुर्घटना राजमार्गमा मात्र हुँदैन । नेपाली फिल्मको कलेजमा पनि हुन्छ । त्यहाँ हिरोहिरोइनले एकआपसमा ठक्कर खाइरहन्छन् । धेरै फिल्ममा यस्तो दुर्घटना भइसकेको छ । हिरोहिरोइन ठोक्किन्छन् र हिरो वा हिरोइनका किताब भुइँमा छरपस्टिन्छ । ओह सरी भन्दै किताब टिप्ने काम हुन्छ । त्यतिबेला फेरि टाउको पनि ठोक्किनसक्छ । हिरोहिरोइन एकअर्कालाई देख्दा मुग्ध भएर टोलाउन थाल्छन् । दायाँबायाँ कोको छन् उनीहरूलाई मतलब हुँदैन । एकनाशले आँखा जुधाउँदै हराउँछन् । सँगैका साथीहरूले उनीहरूका आँखाअगाडि हत्केला हल्लाउँदा पनि ध्यान भंग हुँदैन । गाउँकी हिरोइन यति निर्धक्क हुन्छे कि नुहाउनलाई ऊ जंगलबीचको अप्ठ्यारो झरनामा पुग्छे । घरमै नुहाउन उसलाई मन लाग्दैन । किनभने त्यसो गर्दा उसको जवानी प्रदर्शन हुँदैन । जसरी हुन्छ ? सहरबाट आएको हिरोले देख्ने गरी उसलाई झरनामा गएर नुहाउनैपर्छ । हिरोइनको जवानीमाथि भिलेनको गिद्धेनजर पर्छ । उसले निकै प्रपञ्च रच्दै हिरोइनलाई कब्जामा लिन्छ । त्यतिबेला हिरोइनले सल भएको लुगा अनिवार्य लगाएकी हुन्छे । भिलेनले सललाई तानेर हावामा हुर्र्याइदिन्छ । सल हावामा तैरिँदै झर्छ र हिरोको हातमा पर्छ । त्यसपछि रामधुलाइ । कुनै जटिल रोगले वा गोली लागेर घाइते भएका पात्रले सबैजना भेला भएको बेला लामो संवाद बोल्न थाल्यो भने निश्चित भए हुन्छ ? ऊ मर्दैछ । अरू पात्रहरू पनि एम्बुलेन्स बोलाएर अस्पताल लैजानुको सट्टा त्यो संवाद सुनेर बस्छन् ? मानौ प्राणभन्दा पनि कुरा फुत्किएला भन्ने चिन्ता उनीहरूलाई छ । हिरोहिरोइनको समागमको दृश्य देखाउन नेपाली फिल्मले पुरानै विम्ब प्रयोग गर्न छोडेको छैन । ओछ्यानमा पल्टन्छन् र बत्ती निभाउँछन् । बाँकी कुरा अँध्यारोमा । नयाँ प्रतीकात्मक दृश्य सिर्जना गर्ने जाँगर किन नपलाएको हो ? बुझ्न गाह्रो छ । खोलामा खस्दा लास बेपत्ता हुनुपर्ने हो । तर ? नेपाली फिल्ममा त्यस्तो हुँदैन । खोलामा खसेको पात्र हतपत मर्दैन ? उसलाई कुनै अनजान ठाउँका मान्छेहरूले उद्धार गरिदिन्छन् । बरु ? उसको स्मृति चाहिँ गुम्नसक्छ । त्यसरी गुमेको होश फिर्ता हुन डाक्टर चाहिँदैन ? त्यस्तै कुनै ठूलो घटना घटे पुग्छ । कुनै गाउँले पात्र सहर आयो भने उसलाई सबैभन्दा बढी समस्या केमा हुन्छ ? नेपाली फिल्म हेर्नुस् ? यसको जवाफ पाइनेछ । गाउँलेलाई सहरमा बाटो काट्न निकै समस्या हुन्छ । गाउँलेहरूकै लागि भनेर सहरमा आकाशे पुल बनाइएको हो कि जस्तो लाग्छ । गरिबलाई पैसा अभावले पक्कै सताउँछ । पैसा खट्टे पर्ने कारण थुप्रै हुनसक्छन् । नेपाली फिल्ममा भने अपरेसन गर्न गरिब हिरोलाई पैसा खाँचो परेको देखाइन्छ । हिरोको कोही आफन्त इमरजेन्सीमा भर्ती हुन पुग्छ र डाक्टरले हजारौं वा लाखौं रकम तुरुन्त ल्याएमात्र अप्रेसन गर्छु भन्छ । नेपाली फिल्मका सामन्तीले ऋण असुल गर्ने तरिका अनौठो छ । भाका नाधेपछि ऊ आफ्नो दलबलसहित ऋणीको घर पुग्छ र केटाहरूलाई भाँडाकुडा बाहिर फ्याँक्न हुकुम दिन्छ । ऋणीले त्यतिबेला निकै रोइकराइ गर्छ ? खुट्टा ढोग्छ । तर ? उसको मन पग्लिदैन । सायद यस्तो बिचल्लीमा रमाउने आदत उसलाई लागिसकेको छ । आफ्नो प्रिय व्यक्तिको मृत्यु देख्दा वा त्यसबारे सुन्दा नेपाली फिल्मका पात्रहरू शिथिल बन्नपुग्छन् । त्यतिबेला उनीहरूका हातमा भएका गिलास ? किस्ती ? औषधी वा पैसा सबै खस्छ । नेपाली फिल्ममा यस्तो दृश्य खिचिँदा कति सिसी फुटे ? दूध पोखे वा औषधी बगे कुनै लेखाजोखा छैन । फिल्ममा कुनै प्रिय पात्रको निधन हुँदा रुवाबासी चल्छ । वरिपरि झुम्मिएकाहरूले आआफ्नै पाराले प्रतिक्रिया व्यक्त गर्छन् । प्रहरीले भने अफसोच मान्दै टोपी खोल्छ । श्रद्धा गर्नुपर्ने मान्छे मर्दा प्रहरीले टोपी खोल्छ भन्ने सायद धेरै जसोले नेपाली फिल्म हेरेरै थाहा पाएका होलान् । हिरोलाई आफ्नो मुठ्ठीमा ल्याउन भिलेनले प्रयोग गर्ने अन्तिम उपाय हो ? अपहरण । हिरोइन वा हिरोका परिवारलाई अपहरण गरी आफ्नो अखडामा बाँधिदिएपछि हिरो जसरी पनि आइपुग्छ भन्ने विश्वास भिलेनको हुन्छ । नभन्दै हिरो निरीह बन्दै आइपुग्छ । समाजमा भन्दा बढी अपहरण हुन्छ नेपाली फिल्ममा । विष खुवाएर हिरोलाई मार्नु छ भने फिल्ममा दूधकै सहारा लिइन्छ । षड्यन्त्रकारीले खुसुक्क दूधमा विष मिसाइदिन्छ । तर ? हिरोको भाग्य कति दह्रो हुन्छ भने त्यस्तो दूध कसोकसो गरी भुइँमा पोखिनपुग्छ । र ? अचानक बिरालो आएर खाइदिन्छ । मान्छेको विष बिरालोले खाएको दृश्य कस्तो हुन्छ ? नेपाली फिल्म खोजीखोजी हेर्नुहोला । सासु र नन्द मिलेर षड्यन्त्र बुन्दै गरेको बुहारीले सुन्छे । एउटा पात्र क्यान्सर रोगी हुन्छ र उसको केही महिनापछि मृत्यु निश्चित छ । तर ? यो कुरा उसलाई कसैले भन्दैन । अरूले अर्कै कोठामा कुरा गर्दा उसले सुन्न पुग्छ । यसरी सुन्न नहुने वा सुनाउन नखोजिएको कुरा नेपाली फिल्मका पात्रहरूले सुटुक्क सुनिहाल्छ । हिरो र भिलेन पक्षबीच फाइट भइरहँदा निर्देशकलाई हिरोइन र अरू नारी पात्र झन्झट हुनपुग्छ । यो झन्झटबाट मुक्त हुने तरिका पनि उनीहरूले जानेका छन् । भिलेनसँग झागलझुगल हुँदा हुत्तिएर ढुंगा वा भित्तामा ठोक्काई बेहोश बनाइदिने । त्यसरी बेहोश भएका नारी पात्रहरू फाइट सिद्धिएपछि मात्र ब्युँझिन्छे । हिरोलाई कसैको हत्याको गलत आरोप लागेको हुन्छ । हिरोइनलाई पनि चोरीको आरोप लाग्छ । हिरोको बुबालाई कसले हत्या गरेको हो ? थाहा हुँदैन । यो कुराले हिरोलाई पनि आपत परेको हुन्छ । तर ? अन्तिममा हिरोलाई भिलेनले नै आरोपमुक्त बनाइदिन्छ । हिरोको सबै खान्दान खत्तम हुने भो भन्दै उसले सबैका अगाडि आफ्नो अपराध एकपछि अर्को गर्दै ओकलिदिन्छ । कुनै पात्र मर्नु छ वा पश्चातापको आगोमा जलेको छ भने उसको मृत्यु बडो नाटकीय हुन्छ । हिरो ? हिरोइन वा अरू कुनै महत्वपूर्ण पात्रमाथि प्रहार गरिएको गोली आफ्नो छातीमा थाप्दै उसले मृत्युवरण गर्छ । त्रिकोणात्मक प्रेमकथा भयो भने एउटा जगेडा पात्र हिरो होस् वा हिरोइन ? उसको नियति यस्तै हुने सम्भावना बढी छ । हिरो र भिलेनबीच भीषण लडाइँ हुन्छ । दुवै पक्षका थुप्रै पात्रको निधन हुन्छ । अन्तिममा हिरोले मुख्य भिलेनलाई मार्न लागेको बेला प्रहरी आइपुग्छ । भुइँमा लडेका भुसतिघ्रेहरू र भिलेनलाई पत्रे्कर लैजान्छ । प्रहरीमाथि जनविश्वास घट्नुको कारण फिल्मको यही पाराले पनि हो कि',\n",
       " 'भद्रपुर । स्थानीय तहमा वृद्धि गरिएको करको विरोधमा बुधबार झापाको मेचीनगरमा प्रदर्शन गरिएको छ । नेपाल कम्युनिष्ट पार्टी वडा नं १० को अगुवाइमा भएको विरोध कार्यक्रममा सहभागीहरूले मेचीनगरको मुख्यद्वारमा दुई घण्टा धर्ना दिनुका साथै नाराबाजीसहित नगरपालिका र सम्बन्धित वडा कार्यालयमा ध्यानाकर्षण पत्र बुझाएका थिए । अवैज्ञानिकरूपमा नगरवासीको ढाड सेक्ने गरी नगरपालिकाले गरेको कर वृद्धि तत्काल फिर्ता  ',\n",
       " 'परिक्षामा चिट चोर्ने मामिलामा केटीहरू अति नै अगाडि ? यस्ता स्थानमा यसरी लुकाउछन् चीट हेर्नुहोस् फोटोसहित',\n",
       " 'चर्चित रियालिटी शो टिभिएस अपाचे द भ्वाइस अफ नेपाल  बाट बाहिरिएकी बर्त गन्धर्व र श्रीमानबीच यस्तो प्रेमभिडियो अन्तरवार्ता',\n",
       " 'यी हुन् विश्वका सबैभन्दा महँगा विवाह गर्ने ५ ब्यक्तिहरु ? जसले बगाए पानी सरि पैसा  अवस्य चिन्नुहोस्',\n",
       " 'क्यान्सरको उपचार गराईरहेकी अभिनेत्री सोनाली बेन्द्रेले विवाहको १६ औँ वर्षगाठमा आफ्नै पतिलाई न्यूयोर्कबाट लेखिन यस्तो भावुक चिठी',\n",
       " 'प्रधानमन्त्री ओलीले देवकोटाका परिवारसँग कुरा गरेर उपयुक्त मूल्यमा भग्नावशेषको अवस्थामा रहेको सो घर खरिद गरी रोक्टोफिटिङ गरेर देवकोटा कसरी पलेटी कसेर लेख्थे भन्नेसम्मका स्मृति झल्काउन सरकारले सङ्ग्रहालय बनाउने उद्घोष गर्दा सहभागी स्रष्टाहरुले तालीले स्वागत गरेका थिए । नेपालका प्रधानमन्त्री केपी शर्मा ओलीले महाकवि लक्ष्मीप्रसाद देवकोटाको काठमाडौँ डिल्लीबजार ? धोबीधारास्थित जन्मघर किनेर सङ्ग्रहालय बनाउने घोषणा गर्नु भएको छ । प्रधानमन्त्री ओलीको अग्रसरतामा महाकवि देवकोटाको ११०औँ जन्मजयन्तीका अवसरमा मदन भण्डारी कला ? साहित्य प्रतिष्ठानले हिजो बुधबार प्रधानमन्त्री निवास बालुवाटारमा आयोजना गरेको कार्यक्रममा उहाँले देवकोटा विलक्षण प्रतिभा र बौद्धिक व्यक्तित्व भएकाले उहाँको कृतित्व र व्यक्तित्वबारे भावी पुस्तालाई जानकारी दिनका लागि सङ्ग्रहालयको आवश्यकता रहेको बताउनु भएको राष्ट्रिय समाचार समितिले लेखेको छ । महाकविको उक्त जन्मघर हाल जीर्ण अवस्थामा छ भने नजिकैको नयाँ घरमा उनका छोरा डा पदमप्रसाद देवकोटा बस्दै आउनु भएको छ । विसं १९६६ कात्तिक २७ गते लक्ष्मीपूजाका दिन जन्मिएका र विसं २०१६ भदौ २९ गते निधन भएका महाकावि देवकोटाका मुनामदन ? सुलोचना ? शाकुन्तल पहाडी पुकार लगायत दर्जनौँ लोकप्रिय कृति प्रकाशित छन् । उहाँ पूर्व शिक्षामन्त्रीसमेत हुनुहुन्छ । गतसाता छातिमा संक्रमण भएर उपचारार्थ अस्पताल भर्ना भई निवास फर्केपछि पहिलो पटक सार्वजनिक मन्तव्य दिनु भएका प्रधानमन्त्री ओलीले महाकवि देवकोटाका विभिन्न कविता सुनेपछि भन्नुभयो ? देवकोटाजस्तो बौद्धिक ज्ञानयुक्त विलक्षण कवि मैले विश्वमै देखेको छैन । उहाँ प्राकृतिक प्रतिभा हो ? मूलबाट फुटेर खसेको झाँगोजस्तो हुनुहुन्थ्यो । ',\n",
       " '२८ जेठ ? गौर । रौतहटको कटहरीया नगरपालिका ९ बाट गत शनिवार राति अपहरणमा परेका १३ वर्षीय बालक युवराज साहको गोली हानेर हत्या गरेको अवस्थामा शव फेला परेको छ । कटहरीया ९ डुमरिया टोलका चिकित्सक दिलिप साहको छोरा युवराज शनिवार राति घरमा सुतिरहेकै अवस्थामा हराएका थिए । परिवारका सदस्यहरुसँगै सुतेका युवराजलाई अज्ञात समूहले ओछयानबाटै लगेका थिए । कटहरीया नगरपालिकाभन्दा १५ किलोमिटर टाढा गढीमाई नगरपालिका ५ को बाग्मती नदी किनारमा आज बिहान बालकको शव फेला परेको प्रहरीले जनाएको छ । जिल्लाका विभिन्न क्षेत्रमा बाढी बसेर तथा कृषिउपज डुबानमा परेर सो नोक्सानी भएको हो । सूर्यपूरा गाविसमा रहेको अस्ट्रिच फर्ममा रोहिणी नदीको बाढी पसेपछि ८० भन्दा बढी अस्ट्रिचका छ महिना नाघेका बच्चा बगाएको अस्ट्रिच नेपाल प्रालिका सञ्चालक सिपी शर्माले बताए । बाढीले फर्मको चार बिगाहा जमिन र तारबार कटान गरेको छ । दाङमा पालेका बयस्क अस्ट्रिच ३० वटाभन्दा बढीको मृत्यु भएको कम्पनीले जनाएको छ । बाढीले जिल्लाको करिब १२ हजार ५१९ हेक्टर खेतीयोग्य जमिन डुबानमा परेको छ । तिनाउ ? दानव र रोहिणी नदीमा आएको बाढीले ४० हेक्टर धान खेती नष्ट गरेको छ भने १२ हजार ४६० हेक्टरमा लगाइएको धानबाली डुबानमा परेको जिल्ला कृषि विकास कार्यालयका निमित्त प्रमुख आश्विनी शर्माले बताए । जिल्लामा रु ७५ लाख मूल्य बराबरको धानबाली ? तरकारी रु १५ लाख ? केरा रु दुई लाख ५० हजार ? माछा रु ५० लाख ? मैरीको मह र घार रु ११ लाख ७० हजार तथा बदाम रु १० लाख ८० हजार तथा पशुतर्प करिब रु ३५ लाख ८३ हजार मूल्य बराबरको बाख्रा क्षति भएको पशुसेवा कार्यालयले जनाएको छ । रासस',\n",
       " 'काठमाडौं चैत १ । नेपाली कांग्रेसको भातृ संगठन नेपाल लोकतान्त्रिक खेलकुद संघले नेपाली कांग्रेसका नवनिर्वाचत सभापति एवं पूर्व प्रधानमन्त्री शेरबहादुर देउवालाई सोमबार बधाई तथा शुभकामना प्रदान गरेको छ । ',\n",
       " 'एजेन्सी ? चैत १ । बंगलादेश विश्वकप ट्वेन्टी ट्वेन्टी क्रिकेट प्रतियोगिताको सुपर टेनमा प्रवेश गरेको छ । समुह ए अन्तर्गतको अन्तिम लिग खेलमा ओमानलाई ५४ रनले हराउँदै बंगलादेशले सुपर टेनमा आफ्नो स्थान पक्का ',\n",
       " 'एजेन्सी ? चैत १ । आर्सनललाई हराउँदै वाट्फोर्ड एफए कप फुटबल प्रतियोगिताको सेमिफाइनलमा प्रवेश गरेको छ । आइतबार राति आर्सनलको मैदानमा भएको खेलमा पाहुना टोली वाट्फोर्ड २१ ले विजयी भयो । खेलमा वाट्फोर्डलाई',\n",
       " 'एजेन्सी ? चैत १ । स्पेनिस ला लिगा रियल मड्रिडले लास पालमसविरुद्ध कठिन जित हात पारेको छ । आइतबार राति भएको खेलमा रियल मड्रिडले लास पालमसलाई २१ ले हरायो । रियलको लागि सर्जियो रामोस र कासेमिरोले गोल गरे ',\n",
       " 'काठमाडौं ? फागुन ३० । नेपाली राष्ट्रिय फुटबल टिमका मुख्य प्रशिक्षक ग्योटुकी कोजीले मलेसियासँगको खेलका लागि ४४ खेलाडीलाई प्रशिक्षणमा बोलाएका छन् । कोजीले आगामी चैत्र १४ मा मलेसियाको यु २२ टोलीसँग हुने मै',\n",
       " 'एजेन्सी ? फागुन ३० । चेल्सीलाई हराउँदै एभर्टन एफ । ए कप फुटबल प्रतियोगिताको सेमिफाइनलमा पुगेको छ । घरेलु मैदान गोडिन पार्कमा शनिबार राति भएको क्वाटरफाइनलमा चेल्सीलाई २० ले हराएर एभर्टनले सेमिफाइलनमा स्था',\n",
       " 'एजेन्सी ? फागुन ३० । स्पेनिस ला लिगाको शीर्ष स्थानमा रहेको बार्सिलोनाले विजयी यात्रा कायम राखेको छ । ला लिगा अन्तर्गत शनिबार राति भएको खेलमा बार्सिलोनाले गेटाफेमाथि ६० को जित निकालेको हो । बार्सिलोनाल',\n",
       " 'एजेन्सी ? फागुन ३० । जिम्बाबेलाई हराउँदै अफगानिस्तान ट्वेन्टीट्वेन्टी विश्वकप क्रिकेटको दोस्रो चरण अर्थात सुपर टेनमा पुगेको छ । शनिबार भएको खेलमा जिम्बाबेलाई ५९ रनले हराउँदै अफगानिस्तानले सुपर टेनमा पु',\n",
       " '२९ फागुन ? काठमाडौं । टोलीका युवा स्टारद्धय फरवार्ड अन्जन बिष्ट र रक्षक अनन्त तामाङ ट्रायलका लागि आज नेदरल्याण्डस् जाने भएका छन् । नागरिक दैनिकमा समाचार छ । स्पेनको मावैला युनाइटेड एफसीमा ट्रायलको अ',\n",
       " '२९ फागुन ? एजेन्सी । जारी आइसिसी ट्वेन्टी ट्वेन्टी विश्वकप क्रिकेट भन्दा बढी चर्चामा रहेको पाकीस्तानी टोलीको भारत यात्रा तय भएको छ । पाकिस्तान सरकारले आफ्नो क्रिकेट टिमलाई प्रतियोगितामा सहभागी हुन भा',\n",
       " 'काठमाडौं ? फागुन २८ । नेपालले मलेसियासँग दुई मैत्रिपूर्ण फुटबलको खेल मिति परिर्वतन भएको छ । यसअघि चैत १६ मा हुने भनिएको मलेसियाको राष्ट्रिय टोलीसँगको मैत्रिपूर्ण खेलको मिति परिवर्तन भएको हो । मिति परिवर',\n",
       " 'एजेन्सी ? फागुन २८ । युरोपा लिग फुटबल प्रतियोगिता लिभरपुलले म्यानचेस्टर युनाईटेडलाई २० ले हराएको छ । गएराति भएको खेलमा युरोपियन क्लब फुटबलको दोस्रो ठूलो प्रतियोगिता युरोपा लिग अन्तर्गत प्रि क्वाटरफाइन',\n",
       " 'एजेन्सी ? फागुन २७ । पेसिर सेन्ट जर्मेन पिएसजी च्याम्पियन्स लिग फुटबलको क्वाटर फाइनलमा प्रवेश गरेको छ । बुधबार राति आफ्नै मैदानमा भएको दोस्रो लेगको खेल २१ ले हारेको चेल्सी समग्रमा ४२ ले पराजित हुँदै प',\n",
       " 'सिमरा ? फागुन २६ । त्रिभुवन आर्मी क्लबलाई सडन डेथमा स्तब्ध पार्दै विजय युथ क्लब हेटौडा सिमरा गोल्डकपको फाइनलमा प्रवेश गरेको छ । सिमरा रंगशालामा बुधबार भएको सेमिफाइनल खेलमा आर्मीविरुद्ध सडन डेथमा नतिजा',\n",
       " 'काठमाडौं ? फागुन २६ । ट्वेन्टी ट्वेन्टी आइसीसी वर्ल्डकप प्रतियोगिताको भारत र पाकिस्तानबीचको खेल स्थान परिवर्तन गरिएको छ । खेल हुने स्थलमा सुरक्षाको कारण देखाउँदै यसअघि धर्मशालामा हुने भनिएको खेल अब कोलकत',\n",
       " 'काठमाडौं ? फागुन २६ । आर्सनल एफए कप फुटबल प्रतियोगिताको क्वाटरफाइनलमा प्रवेश गरेको छ । मंगलबार राति हल सिटीको मैदानमा हल सिटीलाई ४ गोल हान्दै आर्सनल क्वाटरफाइनलमा प्रवेश गरेको हो । पाँचौ चरणको रि प्ले',\n",
       " 'काठमाडौं ? फागुन २६ । स्पेनिस क्लव रियल मड्रिड च्याम्पियन्स लिग फुटबलको क्वार्टरफाइनलमा प्रवेश गरेको छ । मंगलबार राति भएको च्याम्पियन्स लिगको अन्तिम १६ को दोस्रो लेगमा रियल मड्रिडले रोमालाई २० ले हरा',\n",
       " 'काठमाडौं ? फागुन २५ । भरतमा आजवाट सुरु भएको आइसिसी ट्वेन्टीट्वेन्टी विश्वकप क्रिकेटमा जिम्बावेले बिजयी सुरुवात गरेको छ । ओपनिङ खेलमा आज जिम्बावेले हङकङलाई १४ रनले हराएको छ । जिम्बावेले दिएको १५९ रनको ल',\n",
       " 'एजेन्सी ? फागुन २५ । आइसिसी ट्वेन्टीट्वेन्टी विश्वकप क्रिकेट आजदेखि भारतमा सुरु हुँदैछ । प्रतियोगिता अन्तर्गत मंगलबारदेखि पहिलो चरणको खेल हुदैछ । अन्तर्राष्ट्रिय क्रिकेट परिषद आइसिसीको आयोजनामा हुन लाग',\n",
       " 'एजेन्सी ? फागुन २४ । स्पेनिस ला लिगाको शीर्ष स्थानमा रहेको बार्सिलोनाले ८ अंकको अग्रता कायमै राखेको छ । आइतबार राति भएको खेलमा एइबरलाई ४० ले हराउँदै बार्सिलोनाले एथ्लेटिको मड्रिडसँगको अग्रता कायमै राखे',\n",
       " 'एजेन्सी ? फागुन २४ । इंग्लिस प्रिमियर लिग फुटबल अन्तर्गत आइतबार राति भएको खेलमा म्यानचेस्टर युनाइटेड पराजित हुँदा १० खेलाडीमा सिमित लिभरपु भने विजयी भएको छ । म्यानचेस्टर युनाइटेड वेष्ट ब्रोमसँग १० ले',\n",
       " '२४ फागुन ? एजेन्सी । एशिया कपको उपाधि भारतले जितेको छ । बंगलादेशलाई उसकै घरेलु मैदानमा ८ विकेटले हराउँदै गएराती भएको खेलमा भारतले एशिया कप क्रिकेटको उपाधि जितेको हो । फाइनलमा भारतले बंगलादेशलाई ८ व',\n",
       " '२४ फागुन ? चितवन । जारी पद्मोदय अन्तर्राष्ट्रिय पुरुष भलिबल प्रतियोगिताको दोस्रो दिन आज ४ खेल हुँदैछन् । चितवनको पटिहानीमा जारी खेलमा दुई समूहमा विभाजित ६ वटा टिमले दिनभर समूह चरणमा ४ खेल खेल्दैछन् । ',\n",
       " 'एजेन्सी ? फागुन २३ । एशिया कप क्रिकेटको उपाधि भिडन्त आज हुदैछ । फाइनलमा आयोजक बंगलादेशले भारतको सामना गर्दैछ । यी दुई देश बीच एशियाकपको फाइनलमा पहिलो पटक भिड्न लागेका हुन् । खेल मीरपुरमा नेपाली समयअनु',\n",
       " 'एजेन्सी ? फागुन २३ । इंग्लिस प्रिमियर लिग फुटबलमा शिर्ष स्थानमा रहेको लेष्टर सिटीले अग्रतालाई फराकिलो पारेको छ । शनिबार राती भएको खेलमा वाट्फोर्डलाई १० ले हराँउदै लेष्टरले अग्रता फराकिलो पारेको हो । त',\n",
       " 'अमेरिकाले उत्तर कोरियाली नेता किम जोङ उनको हत्या योजना बनाएको खुलासा भइरहँदा उत्तर कोरियाले भने अमेरिकामाथि अहिलेसम्मकै खतरनाक आक्रमणको योजना बनाएको खुलासा भएको छ । अमेरिकी गुप्तचर अधिकारीहरुले उत्तर कोरियाले अन्तरिक्षबाटै अमेरिका ? जापान सहित का आफु बिरुद्ध लाग्ने देशहरुमाथि आणविक आक्रमण गरेको दाबी गरेका छन् । कोरियाले इलेक्ट्रोनिक पल्स हतियारलाई अमेरिकाविरुद्ध प्रयोग गर्ने सम्भावना रहेको भन्दै अमेरिकी गुप्तचर अधिकारीहरुले उत्तरले यसको तयारी गरिरहेको दाबी समेत गरेका छन् । उत्तर कोरियाले अन्तरिक्षमा दुई उपग्रह पठाएको र अन्तरिक्षबाटै अमेरिका र जापानमाथि निगरानी गरिरहेको पाइएको उल्लेख गरेका छन् । स्मरणीय छ उत्तर कोरियाकाले सन् २०१२ र २०१६ मा अन्तरिक्षमा दुई उपग्रहको सफल प्रक्षेपण गरेको थियो । जुन अत्याधुनिक अन्तरिक्ष उपग्रहले केवल ९४ मिनेटमा पृथ्वीलाई एक फन्को लगाउन सक्ने क्षमता राख्दछ । अमेरिकी गुप्तचर अधिकारीहरुले यहि उच्च क्षमताको उपग्रहकै मद्दतबाट उत्तर कोरियाले अन्तरिक्षमै हाइ अल्टिच्यूड परमाणु हतियार विस्फोट गराएर अमेरिकामाथि साइबर हमला गर्न सक्ने देखिएको बताएको छ । उत्तर कोरियाले सबै संचार उपकरणलाई ध्वस्त बनाएर हावी हुने प्रयास स्वरुप अन्तरिक्षमा परमाणुयुक्त यान विकास गरेको अमेरिकाको होमल्याण्ड सेक्युरिटी र नेशनल इलेक्ट्रोनिक्स पल्स टास्क फोर्सका कार्यकारी निर्देशक डा पिटर विनसेन्ट प्राइलाई उदृद्ध गर्दै विभिन्न अन्र्तराष्टिय समाचार संस्थाहरुले उल्लेख गरेका छन् । पाल्पाका महिलाहरु हस्तकलाबाट आत्म निर्भर बन्दै गएका छन् । हात ? सिप र कलाको प्रयोग गरी बच्चाका लागि खेलौना ? गुडिया ? घर ? कार्यालय सजावटका लागि कुसन ? चुरा ? ऐना ? फूल लगायका सामग्रीहरु व्यवसायिक रुपमा उत्पादन गर्न थालेपछि जिल्लाका महिलाहरु आत्मनिर्भर बन्दै गएका हुन् । हातले बनाएका सामाग्री पाल्पा घुम्न आउने पर्ययटकको रोजाई बन्दै गएको छ । पाल्पामा मात्रै दुई दर्जन बढी महिलाहरुले हस्तकलाको पेसालाई व्यवसायिक रुपमा संचालन गरेका छन् । दोभान२ पाल्पाकी हरिकला थापा मगरले हस्तकलाबाटै अहिले मासिक ४० देखि ५० हजारसम्म बचत हुने गरेको बताउनुभयो । झण्डै ४ सय बढीलाई तालिम दिनुभएका हरिकलाले आफ्ना उत्पादित सामग्रीहरु जिल्लामैै बढी खपत भएको बताउनु भयो । सुरुमा समय र लगानीको जोखिम रहेको बताउने हरिकला भन्नुहुन्छ ? अहिले ग्राहकको समस्या छैन ? ढुक्कले लगानी गरेका छौ । घरेलु सामानको उत्पादन र बिक्रीबाट राम्रो कमाई गर्नुभएकी तानसेन वडिज्ञान टोलकी देवी सारुले दुई चार पैसाका लागि परिवारसग हात थाप्नु नपर्ने बताउनुहुन्छ । हस्तकलाको तालिम पछि व्यवसायिक रुपमा सिपको प्रयोग हुन थालेपछि मासिक २०२५ हजार आम्दानी गर्न सफल सारुले पहिले गफ गरेर बिताउने समय अहिले नया नया सिप र प्रबिधिको बिकासमा समय जाने बताउनुभयो । कला र सिपबाट कम खर्चमै आकर्षक सजावटका सामाग्रीबाट नगद आर्जन हुने भएकाले पनि हस्तकलाका सामग्री उत्पादन र बिक्रीमा महिलाको आकर्षण बढ्दो छ । महिलालाई व्यवसायिक र अत्मनिर्भर बनाउन हस्तकला ? सिलाइकटाइ ? व्युटिपार्लर ? ढाका ? मोवाइल मर्मत लगायतका तालिमहरु संञ्चालन गर्दै आएको बताउदै घरेलु तथा साना उद्योग कार्यालय पाल्पाका प्रमुख बालकृष्ण गैरेले हस्तकलाको उत्पादन र प्रयोगमा पनि बृद्धि आएको बताउनुभयो । आर्थिक अवस्था कम्जोर भएका महिलाहरुका लागि मेशिन लगायतका आवश्यक औजार समेत वितरण गरेको जानकारी दिदै कार्यालय प्रमुख गैरेले हस्तकलाको व्यवसायिक उत्पादन र बिक्रीमा ध्यान दिनुपर्नेमा जोड दिनुभयो । नेपालमा सन् २००१ देखि विद्युतिय कार्ड भूक्तानी संचालन गर्दै आएको स्मार्ट च्वाइस टेक्नोलोजिस एससीटीले एससीटी मोको मोवाइल एप्लिकेशन सुरु गरेको छ । आइतबार राजधानीमा पत्रकार सम्मेलन आयोजना गरि आफ्ना ग्राहकहरुको सहजताका लागि फोकसवन पेमेन्ट सोलुसनसँगको सहकार्यमा सेवा सुरु गरेको जानकारी गराइएको हो । एसिटी कार्ड प्रयोगकर्ताहरुले आइफोन ? र एन्ड्रोइड स्मार्टफोन मार्फत विभिन्न साइट वा अफ साइट वित्तिय कारोवार गर्न सक्ने कम्पनीले जनाएको छ । एप्लिकेशनमा इन्टरनेट प्रयोग गरिने हुँदा एसएमएसको आवश्यकता नर्पनुका साथै संसारको कुनै पनि कुनामा यो सेवा प्रयोग गर्न सकिने छ । एससिटी नेटवर्कमा ७८ बैंक तथा वित्तिय संस्थान आवद्ध छन् । देशको विभिन्न जिल्लामा गरि १६ सय एटीएम र तीन हजार पिओएस सेवा उपलब्ध छन् । हङकङको एप्पल न्युजका अनुसार जोर्डनस्थित वुसुङ्ग स्ट्रीटमा शुक्रवार बिहान तीन नेपाली आपसमा भिडेका हुन् । १६ ? २१ र २२ वर्षीय नेपालीबीच छुरा हानाहान गरेको देखेपछि बटुवाले प्रहरीलाई खबर गरेका थिए । घाइतेको अस्पतालमा उपचार भइरहेको छ । समाचारका अनुसार हात ? टाउको र घाँटीमा छुराको चोट लागेको छ । फोटो  भिडियो सौजन्यः हङकङ एप्पल नेस्टमिडिया',\n",
       " 'उपभोक्तालाई ठगेको आरोपमा २० दिनअघि सिल गरिएका दरबारमार्गका विभिन्न व्यवसायिक फर्मलाई बिनाकारबाही छाडिएको भन्दै उपभोक्तावादीले विरोध जनाएका छन् । मन्त्रीको संख्या अत्यधिक भएर जनता आजित भइरहेका बेला संसद्को राज्यव्यवस्था समितिले अब केन्द्र सरकारमा बढीमा १५ वटा मात्रै मन्त्रालय राख्न सरकारलाई निर्देशन दिएको छ । समितिको निर्देशनअनुसार केन्द्र सरकारमा रहेका १६ वटा मन्त्रालय खारेजीमा पर्ने भएका छन् । हाल प्रधानमन्त्री तथा मन्त्रिपरिषद् कार्यालयसहित ३१ वटा मन्त्रालय छन् । यो खबर हामीले आजको ',\n",
       " 'स्थानीय तहमा खटिएका कर्मचारी कार्यालयमा नपुगेपछि उनीहरूको खोजी गर्न सरकारले आइतबार उच्चस्तरीय अनुगमन टोली गठन गरेको छ । प्रधानमन्त्री तथा मन्त्रिपरिषद् कार्यालयले संघीय मामिला तथा स्थानीय विकास मन्त्रालयका सहसचिवको नेतृत्वमा सामान्य प्रशासन मन्त्रालयका उपसचिव र महालेखा नियन्त्रक कार्यालयका उपसचिव सदस्य रहेको अनुगमन टोली गठन गरेको हो । काठमाडौँ ? रेखा कवि टंक सम्बाहाङ्फेको एकल कविता वाचन कार्यक्रम सम्पन्न भएको छ । राजधानीको अनामनगरस्थित मण्डला नाटक घरमा उनले पहिलो पटक एकल कविता वाचन गरेका हुन् । सन् २०२२ मा कतारमा हुने विश्वकपमा नेपाली मूलकी भाग्यमानी एक युवतीले रेफ्री बन्ने अबसर पाउने भएकी छन् । कतारमा हुने विश्वकपमा सिक्किमकी रेश्मी थापा क्षेत्रीले रेफ्रीको भूमिका निर्वाह गर्ने भएकी हुन् । उनले भारतमा राष्ट्रिय स्तरका फुटबल प्रतियोगितामा रेफ्रीको भूमिका निर्वाह गर्दै आइरहेकी छिन् । नेपालले विश्वकपमा भाग लिन पाउने दिन आउंला नआउला । तर नेपालीभाषी एक चेलीले विश्वकपमा रेफ्रीको भूमिका पाउनु निश्चय नै खुशी र गर्वको विषय हो । एचकेनेपाल डट कम संवाददाता १२ असोज । बडादसैँको आठौँ दिन आज दुर्गाभवानीको पूजा आराधना गरी महाअष्टमी पर्व मनाइँदै छ । महाअष्ठमीको उपलक्ष्यमा आज दसैँघर र कोतलगायत अधिराज्यका विभिन्न शक्तिपीठमा बलिसहित पूजाअर्चना तथा दुर्गा सप्तशती पाठ गरिन्छ । एचकेनेपाल डट कम संवाददाता ११ असोज । बडादशैँ अन्तर्गत सप्तमी तिथिमा आज नेपालभर परम्पराअनुसार घरघरमा फूलपाती भित्राइँदैछ । दुर्गा भवानीको प्रतिमा राखिएको दसैँघरमा फूलपातीको शुभसाइतको प्रतीकका रूपमा उखु ? अदुवा ? केराको बोट ? धानको बाला ? बेलपत्र ? दारिम ? जयन्ती ? अशोकको फूल तथा अन्य नयाँ पालुवासमेत विधिपूर्वक भित्राइन्छ । आफ्ना माग पूरा गर्न सरकारले पहल नगरेको आरोप लगाउँदै चिकित्सकहरूले दसैंमा समेत आन्दोलन जारी रहने बताएका छन् । मुलुकमा भाइबर आउट आइपी सेवाबाट मोबाइल र ल्यान्डलाइन सेवामा फोन गर्नु गैरकानुनी भएको सरकारले बताएको छ । कुनै पनि प्रतिस्पर्धामा प्रायः वैमनश्य हुन्छ । ईर्ष्या अनि डाह हुन्छ । तर ? नेपाल आइडलको फाइनलमा पुगेका बुद्ध लामा ? निशान भट्टराई र प्रताप दास एकअर्कामा दुःखसुख साटासाट गर्छन् । सरकारले निर्वाचन आचारसंहिता कुल्चँदै दसैंको मुखमा आफ्ना कार्यकर्तालाई पोस्न १० करोडभन्दा बढी रुपैयाँ वितरण गरेको छ । गत २ असोजको मन्त्रिपरिषद्को बैठकले प्रधानमन्त्री ? गृहमन्त्री तथा विभिन्न मन्त्री र कांग्रेस ? माओवादी केन्द्रका उच्च नेता निकटका ८ सय १९ जनालाई १० करोडभन्दा बढी रकम वितरण गरेको प्रधानमन्त्री तथा मन्त्रिपरिषद् कार्यालय उच्च स्रोतले जानकारी ',\n",
       " 'पर्सा ? फागुन २३ गते । हुँदै नभएको जग्गा बिक्री गरी ठग्ने एकै गिरोहका चारजनालाई ठगी गरेको अभियोगमा पर्सा प्रहरीले पक्राउ गरी शनिबार सार्वजनिक गरेको छ । वीरगन्ज उपमहानगरपालिका वडा नं ८ गीतामन्दिर रोडमा सात धुर जग्गा देखाई रु ८० लाखमा बिक्री गरी ठगी गरेको अभियोगमा मालपोत कार्यालयका नासु मुरारीप्रसाद तिमल्सिना ? जग्गाधनी बाराको कचोर्वा गाविस घर भइ हाल वीरगन्ज८ बस्दै आएकी इन्द्रावती देवी गुप्ता ? उनका श्रीमान् सन्दीप गुप्ता र सहयोगी राजेश गुप्तालाई पक्राउ गरिएको पर्साका प्रहरी उपरीक्षक राजुबाबु श्रेष्ठले जानकारी दिनुभयो । जिल्ला प्रहरी कार्यालय पर्साले पत्रकार सम्मेलनको आयोजना गरी वीरगन्जको गीतामन्दिर रोडमा हुँदै नभएको सात धुर जग्गा बिक्री गरी ठगी गरेको अभियोगमा मालपोत कार्यालय पर्साका नायब सुब्बासहित चारजनालाई पक्राउ गरी सार्वजनिक गरेको हो । उहाँले जग्गाको कीर्ते प्रमाण खडा गरी काठमाडौँ ? ललितपुर वडा नं २१ निवासी सचेन्द्र महर्जनसँग रु ८० लाखमा बिक्री गरी राजीनामा नै पास गरेकोे निवेदन महर्जनले प्रहरीलाई दिएको आधारमा अनुसन्धान गर्दा भूमाफियाहरुको गिरोह नै पक्राउ गर्न सफल भएको उपरीक्षक श्रेष्ठले बताउनुभयो । नक्कली कागज प्रमाणका आधारमा वीरगन्जको प्राइम बैंकमा हुँदै नभएको जग्गा धितो राखी रु ४० लाख झिकेकी इन्द्रावतीले पछि महर्जनसँग बिक्री गरेको र सो कार्यमा मालपोत कार्यालय तथा बैंकका कर्मचारीसमेत संलग्न रहेको अनुसन्धानबाट देखिएको उपरीक्षक श्रेष्ठले जानकारी दिनुभयो । उहाँले जग्गा ठगी प्रकारणमा मालपोत कार्यालयको कागजपत्र सच्याइएको र विधिवत् रुपमा राजीनामा समेत पास भएको देखिएकाले यस काममा अन्य कर्मचारीसमेत संलग्न रहेको अनुसन्धानबाट दोषी देखिएका अन्य कर्मचारीको खोजीकार्य भइरहेको बताउनुभयो । पक्राउ परेका अभियुक्तमाथि कीर्ते र ठगी दुवै मुद्दा चलाउने तयारी भएको प्रहरीले जनाएको छ । रासस',\n",
       " 'दक्षिण एसियाको रुपान्तरणका लागि निराशालाई आशामा र नकारात्मकताबाट सकारात्मकतातर्फ लागौँ प्रधानमन्त्री मोदी',\n",
       " 'दिपायलसिलगढी नगरपालिकाका नगरप्रमुख मञ्जु मलासी सुदुरपश्चिम प्रदेशमा एक जना मात्र महिला नगरप्रमुख हुनुहुन्छ । तत्कालीन नेकपा एमालेको तर्फबाट निर्वाचित मेयर',\n",
       " 'बैतडी ? असार १६ गते । बैतडीको मेलौली नगरपालिका१ को वडाध्यक्षमा नेपाली काँग्रेसका सुरतसिंह नेगी ६३० मतका साथ विजयी हुनुभएको छ । उहाँका निकटतम प्रतिद्वन्दी नेकपा एमालेका शरदसिंह नेगीले २७१ मत प्राप्त गर्नुभयो । मेलौली१ का सदस्यमा पनि नेकाकै उम्मेदवार विजयी भएका छन् । द्रोपती चन्द ४१८ ? द्रोपती दयाल ४४१ ? चिन्तासिंह बिष्ट ४४६ र शङ्करसिंह नायकले ४१८ मत ल्याएर विजयी हुनुभएको छ । यसैगरी ? बैतडीको पाटन नगरपालिका१० को अध्यक्षमा नेकपा एमालेका देवेन्द्र कुँवरको प्यानल नै विजयी भएको छ । निकटतम प्रतिद्वन्द्वी नेकाका हरिदत्त भट्टलाई १३३ मतान्तरले पछि पार्दै कुँवर विजयी भएका हुन् । विजयी कुँवरले ३८१ मत प्राप्त गरेका छन् । त्यस्तै ? महिला सदस्यमा जयन्ती पन्त ? दलित महिला सदस्यमा बिमला सार्की र सदस्यहरुमा नरबहादुर महरा तथा रमेशप्रसाद पन्त विजयी भएका छन् । रासस',\n",
       " 'बेलुकी करिब ८ बजे तिर संखुवाखोलामा आएको बाढिले ताम्कु ७ र बाला ६ जोड्ने पुल ? ताम्कु ८ र बाला ८ जोड्ने ? ताम्कु ९ र बाला ९ जोड्ने पुल बगाएको हो । त्यसै गरि पावाखोला ४ ? ५ ? ६ र ७ ? ८ ? ९ जोड्ने चासुवाटारको एक सय मिटर लामो झोलुङ्गे पुल बाढिले बगाएको छ । पावाखोला गाविस कै ईरखुवा खोलामा बन्दै गरेको पुल समेत बगाएको गाविस सचिब महेन्द्र पराजुलीले जानकारी दिनुभयो । बाढिले चैनपुर नगरपालिका र जलजला गाविस हुदै बग्ने हेवाखोलाको पुल समेत बगाएको छ । हेवाखोलाको मुहान क्षेत्रको बाँदरमुखे भीरमा ठुलो पहिरो गएपछि जलजला १ र चैनपुर १ जोड्ने पुल ? जलजला १ कै टाँडेबाट चैनपुर १ तर्ने पुल र आसपासको ४ वटा पुलहरु बगाएको छ । संखुवासभा ? जेठ २९ गते । बुधबार राती परेको बर्षा पछि खोलामा आएको बाढिले जिल्लाका करिब दर्जन भन्दा बढि झोलुङ्गे पुल बगाएको छ । बाढि पहिरोले भौतिक संरचनामा समेत क्षति पुगेको छ । संखुवाखोलामा आएको बाढिले ताम्कु७ मा रहेको खोङ्लुवा खोला लघु जलविद्युत आयोजनाको पावर हाउस समेत बगाएको छ । खाँदवारी १३ मा रहेको क्रसर मेसिनमा पनि सभाखोलामा आएको बाढीले क्षति पुराएको छ । बाढी कै कारण चैनपुर र वाना बिचमा रहेको अरुण हाईड्रोपावरमा पनि क्षति पुराएको छ । नुम १ मा रहेको ईन्दुवा खोलाको छेउछाउमा पहिरो गएर क्षति पुराएको स्थानियले बताएका छन् । पावाखोलामा रहेको कारमाराङ खोलाले धार परिवर्तन गरेर सिरुटार बस्तिमा पसेको स्थानिय बसन्त कोरङ्गी गुरुङले जानकारी गराउनुभयो । गाउँ नै बाढि पसेपछि अहिले सिरुटार बासीले गाउँ छोडेर अन्यत्र नै बस्दै आएका छन् । जिल्लाको प्राय सवै क्षेत्रमा बाढि पहिरो आए पनि मानविय क्षति भने नभएको जिल्ला प्रहरी कार्यालयले जनाएको छ । चीनका राष्ट्राध्यक्ष सी चिन फिङ र श्रीमती फङ ली युआनले ४ तारिख बेलुकी शाङ हाइमा स्वागत रात्री भोजको आयोजना गरी पहिलो चीन अन्तर्राष्ट्रिय आयात एक्स्पोमा उपस्थित हुनुभएका विभिन्न मुलुकका अतिथिहरुलाई स्वागत गर्नुभएको छ । सो अवसरमा सी चिन फिङले सम्बोधन गर्नुहुँदै चीन अन्तर्राष्ट्रिय आयात एक्स्पो चीनको हो र साथै विश्वको पनि हो भन्दै यो सामान्य प्रदर्शनी नभएको र चीनले अर्को चरणको उच्च स्तरीय सुधार तथा खुलापन बढाउनका लागि गरेको महत्वपूर्ण निर्णय हुनुका साथै चीनले विश्वलाई सक्रिय रुपमा बजारलाई खुला गर्ने ठूलो कदम भएको दोहोर्याउनुभएको छ । चीन अन्तर्राष्ट्रिय आयात एक्स्पोले शाङ हाइको शानलाई अझै चम्काउनसक्ने विश्वास रहेको पनि उहाँले उल्लेख गर्नुभयो । सो रात्री भोजमा सी चिन फिङले विभिन्न मुलुकका नेतासँग हार्दिक आदानप्रदान गर्नुभएको छ । अतिथिहरुले पहिलो चीन अन्तर्राष्ट्रिय आयात एक्स्पो सफलताका साथ सम्पन्न हुने र विभिन्न मुलुक सँगसँगै मिलेर विकास गरी विश्वलाई शान्तिमय र समृद्ध पार्ने इच्छा व्यक्त गरेका छन् । धर्म संस्कृति मान्ने महिलाहरूले लगाउने मेहन्दी र चुरा पछिल्लो समय प्राय सबै जसो महिला र युवतीको लगाउने गरेका छन् । अहिले त मेहन्दी र चुरा फेसनकै रूपमा स्थापित भएको पाइन्छ । रङ्गीचङ्गी पहिरनसँगै बजारमा चुरा ? मेहेन्दी र कपडाहरूको व्यापार बढेको छ । साउनमा विशेष रूपमा मनाउने सोमबारको व्रत बस्ने हरियो ? पंहेलो रङ्गको चुराका साथै मेहन्दी र हरियो रातो कपडा र श्रृङ्गारमा सजिने गर्छन् । साउन महिना आएलगत्तै हरियो चुरा र महेन्दीलगायत अन्य श्रृङ्गारिक सामानको बिक्री राम्रो भइरहेको छ । अन्य समयमाभन्दा साउनमा यस्ता सामग्रीको कारोबारबाट मनग्य आम्दानी हुँदै आएको व्यापारीको भनाई छ । प्रकृति मात्र नभई साउनसँग हरियालीको विशेष सम्बन्ध र सामिप्यता रहन्छ । यही रङ्ग भित्र उपस्थिति जनाउने एउटा महत्त्वपूर्ण पक्ष मेहन्दी पनि हो । विशेष हिन्दु नारीहरूले सुन्दरता र सौभाग्यका लागि साउन महिनाभर प्रयोग गर्ने मेहन्दीको गुनिलो पक्ष बढी भए पनि बजारमा जथाभाबी पाइने मेहन्दीले स्वास्थ्यमा पनि उत्तिकै हानी पुयाउने गरेको छ । कतिपय ठाउँमा मेहन्दीलाई डार्क बनाउनका लागि विभिन्न केमिकल ? पेट्रोल ? एसीड लगायतका वस्तु राखिन्छ जुन मानिसको स्वास्थ्यको लागि घातक हो । यस्ता मेहेन्दीको प्रयोगले छाला पोल्ने छालाको एलर्जी देखि क्यान्सरसम्मको रोग लाग्ने खतरा बढी हुन्छ । सस्तो मूल्यमा पाइयो भन्दैमा जस्तोसुकै मेहन्दी लगाउँदा मानिसको स्वास्थ्यमा गम्भीर असर परिररहेको छ । त्यसैले मेहन्दी लगाउँदा प्राकृतिक मेहेन्दी प्रयोग गर्न चिकित्सकहरूको सुझाव छ । नेपालमा मेहन्दी व्यापारका लागि न कुनै मापदण्ड छ ? न कुनै अनुगमन नै । जसले ? जहाँबाट जस्तोसुकै मेहन्दी ल्याएर बेचे पनि कसैले केही गर्दैन । नेपालमा कस्मेटिक्सको मापदण्ड र नियगमन गर्ने कुनै निकाय छैन । यस्तो विधिहिनताको पीडामा नेपाली नारीहरू हरेक साउनसँगै शरीरमा घातक रोग सङ्क्रमणको पीडित भइरहेका छन् । छालामा रोग प्रतिरोधी क्षमता बढाउन पनि उत्तिकै मद्दत गर्छ । त्यस्तै मेहन्दीको सेवन उच्च रक्तचापका बिरामीहरूको लागि वरदान मानिन्छ भने कपालको रक्षाका लागि मेहन्दीलाई रक्षाकवच नै ठानिन्छ । तर अब स्वास्थ्य प्रयोगकर्तामा नै निर्भर छ । मेहन्दीका प्रयोगकर्ताहरूले सतर्कता अपनाए मेहन्दी स्वास्थ्यका लागि हो भने जथाभाबीको प्रयोग गर्नेको लागि खतराको खानी पनि हो ।  तरकारीवाला बालकको फोटो बन्यो भाइरल ? यी बालकको खोजीमा लागेको छ पुरै सरकारपूरा पढ्नु होस ।  अरुणखोलामा सवारी दुर्घटना ? २४ यात्रु घाइते',\n",
       " 'पारिवारिक चलचित्र अनुग्रहले कार्तिक ३०गते बाट देशभरका हलहरुमा धमाका पिट्दैहेर्नुहोस् भिडियो सहित',\n",
       " 'पवन पौडेलसानफ्रान्सिस्को  आइरिस् आयरल्यान्डका बासिन्दा र स्कटिस् स्कटल्यान्डका बासिन्दाले यो चाड उत्तर अमेरिका ल्याएका हुन् । यो चाड मनाउने देशको संख्या बढ्दै गैरहेको छ र नेपाली समाजले पनि यस चाडलाई मनाउदै आइरहेको छ । नेपालीहरुको चाड दशै ? तिहार ? गाइजात्रा र गठे मंगल सँग यो फेस्टिवलको गतिबिधीहरु मिल्न जुल्न आउछन । बालबालिकाहरुले काला ? सेता बस्त्र ? भूत पर्यटक ? होलिका ? भोजमा लगाईने बिशेष पोशाकमा घर दैलोमा आई मिठाई मागी मनोरंजन गर्नु यो फेस्टिभलको गतिबिधी अन्तर्गत पर्दछ । गर्मीलाई बिदाई गर्न यो चाड मनाइन्छ । यो चाड पश्चिमा देशहरू संयुक्त राज्य अमेरिका ? क्यानडा ? स्कटल्यान्ड ? न्युजिल्यान्ड आयरल्यान्ड ? संयुक्त अधिराज्य ? जर्मनी जस्ता देशमा बढी मात्रामा मनाइन्छ । हलोविन वा ह्यालविन    को छोटकरी रुप हो । यो आत्मा दिवस को पूर्व सन्ध्या हो भने सन्यासी दिवसको नामले पनि जानिन्छ  विश्वका धेरै देशहरूमा प्रत्येक वर्ष अक्टोबर ३१ मा मनाइने पर्व हो । यो चाड इसाई धर्ममा ज्यादा प्रचलित छ । विभिन्न भूतप्रेतको पोसाकमा आएका बालबालिकालाई घर धनीले झोला लिएर आएकालाई विभिन्न प्रकारका मिठाई अजुली भरि दिने गर्दछन  हेलोविन खेल्न आउनेलाई केवल मिठाई मात्र दिनु पर्छ  यसरी बालबालिका खेल्न जाँदा उनीहरुको सुरक्षा तथा मार्ग दर्शन गर्न उनीहरुको साथमा अभिभावक पनि आउने गर्दछन ',\n",
       " 'बिगत २ वर्षदेखि हलोविन नाइटको आयोजना गर्दै आएको एस बाबु प्रोडक्सनले यसपाली पनि हलोविन नाइट ३ कार्यक्रमलाइ निरन्तरता दिएको छ । बिशेष गरि अमेरिकामा जन्मेका दोस्रा पुस्ता र बिभिन्न उदेश्य लिएर अमेरिका आएका फरक फरक उमेर समुहले यस पर्बलाई पछील्लो केही समय यता मनोरन्जनका साथ मनाउन थालेका छन् । अक्टोबर २७ तारिक शनिबारका दिन    ?   ?   ?      मा हुँदै गरेको कार्यक्रममा बे एरियाका नेपाली डी जेहरु ? डी जे एस बाबु ? डी जे काले का साथ् साथै अमेरिकन डी जे रोजको पनि जोडदार प्रस्तुति रहने जानकारी एस बाबु प्रोडक्सनका प्रमुख सन्दिप बाबुले दिनु भएको छ  फोटो बूथ र खाना पनि कार्यक्रमको आकर्शणको रुपमा रहनेछन ',\n",
       " 'टोकियो ? जापान ? २३ असार राससएएफपी  अमेरिकी विदेशमन्त्री माइक पोम्पेओले उत्तर कोरियाको दुई दिने भ्रमणका क्रममा उत्तर कोरियाली समकक्षीसँग भएको वार्ता प्योङयाङलाई आणविक रूपमा निशस्त्र बनाउन निकै लाभकारी भएको शनिबार बताउनुभएको छ । उत्तर कोरियाको भ्रमणपछि टोकियो प्रस्थान गर्नुपूर्व सञ्चारकर्मीहरूसँगको कुराकानीमा उहाँले कसरी उत्तर कोरियाले आणविक निशस्त्रीकरणबारे आफ्नो प्रतिबद्धतालाई सम्मान गर्न सक्छ भन्ने बिषयमा आफूहरूले केही नयाँ विवरणहरू आदानप्रदान गरेको बताउनुभयो । उहाँले भन्नुभयो ? यी पक्कै पनि जटिल बिषयहरू हुन् ? तर हामीले सवैखाले प्रमुख मुद्दामा प्रगति गरिसकेका छौँ । केही कुरामा निकै नै प्रगति भएको छ भने अन्यमा पनि सफलताका लागि थप कामहरू भइरहेका छन् । ',\n",
       " 'यसैबीच ? दक्षिण कोरियाली समाचार संस्था योन्हापले बताए अनुसार वासिङ्टनका सर्वग्राही माग र अति नै अफसोसजनक धारणाको आलोचना गरेको छ । पोम्पेओसँगको वार्ता सकिएको केही घण्टामै आएको हो । उत्तर कोरियाली विदेश मन्त्रालयद्वारा जारी विज्ञप्तिलाई उद्धृत गर्दै योन्हापले जनाएको छ ? उच्च स्तरीय वार्तामा अमेरिकी धारणा र अडान निकै अफसोसपूर्ण पाइयो । न्यूज अभियान डट कमको एन्ड्रोइड एपका लागि यहाँ क्लिक गर्नुहोस् । हरेक खबर विशेष खबरका लागि अफिसियल फेसबुक र ट्वीटर मार्फत जोडिनुहोस् । काठमाडौं  सरकारले सहरी गरिब ? सुकुम्बासी तथा न्यून आय भएका परिवारका लागि बनाइएको अपार्टमेन्टमा मुख्यमन्त्री र प्रदेश प्रमुखलाई राख्ने तयारी गरेको छ । नागार्जुन नगरपालिकाको इचंगुनारायणमा २०७० मा निर्मित अपार्टमेन्टमा मुख्यमन्त्री र प्रदेश प्रमुखको अस्थायी आवास बनाउन लागिएको हो । आजको कान्तिपुरको पृष्ठ ३ मा यो समाचार छापिएको छ । केही दिनअघि गृहसचिव प्रेमकुमार राई ? सहरी विकास सचिव दीपेन्द्रनाथ शर्मा ? प्रधानमन्त्रीका प्रमुख सल्लाहकार विष्णु रिमाल र उपत्यका विकास प्राधिकरणका प्रम',\n",
       " 'हे परमप्रभु ? हामी साँचो मनले तपाईंलाई पूज्दछौं यसर्थ हामीलाई तपाईंको महान् प्रेम दर्शाउनुहोस् । दाऊदको एउटा भजन । म परमेश्वरको संगत पाउनलाई उहाँकहाँ गएँ । अनि उहाँले मलाई उत्तर दिनुभयो । मेरा सब डरहरूबाट उहाँले मलाई बचाउनु भयो । यस दुःखी मानिसहले उहाँको पाउनलाई परमप्रभुलाई बोलायो । अनि परमप्रभुले मेरो कुरा सुन्नुभयो । उहाँले मलाई सबै संकटहरूबाट बचाउनुभयो । परमप्रभुको र्स्वगदूतले जुन मानिसहरूले परमप्रभुको अनुशरण गरे ? उनीहरूको चारैतिर छाउनी बनाउँछ अनि तिनीहरूलाई बचाउँछन् । स्वर्गदूतहरूले तिनीहरूको रक्षा गरे । परमप्रभुलाई जाँच गरेर हेर ? उहाँ कति असल हुनुहुन्छ । जुन मानिसले उहाँमाथि आश्रय राख्दछ ऊ अवश्य खुशी हुनेछ । परमप्रभुका पवित्र भक्तजनहरू उहाँको उपासना गर । उहाँ बाहेक परमप्रभुका भक्तजनहरूका लागि अरू कुनै सुरक्षित ठाउँ छैन । बलवान मानिसहरू कमजोर र भोका हुनेछन् । तर जुन मानिसहरू सहयोग माग्न परमेश्वर कहाँ जान्छन् तिनीहरू मध्ये प्रत्येकले असल थोकहरू पाउनेछन् । नराम्रा कामहरू गर्नदेखि आफूलाई रोक राम्रो ? असल कुराहरू मात्र गर । शान्तिको पछाडी दगुर अनि त्यसलाई समात । तर परमप्रभु ती मानिसहरूका विरूद्ध हुनुहुन्छ जसले नरम्रा कामहरू गर्छन् । उहाँले तिनीहरूलाई पूर्ण प्रकारले ध्वंश पारिदिनु हुन्छ',\n",
       " 'जब कष्टले सताँउछ केही मानिसहरू नम्र हुन्छन् । परमप्रभु तिनीहरूको नजिक हुनुहुन्छ । अनि उहाँले तिनीहरूलाई बचाउनु हुन्छ । धार्मिक मानिसहरूका धेरै समस्याहरू हुन सक्छन् ? तर परमप्रभुले तिनीहरूका प्रत्येक समस्याबाट बचाँउनु हुनेछ । यस रिचमण्ड फेलोशिप नेपाल डिएडिक्सन सेन्टर हालको लागि चोभारमा नै रहेको रिचमण्ड फेलोशिप नेपाल मेल सेन्टरमा मोर्डन ईन्डियन स्कूल सँगै सारेको व्यहोरा जानकारी गराउँदछौँ ।  कोसेली पुयाउन गएका आफ्नै साथीको हत्या गर्ने एक नेपाली युवालाई साउदीमा मृत्यु दण्डको सजाय',\n",
       " 'पीडित परिवारलाई ब्लडमनि लिएर माफी दिन स्थानीय अधिकारीहरुले समेत गरेको प्रस्ताव अस्वीकार गरेपछि साउदीको अदालतले एक नेपालीलाई मृत्युदण्डको सजाय सुनाएको छ । यो फैसलालाई साउदी प्रशासनले जुन सुकै बेला कार्यान्वयन गर्न सक्नेछ । एउटै कम्पनीमा कार्यरत भएको बेला कामीको कोसेली लिएर कुश्मेली पर्वत पुगेका थिए । बिदाबाट फर्के लगत्तै आबेगमा आएका कामीले कुश्मेलीको हत्या गर्न पुगेका थिए । उसैको कोसेली पुर्याउन घर गइदिनुछ । फेरि उसैबाट मारिएको छ ?  पीडित परिवारको भनाइ उद्धत गर्दै दूतावासका एक अधिकारीले भने ? हामी कसरी माफी दिन सक्छौं ? ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffe64cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 5000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_iter),len(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ee116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(None)\n",
    "vocab = build_vocab_from_iterator(\n",
    "    map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32332624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long)\n",
    "            for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc61642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e41fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test Split\n",
    "\n",
    "train_data = data_process(train_iter)\n",
    "test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0ca4903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 19020,   1025,    511,    761,  24762,    827, 103693,      1,   3726,\n",
       "            32,  11495,      3,    792,      1,   1374,     32,   2702,    324,\n",
       "             2,    107,     14,    330,  24336,     77,   5527,   8387,  28118,\n",
       "        123255,  11037,     33,   6619,    480,     14,  34020,   3207,     13,\n",
       "        176772,  44337,    640,   2987, 314958,   4492,   1852, 316978,    436,\n",
       "         20999,    130,      3, 321145,      9, 151169,      4,      1,    950,\n",
       "           305,  67904,  10488,    693,      1,     60,      2,    203,  27756,\n",
       "             1,    387,   3505,   1589,  15993,   9954,    230,  11080,   1190,\n",
       "          1417,     98,   6243,   3793,    783,      7,      1,    387,   3505,\n",
       "          1589,  14473,  16876,   7713,  33868,  40783,  32454,   4344,  47676,\n",
       "          7188,   9612,    230,  11080,   1190,    161,      2,    593,    302,\n",
       "             1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b8f1855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342570\n",
      "30000 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(len(train_iter),len(test_iter))\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "torch.cuda.memory_allocated() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5e24971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b349f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 35\n",
    "# eval_batch_size = 10\n",
    "\n",
    "# train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "# test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fda3aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Data\n",
    "\n",
    "\n",
    "text = ['आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य']\n",
    "#text = ['जनसंख्या']\n",
    "sample_data = data_process(\n",
    "    text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16c88184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2086,  5694,   568,     0,   897, 28361,     0,   357,   465,   410,\n",
       "         6548,   293,     0,   357,   465])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9de8a21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given word: आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2086,   357],\n",
       "        [ 5694,   465],\n",
       "        [  568,   410],\n",
       "        [    0,  6548],\n",
       "        [  897,   293],\n",
       "        [28361,     0],\n",
       "        [    0,   357]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = batchify(sample_data, 2)\n",
    "print(\"Given word:\", text[0])\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "185318b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6406801])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb776cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 16\n",
    "train_data = batchify(train_data, bptt)  # shape [seq_len, batch_size]\n",
    "test_data = batchify(test_data, bptt)\n",
    "\n",
    "\n",
    "import math\n",
    "def get_batch(source: Tensor, i: int) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    #target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd7744",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6290c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)\n",
    "                             * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4619fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "037dbee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60324352"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60e6bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf7ec3",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c7cfdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 300  # embedding dimension\n",
    "d_hid = 800  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4 # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.05  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize,nhead, d_hid,\n",
    "                         nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52bed039",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99227194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bcf16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        #print(type(output))\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e875bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2)\n",
    "softmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61169dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_softmax = softmax(output)\n",
    "            output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "            indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "            target_indices = targets.t()\n",
    "            # print(output)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "\n",
    "    return total_loss / (len(eval_data) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "405c2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#            print('data')\n",
    "#             print(list([vocab.lookup_tokens(list(index))\n",
    "#                         for index in data.t()]))\n",
    "#             print(indices)\n",
    "#             print(list([vocab.lookup_tokens(list(index))\n",
    "#                         for index in indices]))\n",
    "#             print(len(targets))\n",
    "#             print(list([vocab.lookup_tokens(list(index))\n",
    "#                         for index in target_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9a140c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # target_indices = targets.t()\n",
    "        # print(indices.shape)\n",
    "        # print(indices)\n",
    "        # temp_gen_data = [vocab.lookup_tokens(\n",
    "        #    list(index)) for index in indices][0][i]\n",
    "        # gen_data = [vocab.lookup_tokens(\n",
    "        #    list(index)) for index in indices][0]\n",
    "        # print(temp_text)\n",
    "        # print([[vocab.lookup_tokens(list(index))\n",
    "        #        for index in indices][0][i]])\n",
    "#         temp_text = [[vocab.lookup_tokens(list(index))]\n",
    "#                       for index in indices[0][i]]\n",
    "        # print(temp_text)\n",
    "#         temp_text = [' '.join(temp_text)]\n",
    "        # print(temp_text)\n",
    "        # gen_data = vocab.lookup_tokens((list(gen_data))) + list(temp_gen_data)\n",
    "        # gen_data = torch.tensor(\n",
    "        #    (list(gen_data[0])+list(indices[0][i]))).unsqueeze(0)\n",
    "        # gen_data = torch.concat([gen_data, torch.tensor(indices[0][i])], dim=1)\n",
    "        #gen_data = ' '.join(gen_data)\n",
    "        #return temp_txt\n",
    "#         gen_data = data_process(temp_text)\n",
    "#         gen_data = batchify(gen_data, 1)\n",
    "        #return gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60119ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d73f1d",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f532766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/25026 batches | lr 1.00 | ms/batch 167.88 | loss 10.69 | ppl 44002.80\n",
      "| epoch   1 |   400/25026 batches | lr 1.00 | ms/batch 151.02 | loss  9.64 | ppl 15402.88\n",
      "| epoch   1 |   600/25026 batches | lr 1.00 | ms/batch 151.48 | loss  9.36 | ppl 11595.82\n",
      "| epoch   1 |   800/25026 batches | lr 1.00 | ms/batch 151.92 | loss  9.17 | ppl  9625.36\n",
      "| epoch   1 |  1000/25026 batches | lr 1.00 | ms/batch 152.19 | loss  8.96 | ppl  7760.74\n",
      "| epoch   1 |  1200/25026 batches | lr 1.00 | ms/batch 152.30 | loss  8.81 | ppl  6694.05\n",
      "| epoch   1 |  1400/25026 batches | lr 1.00 | ms/batch 152.40 | loss  8.81 | ppl  6690.16\n",
      "| epoch   1 |  1600/25026 batches | lr 1.00 | ms/batch 152.49 | loss  8.81 | ppl  6717.32\n",
      "| epoch   1 |  1800/25026 batches | lr 1.00 | ms/batch 152.49 | loss  8.87 | ppl  7095.37\n",
      "| epoch   1 |  2000/25026 batches | lr 1.00 | ms/batch 152.59 | loss  8.89 | ppl  7295.01\n",
      "| epoch   1 |  2200/25026 batches | lr 1.00 | ms/batch 152.59 | loss  8.68 | ppl  5895.12\n",
      "| epoch   1 |  2400/25026 batches | lr 1.00 | ms/batch 152.71 | loss  8.67 | ppl  5817.79\n",
      "| epoch   1 |  2600/25026 batches | lr 1.00 | ms/batch 152.74 | loss  8.61 | ppl  5476.21\n",
      "| epoch   1 |  2800/25026 batches | lr 1.00 | ms/batch 152.84 | loss  8.47 | ppl  4750.72\n",
      "| epoch   1 |  3000/25026 batches | lr 1.00 | ms/batch 152.91 | loss  8.40 | ppl  4433.07\n",
      "| epoch   1 |  3200/25026 batches | lr 1.00 | ms/batch 152.90 | loss  8.45 | ppl  4680.56\n",
      "| epoch   1 |  3400/25026 batches | lr 1.00 | ms/batch 152.84 | loss  8.28 | ppl  3929.68\n",
      "| epoch   1 |  3600/25026 batches | lr 1.00 | ms/batch 152.93 | loss  8.30 | ppl  4006.41\n",
      "| epoch   1 |  3800/25026 batches | lr 1.00 | ms/batch 152.89 | loss  8.31 | ppl  4049.83\n",
      "| epoch   1 |  4000/25026 batches | lr 1.00 | ms/batch 152.98 | loss  8.23 | ppl  3744.63\n",
      "| epoch   1 |  4200/25026 batches | lr 1.00 | ms/batch 152.98 | loss  8.26 | ppl  3874.58\n",
      "| epoch   1 |  4400/25026 batches | lr 1.00 | ms/batch 153.03 | loss  8.08 | ppl  3218.68\n",
      "| epoch   1 |  4600/25026 batches | lr 1.00 | ms/batch 153.08 | loss  8.07 | ppl  3209.32\n",
      "| epoch   1 |  4800/25026 batches | lr 1.00 | ms/batch 153.11 | loss  8.15 | ppl  3451.08\n",
      "| epoch   1 |  5000/25026 batches | lr 1.00 | ms/batch 153.16 | loss  8.06 | ppl  3176.04\n",
      "| epoch   1 |  5200/25026 batches | lr 1.00 | ms/batch 153.09 | loss  8.08 | ppl  3241.53\n",
      "| epoch   1 |  5400/25026 batches | lr 1.00 | ms/batch 153.40 | loss  8.06 | ppl  3173.83\n",
      "| epoch   1 |  5600/25026 batches | lr 1.00 | ms/batch 153.37 | loss  8.01 | ppl  3002.43\n",
      "| epoch   1 |  5800/25026 batches | lr 1.00 | ms/batch 153.37 | loss  7.99 | ppl  2945.14\n",
      "| epoch   1 |  6000/25026 batches | lr 1.00 | ms/batch 153.24 | loss  8.25 | ppl  3840.50\n",
      "| epoch   1 |  6200/25026 batches | lr 1.00 | ms/batch 153.29 | loss  8.13 | ppl  3392.26\n",
      "| epoch   1 |  6400/25026 batches | lr 1.00 | ms/batch 153.42 | loss  7.97 | ppl  2894.38\n",
      "| epoch   1 |  6600/25026 batches | lr 1.00 | ms/batch 153.57 | loss  8.10 | ppl  3291.37\n",
      "| epoch   1 |  6800/25026 batches | lr 1.00 | ms/batch 153.48 | loss  8.12 | ppl  3366.99\n",
      "| epoch   1 |  7000/25026 batches | lr 1.00 | ms/batch 153.31 | loss  8.04 | ppl  3101.82\n",
      "| epoch   1 |  7200/25026 batches | lr 1.00 | ms/batch 153.37 | loss  8.05 | ppl  3133.45\n",
      "| epoch   1 |  7400/25026 batches | lr 1.00 | ms/batch 153.36 | loss  7.87 | ppl  2610.03\n",
      "| epoch   1 |  7600/25026 batches | lr 1.00 | ms/batch 153.38 | loss  7.99 | ppl  2947.19\n",
      "| epoch   1 |  7800/25026 batches | lr 1.00 | ms/batch 153.50 | loss  7.92 | ppl  2762.51\n",
      "| epoch   1 |  8000/25026 batches | lr 1.00 | ms/batch 153.38 | loss  7.97 | ppl  2888.74\n",
      "| epoch   1 |  8200/25026 batches | lr 1.00 | ms/batch 153.49 | loss  7.74 | ppl  2308.96\n",
      "| epoch   1 |  8400/25026 batches | lr 1.00 | ms/batch 153.66 | loss  7.83 | ppl  2504.36\n",
      "| epoch   1 |  8600/25026 batches | lr 1.00 | ms/batch 153.64 | loss  7.85 | ppl  2577.24\n",
      "| epoch   1 |  8800/25026 batches | lr 1.00 | ms/batch 153.69 | loss  7.95 | ppl  2828.30\n",
      "| epoch   1 |  9000/25026 batches | lr 1.00 | ms/batch 153.48 | loss  7.93 | ppl  2767.79\n",
      "| epoch   1 |  9200/25026 batches | lr 1.00 | ms/batch 153.81 | loss  7.87 | ppl  2623.06\n",
      "| epoch   1 |  9400/25026 batches | lr 1.00 | ms/batch 153.57 | loss  7.81 | ppl  2465.05\n",
      "| epoch   1 |  9600/25026 batches | lr 1.00 | ms/batch 153.56 | loss  7.78 | ppl  2399.31\n",
      "| epoch   1 |  9800/25026 batches | lr 1.00 | ms/batch 153.51 | loss  7.91 | ppl  2721.80\n",
      "| epoch   1 | 10000/25026 batches | lr 1.00 | ms/batch 153.48 | loss  7.82 | ppl  2489.64\n",
      "| epoch   1 | 10200/25026 batches | lr 1.00 | ms/batch 153.42 | loss  7.76 | ppl  2336.51\n",
      "| epoch   1 | 10400/25026 batches | lr 1.00 | ms/batch 153.70 | loss  7.67 | ppl  2153.65\n",
      "| epoch   1 | 10600/25026 batches | lr 1.00 | ms/batch 153.62 | loss  7.72 | ppl  2262.84\n",
      "| epoch   1 | 10800/25026 batches | lr 1.00 | ms/batch 153.81 | loss  7.75 | ppl  2319.85\n",
      "| epoch   1 | 11000/25026 batches | lr 1.00 | ms/batch 153.73 | loss  7.85 | ppl  2561.10\n",
      "| epoch   1 | 11200/25026 batches | lr 1.00 | ms/batch 153.72 | loss  7.74 | ppl  2290.40\n",
      "| epoch   1 | 11400/25026 batches | lr 1.00 | ms/batch 153.66 | loss  7.76 | ppl  2354.51\n",
      "| epoch   1 | 11600/25026 batches | lr 1.00 | ms/batch 153.70 | loss  7.66 | ppl  2111.83\n",
      "| epoch   1 | 11800/25026 batches | lr 1.00 | ms/batch 153.47 | loss  7.84 | ppl  2537.10\n",
      "| epoch   1 | 12000/25026 batches | lr 1.00 | ms/batch 153.61 | loss  7.56 | ppl  1923.82\n",
      "| epoch   1 | 12200/25026 batches | lr 1.00 | ms/batch 153.56 | loss  7.57 | ppl  1941.95\n",
      "| epoch   1 | 12400/25026 batches | lr 1.00 | ms/batch 153.51 | loss  7.69 | ppl  2175.63\n",
      "| epoch   1 | 12600/25026 batches | lr 1.00 | ms/batch 153.59 | loss  7.78 | ppl  2383.44\n",
      "| epoch   1 | 12800/25026 batches | lr 1.00 | ms/batch 153.49 | loss  7.76 | ppl  2337.05\n",
      "| epoch   1 | 13000/25026 batches | lr 1.00 | ms/batch 153.48 | loss  7.78 | ppl  2401.33\n",
      "| epoch   1 | 13200/25026 batches | lr 1.00 | ms/batch 153.71 | loss  7.67 | ppl  2142.77\n",
      "| epoch   1 | 13400/25026 batches | lr 1.00 | ms/batch 153.48 | loss  7.54 | ppl  1885.17\n",
      "| epoch   1 | 13600/25026 batches | lr 1.00 | ms/batch 153.54 | loss  7.67 | ppl  2150.41\n",
      "| epoch   1 | 13800/25026 batches | lr 1.00 | ms/batch 153.62 | loss  7.70 | ppl  2207.03\n",
      "| epoch   1 | 14000/25026 batches | lr 1.00 | ms/batch 153.60 | loss  7.60 | ppl  2006.79\n",
      "| epoch   1 | 14200/25026 batches | lr 1.00 | ms/batch 153.56 | loss  7.60 | ppl  2005.83\n",
      "| epoch   1 | 14400/25026 batches | lr 1.00 | ms/batch 153.76 | loss  7.57 | ppl  1943.42\n",
      "| epoch   1 | 14600/25026 batches | lr 1.00 | ms/batch 154.03 | loss  7.51 | ppl  1827.26\n",
      "| epoch   1 | 14800/25026 batches | lr 1.00 | ms/batch 153.64 | loss  7.40 | ppl  1628.55\n",
      "| epoch   1 | 15000/25026 batches | lr 1.00 | ms/batch 153.46 | loss  7.58 | ppl  1951.94\n",
      "| epoch   1 | 15200/25026 batches | lr 1.00 | ms/batch 154.33 | loss  7.57 | ppl  1946.50\n",
      "| epoch   1 | 15400/25026 batches | lr 1.00 | ms/batch 153.28 | loss  7.58 | ppl  1950.98\n",
      "| epoch   1 | 15600/25026 batches | lr 1.00 | ms/batch 153.32 | loss  7.50 | ppl  1806.98\n",
      "| epoch   1 | 15800/25026 batches | lr 1.00 | ms/batch 153.16 | loss  7.43 | ppl  1691.28\n",
      "| epoch   1 | 16000/25026 batches | lr 1.00 | ms/batch 153.14 | loss  7.53 | ppl  1860.13\n",
      "| epoch   1 | 16200/25026 batches | lr 1.00 | ms/batch 153.12 | loss  7.46 | ppl  1728.96\n",
      "| epoch   1 | 16400/25026 batches | lr 1.00 | ms/batch 153.13 | loss  7.68 | ppl  2165.57\n",
      "| epoch   1 | 16600/25026 batches | lr 1.00 | ms/batch 153.11 | loss  7.52 | ppl  1846.56\n",
      "| epoch   1 | 16800/25026 batches | lr 1.00 | ms/batch 153.08 | loss  7.50 | ppl  1814.81\n",
      "| epoch   1 | 17000/25026 batches | lr 1.00 | ms/batch 153.16 | loss  7.52 | ppl  1842.14\n",
      "| epoch   1 | 17200/25026 batches | lr 1.00 | ms/batch 153.12 | loss  7.52 | ppl  1839.48\n",
      "| epoch   1 | 17400/25026 batches | lr 1.00 | ms/batch 153.12 | loss  7.44 | ppl  1706.51\n",
      "| epoch   1 | 17600/25026 batches | lr 1.00 | ms/batch 153.05 | loss  7.46 | ppl  1730.44\n",
      "| epoch   1 | 17800/25026 batches | lr 1.00 | ms/batch 153.02 | loss  7.57 | ppl  1935.21\n",
      "| epoch   1 | 18000/25026 batches | lr 1.00 | ms/batch 153.11 | loss  7.54 | ppl  1873.37\n",
      "| epoch   1 | 18200/25026 batches | lr 1.00 | ms/batch 153.14 | loss  7.59 | ppl  1969.93\n",
      "| epoch   1 | 18400/25026 batches | lr 1.00 | ms/batch 153.09 | loss  7.37 | ppl  1586.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 18600/25026 batches | lr 1.00 | ms/batch 153.10 | loss  7.48 | ppl  1776.38\n",
      "| epoch   1 | 18800/25026 batches | lr 1.00 | ms/batch 153.08 | loss  7.31 | ppl  1494.77\n",
      "| epoch   1 | 19000/25026 batches | lr 1.00 | ms/batch 153.09 | loss  7.54 | ppl  1877.55\n",
      "| epoch   1 | 19200/25026 batches | lr 1.00 | ms/batch 153.00 | loss  7.60 | ppl  2006.04\n",
      "| epoch   1 | 19400/25026 batches | lr 1.00 | ms/batch 153.07 | loss  7.56 | ppl  1911.22\n",
      "| epoch   1 | 19600/25026 batches | lr 1.00 | ms/batch 153.03 | loss  7.33 | ppl  1528.54\n",
      "| epoch   1 | 19800/25026 batches | lr 1.00 | ms/batch 153.07 | loss  7.35 | ppl  1553.95\n",
      "| epoch   1 | 20000/25026 batches | lr 1.00 | ms/batch 153.10 | loss  7.46 | ppl  1730.13\n",
      "| epoch   1 | 20200/25026 batches | lr 1.00 | ms/batch 153.06 | loss  7.57 | ppl  1932.99\n",
      "| epoch   1 | 20400/25026 batches | lr 1.00 | ms/batch 153.00 | loss  7.35 | ppl  1558.16\n",
      "| epoch   1 | 20600/25026 batches | lr 1.00 | ms/batch 152.95 | loss  7.42 | ppl  1667.75\n",
      "| epoch   1 | 20800/25026 batches | lr 1.00 | ms/batch 153.04 | loss  7.23 | ppl  1379.65\n",
      "| epoch   1 | 21000/25026 batches | lr 1.00 | ms/batch 153.07 | loss  7.38 | ppl  1611.49\n",
      "| epoch   1 | 21200/25026 batches | lr 1.00 | ms/batch 153.04 | loss  7.42 | ppl  1676.65\n",
      "| epoch   1 | 21400/25026 batches | lr 1.00 | ms/batch 153.04 | loss  7.27 | ppl  1432.76\n",
      "| epoch   1 | 21600/25026 batches | lr 1.00 | ms/batch 153.02 | loss  7.48 | ppl  1773.10\n",
      "| epoch   1 | 21800/25026 batches | lr 1.00 | ms/batch 153.29 | loss  7.38 | ppl  1606.65\n",
      "| epoch   1 | 22000/25026 batches | lr 1.00 | ms/batch 154.84 | loss  7.43 | ppl  1685.84\n",
      "| epoch   1 | 22200/25026 batches | lr 1.00 | ms/batch 153.29 | loss  7.47 | ppl  1748.80\n",
      "| epoch   1 | 22400/25026 batches | lr 1.00 | ms/batch 152.97 | loss  7.40 | ppl  1641.71\n",
      "| epoch   1 | 22600/25026 batches | lr 1.00 | ms/batch 152.95 | loss  7.45 | ppl  1722.22\n",
      "| epoch   1 | 22800/25026 batches | lr 1.00 | ms/batch 152.99 | loss  7.37 | ppl  1581.38\n",
      "| epoch   1 | 23000/25026 batches | lr 1.00 | ms/batch 153.21 | loss  7.30 | ppl  1474.38\n",
      "| epoch   1 | 23200/25026 batches | lr 1.00 | ms/batch 153.01 | loss  7.42 | ppl  1674.87\n",
      "| epoch   1 | 23400/25026 batches | lr 1.00 | ms/batch 153.05 | loss  7.13 | ppl  1244.07\n",
      "| epoch   1 | 23600/25026 batches | lr 1.00 | ms/batch 153.07 | loss  7.29 | ppl  1460.62\n",
      "| epoch   1 | 23800/25026 batches | lr 1.00 | ms/batch 153.02 | loss  7.36 | ppl  1577.87\n",
      "| epoch   1 | 24000/25026 batches | lr 1.00 | ms/batch 153.26 | loss  7.24 | ppl  1389.93\n",
      "| epoch   1 | 24200/25026 batches | lr 1.00 | ms/batch 153.30 | loss  7.07 | ppl  1180.94\n",
      "| epoch   1 | 24400/25026 batches | lr 1.00 | ms/batch 153.02 | loss  7.35 | ppl  1555.92\n",
      "| epoch   1 | 24600/25026 batches | lr 1.00 | ms/batch 153.00 | loss  7.42 | ppl  1674.07\n",
      "| epoch   1 | 24800/25026 batches | lr 1.00 | ms/batch 153.07 | loss  7.45 | ppl  1712.03\n",
      "| epoch   1 | 25000/25026 batches | lr 1.00 | ms/batch 153.70 | loss  7.43 | ppl  1686.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 4038.65s | valid loss  7.48 | valid ppl  1770.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/25026 batches | lr 1.00 | ms/batch 154.31 | loss  7.39 | ppl  1618.80\n",
      "| epoch   2 |   400/25026 batches | lr 1.00 | ms/batch 153.19 | loss  7.33 | ppl  1526.05\n",
      "| epoch   2 |   600/25026 batches | lr 1.00 | ms/batch 153.05 | loss  7.35 | ppl  1555.44\n",
      "| epoch   2 |   800/25026 batches | lr 1.00 | ms/batch 153.09 | loss  7.34 | ppl  1534.55\n",
      "| epoch   2 |  1000/25026 batches | lr 1.00 | ms/batch 153.06 | loss  7.22 | ppl  1359.78\n",
      "| epoch   2 |  1200/25026 batches | lr 1.00 | ms/batch 153.06 | loss  7.08 | ppl  1184.31\n",
      "| epoch   2 |  1400/25026 batches | lr 1.00 | ms/batch 152.91 | loss  7.25 | ppl  1409.88\n",
      "| epoch   2 |  1600/25026 batches | lr 1.00 | ms/batch 152.92 | loss  7.39 | ppl  1622.40\n",
      "| epoch   2 |  1800/25026 batches | lr 1.00 | ms/batch 153.01 | loss  7.49 | ppl  1791.06\n",
      "| epoch   2 |  2000/25026 batches | lr 1.00 | ms/batch 152.93 | loss  7.60 | ppl  2007.37\n",
      "| epoch   2 |  2200/25026 batches | lr 1.00 | ms/batch 152.97 | loss  7.36 | ppl  1576.66\n",
      "| epoch   2 |  2400/25026 batches | lr 1.00 | ms/batch 153.28 | loss  7.42 | ppl  1665.77\n",
      "| epoch   2 |  2600/25026 batches | lr 1.00 | ms/batch 153.15 | loss  7.38 | ppl  1611.35\n",
      "| epoch   2 |  2800/25026 batches | lr 1.00 | ms/batch 153.02 | loss  7.19 | ppl  1327.49\n",
      "| epoch   2 |  3000/25026 batches | lr 1.00 | ms/batch 152.98 | loss  7.19 | ppl  1325.86\n",
      "| epoch   2 |  3200/25026 batches | lr 1.00 | ms/batch 153.00 | loss  7.24 | ppl  1393.18\n",
      "| epoch   2 |  3400/25026 batches | lr 1.00 | ms/batch 153.05 | loss  7.11 | ppl  1224.81\n",
      "| epoch   2 |  3600/25026 batches | lr 1.00 | ms/batch 153.01 | loss  7.17 | ppl  1301.20\n",
      "| epoch   2 |  3800/25026 batches | lr 1.00 | ms/batch 152.99 | loss  7.16 | ppl  1292.46\n",
      "| epoch   2 |  4000/25026 batches | lr 1.00 | ms/batch 152.98 | loss  7.13 | ppl  1248.32\n",
      "| epoch   2 |  4200/25026 batches | lr 1.00 | ms/batch 152.97 | loss  7.23 | ppl  1380.35\n",
      "| epoch   2 |  4400/25026 batches | lr 1.00 | ms/batch 152.89 | loss  7.04 | ppl  1145.63\n",
      "| epoch   2 |  4600/25026 batches | lr 1.00 | ms/batch 152.88 | loss  7.02 | ppl  1123.14\n",
      "| epoch   2 |  4800/25026 batches | lr 1.00 | ms/batch 152.90 | loss  7.15 | ppl  1280.07\n",
      "| epoch   2 |  5000/25026 batches | lr 1.00 | ms/batch 152.78 | loss  7.08 | ppl  1188.10\n",
      "| epoch   2 |  5200/25026 batches | lr 1.00 | ms/batch 152.74 | loss  7.05 | ppl  1158.00\n",
      "| epoch   2 |  5400/25026 batches | lr 1.00 | ms/batch 152.84 | loss  7.07 | ppl  1181.44\n",
      "| epoch   2 |  5600/25026 batches | lr 1.00 | ms/batch 153.01 | loss  7.00 | ppl  1101.39\n",
      "| epoch   2 |  5800/25026 batches | lr 1.00 | ms/batch 152.87 | loss  6.99 | ppl  1086.23\n",
      "| epoch   2 |  6000/25026 batches | lr 1.00 | ms/batch 153.11 | loss  7.32 | ppl  1517.69\n",
      "| epoch   2 |  6200/25026 batches | lr 1.00 | ms/batch 153.03 | loss  7.22 | ppl  1363.35\n",
      "| epoch   2 |  6400/25026 batches | lr 1.00 | ms/batch 154.65 | loss  7.06 | ppl  1167.13\n",
      "| epoch   2 |  6600/25026 batches | lr 1.00 | ms/batch 153.18 | loss  7.21 | ppl  1348.21\n",
      "| epoch   2 |  6800/25026 batches | lr 1.00 | ms/batch 153.16 | loss  7.31 | ppl  1499.46\n",
      "| epoch   2 |  7000/25026 batches | lr 1.00 | ms/batch 153.37 | loss  7.18 | ppl  1308.30\n",
      "| epoch   2 |  7200/25026 batches | lr 1.00 | ms/batch 153.18 | loss  7.21 | ppl  1352.35\n",
      "| epoch   2 |  7400/25026 batches | lr 1.00 | ms/batch 152.95 | loss  7.00 | ppl  1095.54\n",
      "| epoch   2 |  7600/25026 batches | lr 1.00 | ms/batch 152.98 | loss  7.16 | ppl  1292.34\n",
      "| epoch   2 |  7800/25026 batches | lr 1.00 | ms/batch 152.91 | loss  7.08 | ppl  1193.62\n",
      "| epoch   2 |  8000/25026 batches | lr 1.00 | ms/batch 152.78 | loss  7.15 | ppl  1275.95\n",
      "| epoch   2 |  8200/25026 batches | lr 1.00 | ms/batch 152.81 | loss  6.90 | ppl   993.39\n",
      "| epoch   2 |  8400/25026 batches | lr 1.00 | ms/batch 152.80 | loss  7.01 | ppl  1106.72\n",
      "| epoch   2 |  8600/25026 batches | lr 1.00 | ms/batch 152.85 | loss  7.05 | ppl  1150.39\n",
      "| epoch   2 |  8800/25026 batches | lr 1.00 | ms/batch 152.77 | loss  7.18 | ppl  1319.16\n",
      "| epoch   2 |  9000/25026 batches | lr 1.00 | ms/batch 152.73 | loss  7.14 | ppl  1263.56\n",
      "| epoch   2 |  9200/25026 batches | lr 1.00 | ms/batch 152.75 | loss  7.12 | ppl  1237.45\n",
      "| epoch   2 |  9400/25026 batches | lr 1.00 | ms/batch 152.80 | loss  7.07 | ppl  1171.71\n",
      "| epoch   2 |  9600/25026 batches | lr 1.00 | ms/batch 152.80 | loss  7.02 | ppl  1116.97\n",
      "| epoch   2 |  9800/25026 batches | lr 1.00 | ms/batch 152.69 | loss  7.17 | ppl  1294.00\n",
      "| epoch   2 | 10000/25026 batches | lr 1.00 | ms/batch 152.80 | loss  7.08 | ppl  1186.85\n",
      "| epoch   2 | 10200/25026 batches | lr 1.00 | ms/batch 152.74 | loss  7.02 | ppl  1117.24\n",
      "| epoch   2 | 10400/25026 batches | lr 1.00 | ms/batch 152.97 | loss  6.93 | ppl  1020.05\n",
      "| epoch   2 | 10600/25026 batches | lr 1.00 | ms/batch 153.02 | loss  7.02 | ppl  1119.95\n",
      "| epoch   2 | 10800/25026 batches | lr 1.00 | ms/batch 152.96 | loss  7.03 | ppl  1132.67\n",
      "| epoch   2 | 11000/25026 batches | lr 1.00 | ms/batch 152.91 | loss  7.15 | ppl  1275.47\n",
      "| epoch   2 | 11200/25026 batches | lr 1.00 | ms/batch 152.86 | loss  7.02 | ppl  1121.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 11400/25026 batches | lr 1.00 | ms/batch 152.92 | loss  7.07 | ppl  1170.40\n",
      "| epoch   2 | 11600/25026 batches | lr 1.00 | ms/batch 152.90 | loss  6.94 | ppl  1028.05\n",
      "| epoch   2 | 11800/25026 batches | lr 1.00 | ms/batch 152.81 | loss  7.17 | ppl  1300.87\n",
      "| epoch   2 | 12000/25026 batches | lr 1.00 | ms/batch 152.89 | loss  6.85 | ppl   944.62\n",
      "| epoch   2 | 12200/25026 batches | lr 1.00 | ms/batch 152.79 | loss  6.88 | ppl   970.51\n",
      "| epoch   2 | 12400/25026 batches | lr 1.00 | ms/batch 152.83 | loss  7.03 | ppl  1129.12\n",
      "| epoch   2 | 12600/25026 batches | lr 1.00 | ms/batch 152.89 | loss  7.13 | ppl  1246.17\n",
      "| epoch   2 | 12800/25026 batches | lr 1.00 | ms/batch 152.82 | loss  7.11 | ppl  1220.92\n",
      "| epoch   2 | 13000/25026 batches | lr 1.00 | ms/batch 152.81 | loss  7.14 | ppl  1265.39\n",
      "| epoch   2 | 13200/25026 batches | lr 1.00 | ms/batch 152.92 | loss  6.99 | ppl  1085.51\n",
      "| epoch   2 | 13400/25026 batches | lr 1.00 | ms/batch 152.84 | loss  6.90 | ppl   988.65\n",
      "| epoch   2 | 13600/25026 batches | lr 1.00 | ms/batch 152.96 | loss  7.05 | ppl  1151.26\n",
      "| epoch   2 | 13800/25026 batches | lr 1.00 | ms/batch 152.94 | loss  7.05 | ppl  1151.16\n",
      "| epoch   2 | 14000/25026 batches | lr 1.00 | ms/batch 152.91 | loss  6.96 | ppl  1055.50\n",
      "| epoch   2 | 14200/25026 batches | lr 1.00 | ms/batch 152.91 | loss  6.99 | ppl  1085.57\n",
      "| epoch   2 | 14400/25026 batches | lr 1.00 | ms/batch 152.90 | loss  6.96 | ppl  1057.58\n",
      "| epoch   2 | 14600/25026 batches | lr 1.00 | ms/batch 152.89 | loss  6.86 | ppl   952.26\n",
      "| epoch   2 | 14800/25026 batches | lr 1.00 | ms/batch 152.91 | loss  6.78 | ppl   877.10\n",
      "| epoch   2 | 15000/25026 batches | lr 1.00 | ms/batch 152.83 | loss  6.98 | ppl  1072.20\n",
      "| epoch   2 | 15200/25026 batches | lr 1.00 | ms/batch 152.81 | loss  6.99 | ppl  1084.94\n",
      "| epoch   2 | 15400/25026 batches | lr 1.00 | ms/batch 152.83 | loss  6.92 | ppl  1008.03\n",
      "| epoch   2 | 15600/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.89 | ppl   981.06\n",
      "| epoch   2 | 15800/25026 batches | lr 1.00 | ms/batch 152.96 | loss  6.83 | ppl   923.21\n",
      "| epoch   2 | 16000/25026 batches | lr 1.00 | ms/batch 152.78 | loss  6.93 | ppl  1022.86\n",
      "| epoch   2 | 16200/25026 batches | lr 1.00 | ms/batch 152.89 | loss  6.86 | ppl   950.16\n",
      "| epoch   2 | 16400/25026 batches | lr 1.00 | ms/batch 152.86 | loss  7.08 | ppl  1192.21\n",
      "| epoch   2 | 16600/25026 batches | lr 1.00 | ms/batch 152.90 | loss  6.95 | ppl  1046.85\n",
      "| epoch   2 | 16800/25026 batches | lr 1.00 | ms/batch 152.85 | loss  6.94 | ppl  1037.08\n",
      "| epoch   2 | 17000/25026 batches | lr 1.00 | ms/batch 152.84 | loss  6.96 | ppl  1052.79\n",
      "| epoch   2 | 17200/25026 batches | lr 1.00 | ms/batch 152.82 | loss  6.95 | ppl  1039.54\n",
      "| epoch   2 | 17400/25026 batches | lr 1.00 | ms/batch 152.94 | loss  6.89 | ppl   980.78\n",
      "| epoch   2 | 17600/25026 batches | lr 1.00 | ms/batch 153.09 | loss  6.91 | ppl  1003.06\n",
      "| epoch   2 | 17800/25026 batches | lr 1.00 | ms/batch 152.97 | loss  7.01 | ppl  1107.75\n",
      "| epoch   2 | 18000/25026 batches | lr 1.00 | ms/batch 152.96 | loss  6.96 | ppl  1055.40\n",
      "| epoch   2 | 18200/25026 batches | lr 1.00 | ms/batch 153.05 | loss  7.06 | ppl  1164.59\n",
      "| epoch   2 | 18400/25026 batches | lr 1.00 | ms/batch 152.92 | loss  6.82 | ppl   914.98\n",
      "| epoch   2 | 18600/25026 batches | lr 1.00 | ms/batch 152.90 | loss  6.91 | ppl  1001.58\n",
      "| epoch   2 | 18800/25026 batches | lr 1.00 | ms/batch 153.00 | loss  6.75 | ppl   856.39\n",
      "| epoch   2 | 19000/25026 batches | lr 1.00 | ms/batch 153.00 | loss  7.00 | ppl  1096.33\n",
      "| epoch   2 | 19200/25026 batches | lr 1.00 | ms/batch 152.93 | loss  7.09 | ppl  1198.01\n",
      "| epoch   2 | 19400/25026 batches | lr 1.00 | ms/batch 152.91 | loss  7.02 | ppl  1123.79\n",
      "| epoch   2 | 19600/25026 batches | lr 1.00 | ms/batch 152.82 | loss  6.79 | ppl   888.81\n",
      "| epoch   2 | 19800/25026 batches | lr 1.00 | ms/batch 152.84 | loss  6.83 | ppl   926.52\n",
      "| epoch   2 | 20000/25026 batches | lr 1.00 | ms/batch 152.76 | loss  6.94 | ppl  1034.86\n",
      "| epoch   2 | 20200/25026 batches | lr 1.00 | ms/batch 152.76 | loss  7.07 | ppl  1177.18\n",
      "| epoch   2 | 20400/25026 batches | lr 1.00 | ms/batch 152.73 | loss  6.85 | ppl   946.66\n",
      "| epoch   2 | 20600/25026 batches | lr 1.00 | ms/batch 152.76 | loss  6.93 | ppl  1020.17\n",
      "| epoch   2 | 20800/25026 batches | lr 1.00 | ms/batch 152.71 | loss  6.72 | ppl   829.65\n",
      "| epoch   2 | 21000/25026 batches | lr 1.00 | ms/batch 152.79 | loss  6.91 | ppl  1002.46\n",
      "| epoch   2 | 21200/25026 batches | lr 1.00 | ms/batch 152.85 | loss  6.94 | ppl  1033.68\n",
      "| epoch   2 | 21400/25026 batches | lr 1.00 | ms/batch 153.47 | loss  6.77 | ppl   871.60\n",
      "| epoch   2 | 21600/25026 batches | lr 1.00 | ms/batch 155.46 | loss  7.01 | ppl  1106.11\n",
      "| epoch   2 | 21800/25026 batches | lr 1.00 | ms/batch 153.51 | loss  6.90 | ppl   987.66\n",
      "| epoch   2 | 22000/25026 batches | lr 1.00 | ms/batch 153.53 | loss  6.94 | ppl  1033.18\n",
      "| epoch   2 | 22200/25026 batches | lr 1.00 | ms/batch 153.60 | loss  6.98 | ppl  1073.31\n",
      "| epoch   2 | 22400/25026 batches | lr 1.00 | ms/batch 153.42 | loss  6.94 | ppl  1037.39\n",
      "| epoch   2 | 22600/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.95 | ppl  1044.95\n",
      "| epoch   2 | 22800/25026 batches | lr 1.00 | ms/batch 153.07 | loss  6.88 | ppl   977.38\n",
      "| epoch   2 | 23000/25026 batches | lr 1.00 | ms/batch 152.91 | loss  6.84 | ppl   939.02\n",
      "| epoch   2 | 23200/25026 batches | lr 1.00 | ms/batch 152.88 | loss  6.96 | ppl  1051.19\n",
      "| epoch   2 | 23400/25026 batches | lr 1.00 | ms/batch 152.86 | loss  6.63 | ppl   760.44\n",
      "| epoch   2 | 23600/25026 batches | lr 1.00 | ms/batch 152.86 | loss  6.80 | ppl   896.32\n",
      "| epoch   2 | 23800/25026 batches | lr 1.00 | ms/batch 152.89 | loss  6.90 | ppl   989.57\n",
      "| epoch   2 | 24000/25026 batches | lr 1.00 | ms/batch 152.84 | loss  6.78 | ppl   882.89\n",
      "| epoch   2 | 24200/25026 batches | lr 1.00 | ms/batch 152.73 | loss  6.57 | ppl   716.33\n",
      "| epoch   2 | 24400/25026 batches | lr 1.00 | ms/batch 152.72 | loss  6.89 | ppl   981.31\n",
      "| epoch   2 | 24600/25026 batches | lr 1.00 | ms/batch 152.64 | loss  6.97 | ppl  1065.10\n",
      "| epoch   2 | 24800/25026 batches | lr 1.00 | ms/batch 152.68 | loss  7.03 | ppl  1130.45\n",
      "| epoch   2 | 25000/25026 batches | lr 1.00 | ms/batch 152.66 | loss  6.99 | ppl  1083.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 4028.06s | valid loss  7.17 | valid ppl  1300.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/25026 batches | lr 1.00 | ms/batch 153.68 | loss  6.96 | ppl  1058.70\n",
      "| epoch   3 |   400/25026 batches | lr 1.00 | ms/batch 152.91 | loss  6.91 | ppl   999.32\n",
      "| epoch   3 |   600/25026 batches | lr 1.00 | ms/batch 152.78 | loss  6.91 | ppl  1000.13\n",
      "| epoch   3 |   800/25026 batches | lr 1.00 | ms/batch 152.72 | loss  6.89 | ppl   978.42\n",
      "| epoch   3 |  1000/25026 batches | lr 1.00 | ms/batch 152.82 | loss  6.78 | ppl   881.65\n",
      "| epoch   3 |  1200/25026 batches | lr 1.00 | ms/batch 152.76 | loss  6.63 | ppl   754.35\n",
      "| epoch   3 |  1400/25026 batches | lr 1.00 | ms/batch 152.77 | loss  6.83 | ppl   922.82\n",
      "| epoch   3 |  1600/25026 batches | lr 1.00 | ms/batch 152.69 | loss  6.99 | ppl  1082.13\n",
      "| epoch   3 |  1800/25026 batches | lr 1.00 | ms/batch 152.77 | loss  7.08 | ppl  1191.39\n",
      "| epoch   3 |  2000/25026 batches | lr 1.00 | ms/batch 152.77 | loss  7.21 | ppl  1346.40\n",
      "| epoch   3 |  2200/25026 batches | lr 1.00 | ms/batch 152.72 | loss  6.96 | ppl  1049.96\n",
      "| epoch   3 |  2400/25026 batches | lr 1.00 | ms/batch 152.66 | loss  7.02 | ppl  1116.21\n",
      "| epoch   3 |  2600/25026 batches | lr 1.00 | ms/batch 153.13 | loss  6.98 | ppl  1074.94\n",
      "| epoch   3 |  2800/25026 batches | lr 1.00 | ms/batch 152.95 | loss  6.77 | ppl   869.42\n",
      "| epoch   3 |  3000/25026 batches | lr 1.00 | ms/batch 152.86 | loss  6.80 | ppl   896.09\n",
      "| epoch   3 |  3200/25026 batches | lr 1.00 | ms/batch 152.82 | loss  6.83 | ppl   920.95\n",
      "| epoch   3 |  3400/25026 batches | lr 1.00 | ms/batch 152.86 | loss  6.70 | ppl   815.54\n",
      "| epoch   3 |  3600/25026 batches | lr 1.00 | ms/batch 153.05 | loss  6.77 | ppl   871.34\n",
      "| epoch   3 |  3800/25026 batches | lr 1.00 | ms/batch 152.92 | loss  6.75 | ppl   854.51\n",
      "| epoch   3 |  4000/25026 batches | lr 1.00 | ms/batch 152.80 | loss  6.73 | ppl   833.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  4200/25026 batches | lr 1.00 | ms/batch 152.79 | loss  6.86 | ppl   952.50\n",
      "| epoch   3 |  4400/25026 batches | lr 1.00 | ms/batch 152.76 | loss  6.66 | ppl   777.90\n",
      "| epoch   3 |  4600/25026 batches | lr 1.00 | ms/batch 152.78 | loss  6.63 | ppl   754.05\n",
      "| epoch   3 |  4800/25026 batches | lr 1.00 | ms/batch 152.77 | loss  6.78 | ppl   878.62\n",
      "| epoch   3 |  5000/25026 batches | lr 1.00 | ms/batch 152.66 | loss  6.70 | ppl   811.28\n",
      "| epoch   3 |  5200/25026 batches | lr 1.00 | ms/batch 152.86 | loss  6.66 | ppl   780.31\n",
      "| epoch   3 |  5400/25026 batches | lr 1.00 | ms/batch 152.83 | loss  6.69 | ppl   805.92\n",
      "| epoch   3 |  5600/25026 batches | lr 1.00 | ms/batch 153.12 | loss  6.61 | ppl   745.80\n",
      "| epoch   3 |  5800/25026 batches | lr 1.00 | ms/batch 153.11 | loss  6.60 | ppl   736.28\n",
      "| epoch   3 |  6000/25026 batches | lr 1.00 | ms/batch 153.06 | loss  6.94 | ppl  1034.48\n",
      "| epoch   3 |  6200/25026 batches | lr 1.00 | ms/batch 153.12 | loss  6.83 | ppl   924.70\n",
      "| epoch   3 |  6400/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.68 | ppl   799.84\n",
      "| epoch   3 |  6600/25026 batches | lr 1.00 | ms/batch 153.58 | loss  6.82 | ppl   917.04\n",
      "| epoch   3 |  6800/25026 batches | lr 1.00 | ms/batch 153.49 | loss  6.96 | ppl  1057.31\n",
      "| epoch   3 |  7000/25026 batches | lr 1.00 | ms/batch 153.48 | loss  6.80 | ppl   900.90\n",
      "| epoch   3 |  7200/25026 batches | lr 1.00 | ms/batch 153.72 | loss  6.84 | ppl   932.38\n",
      "| epoch   3 |  7400/25026 batches | lr 1.00 | ms/batch 153.61 | loss  6.61 | ppl   745.81\n",
      "| epoch   3 |  7600/25026 batches | lr 1.00 | ms/batch 155.22 | loss  6.81 | ppl   904.42\n",
      "| epoch   3 |  7800/25026 batches | lr 1.00 | ms/batch 153.36 | loss  6.71 | ppl   823.25\n",
      "| epoch   3 |  8000/25026 batches | lr 1.00 | ms/batch 153.70 | loss  6.79 | ppl   886.16\n",
      "| epoch   3 |  8200/25026 batches | lr 1.00 | ms/batch 153.75 | loss  6.54 | ppl   691.79\n",
      "| epoch   3 |  8400/25026 batches | lr 1.00 | ms/batch 153.61 | loss  6.63 | ppl   760.41\n",
      "| epoch   3 |  8600/25026 batches | lr 1.00 | ms/batch 153.36 | loss  6.68 | ppl   797.92\n",
      "| epoch   3 |  8800/25026 batches | lr 1.00 | ms/batch 153.38 | loss  6.83 | ppl   927.70\n",
      "| epoch   3 |  9000/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.77 | ppl   868.47\n",
      "| epoch   3 |  9200/25026 batches | lr 1.00 | ms/batch 153.62 | loss  6.76 | ppl   864.24\n",
      "| epoch   3 |  9400/25026 batches | lr 1.00 | ms/batch 153.61 | loss  6.72 | ppl   830.04\n",
      "| epoch   3 |  9600/25026 batches | lr 1.00 | ms/batch 153.49 | loss  6.65 | ppl   772.72\n",
      "| epoch   3 |  9800/25026 batches | lr 1.00 | ms/batch 153.46 | loss  6.82 | ppl   912.47\n",
      "| epoch   3 | 10000/25026 batches | lr 1.00 | ms/batch 153.51 | loss  6.72 | ppl   829.44\n",
      "| epoch   3 | 10200/25026 batches | lr 1.00 | ms/batch 153.50 | loss  6.67 | ppl   788.54\n",
      "| epoch   3 | 10400/25026 batches | lr 1.00 | ms/batch 153.52 | loss  6.57 | ppl   715.12\n",
      "| epoch   3 | 10600/25026 batches | lr 1.00 | ms/batch 153.51 | loss  6.68 | ppl   798.98\n",
      "| epoch   3 | 10800/25026 batches | lr 1.00 | ms/batch 155.12 | loss  6.68 | ppl   797.91\n",
      "| epoch   3 | 11000/25026 batches | lr 1.00 | ms/batch 153.60 | loss  6.81 | ppl   908.98\n",
      "| epoch   3 | 11200/25026 batches | lr 1.00 | ms/batch 153.71 | loss  6.66 | ppl   782.99\n",
      "| epoch   3 | 11400/25026 batches | lr 1.00 | ms/batch 154.95 | loss  6.72 | ppl   830.84\n",
      "| epoch   3 | 11600/25026 batches | lr 1.00 | ms/batch 154.20 | loss  6.58 | ppl   717.44\n",
      "| epoch   3 | 11800/25026 batches | lr 1.00 | ms/batch 154.16 | loss  6.83 | ppl   924.96\n",
      "| epoch   3 | 12000/25026 batches | lr 1.00 | ms/batch 153.95 | loss  6.50 | ppl   664.98\n",
      "| epoch   3 | 12200/25026 batches | lr 1.00 | ms/batch 153.86 | loss  6.53 | ppl   685.68\n",
      "| epoch   3 | 12400/25026 batches | lr 1.00 | ms/batch 154.95 | loss  6.69 | ppl   807.14\n",
      "| epoch   3 | 12600/25026 batches | lr 1.00 | ms/batch 153.48 | loss  6.79 | ppl   892.74\n",
      "| epoch   3 | 12800/25026 batches | lr 1.00 | ms/batch 153.47 | loss  6.77 | ppl   872.88\n",
      "| epoch   3 | 13000/25026 batches | lr 1.00 | ms/batch 153.69 | loss  6.81 | ppl   902.86\n",
      "| epoch   3 | 13200/25026 batches | lr 1.00 | ms/batch 153.46 | loss  6.64 | ppl   766.97\n",
      "| epoch   3 | 13400/25026 batches | lr 1.00 | ms/batch 153.45 | loss  6.56 | ppl   708.43\n",
      "| epoch   3 | 13600/25026 batches | lr 1.00 | ms/batch 153.50 | loss  6.72 | ppl   825.59\n",
      "| epoch   3 | 13800/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.70 | ppl   813.85\n",
      "| epoch   3 | 14000/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.62 | ppl   751.88\n",
      "| epoch   3 | 14200/25026 batches | lr 1.00 | ms/batch 153.39 | loss  6.66 | ppl   781.35\n",
      "| epoch   3 | 14400/25026 batches | lr 1.00 | ms/batch 154.27 | loss  6.64 | ppl   768.59\n",
      "| epoch   3 | 14600/25026 batches | lr 1.00 | ms/batch 153.42 | loss  6.52 | ppl   676.58\n",
      "| epoch   3 | 14800/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.46 | ppl   641.48\n",
      "| epoch   3 | 15000/25026 batches | lr 1.00 | ms/batch 268.74 | loss  6.66 | ppl   781.39\n",
      "| epoch   3 | 15200/25026 batches | lr 1.00 | ms/batch 152.47 | loss  6.67 | ppl   784.71\n",
      "| epoch   3 | 15400/25026 batches | lr 1.00 | ms/batch 152.76 | loss  6.58 | ppl   721.36\n",
      "| epoch   3 | 15600/25026 batches | lr 1.00 | ms/batch 152.79 | loss  6.55 | ppl   699.73\n",
      "| epoch   3 | 15800/25026 batches | lr 1.00 | ms/batch 152.94 | loss  6.49 | ppl   656.68\n",
      "| epoch   3 | 16000/25026 batches | lr 1.00 | ms/batch 153.02 | loss  6.61 | ppl   745.68\n",
      "| epoch   3 | 16200/25026 batches | lr 1.00 | ms/batch 153.13 | loss  6.53 | ppl   684.99\n",
      "| epoch   3 | 16400/25026 batches | lr 1.00 | ms/batch 153.10 | loss  6.74 | ppl   848.28\n",
      "| epoch   3 | 16600/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.64 | ppl   763.50\n",
      "| epoch   3 | 16800/25026 batches | lr 1.00 | ms/batch 153.16 | loss  6.63 | ppl   754.95\n",
      "| epoch   3 | 17000/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.64 | ppl   763.99\n",
      "| epoch   3 | 17200/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.63 | ppl   754.37\n",
      "| epoch   3 | 17400/25026 batches | lr 1.00 | ms/batch 153.35 | loss  6.58 | ppl   717.79\n",
      "| epoch   3 | 17600/25026 batches | lr 1.00 | ms/batch 153.30 | loss  6.60 | ppl   737.78\n",
      "| epoch   3 | 17800/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.70 | ppl   814.62\n",
      "| epoch   3 | 18000/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.63 | ppl   760.46\n",
      "| epoch   3 | 18200/25026 batches | lr 1.00 | ms/batch 153.30 | loss  6.75 | ppl   856.59\n",
      "| epoch   3 | 18400/25026 batches | lr 1.00 | ms/batch 153.38 | loss  6.51 | ppl   672.40\n",
      "| epoch   3 | 18600/25026 batches | lr 1.00 | ms/batch 153.37 | loss  6.58 | ppl   717.82\n",
      "| epoch   3 | 18800/25026 batches | lr 1.00 | ms/batch 153.40 | loss  6.43 | ppl   622.05\n",
      "| epoch   3 | 19000/25026 batches | lr 1.00 | ms/batch 153.48 | loss  6.68 | ppl   794.43\n",
      "| epoch   3 | 19200/25026 batches | lr 1.00 | ms/batch 153.42 | loss  6.79 | ppl   884.94\n",
      "| epoch   3 | 19400/25026 batches | lr 1.00 | ms/batch 153.39 | loss  6.72 | ppl   826.58\n",
      "| epoch   3 | 19600/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.48 | ppl   650.87\n",
      "| epoch   3 | 19800/25026 batches | lr 1.00 | ms/batch 153.35 | loss  6.53 | ppl   686.31\n",
      "| epoch   3 | 20000/25026 batches | lr 1.00 | ms/batch 153.35 | loss  6.64 | ppl   767.40\n",
      "| epoch   3 | 20200/25026 batches | lr 1.00 | ms/batch 153.34 | loss  6.78 | ppl   877.45\n",
      "| epoch   3 | 20400/25026 batches | lr 1.00 | ms/batch 153.34 | loss  6.57 | ppl   711.46\n",
      "| epoch   3 | 20600/25026 batches | lr 1.00 | ms/batch 153.35 | loss  6.64 | ppl   765.49\n",
      "| epoch   3 | 20800/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.42 | ppl   616.71\n",
      "| epoch   3 | 21000/25026 batches | lr 1.00 | ms/batch 153.35 | loss  6.63 | ppl   754.90\n",
      "| epoch   3 | 21200/25026 batches | lr 1.00 | ms/batch 153.35 | loss  6.66 | ppl   778.96\n",
      "| epoch   3 | 21400/25026 batches | lr 1.00 | ms/batch 153.39 | loss  6.48 | ppl   652.54\n",
      "| epoch   3 | 21600/25026 batches | lr 1.00 | ms/batch 153.39 | loss  6.72 | ppl   828.12\n",
      "| epoch   3 | 21800/25026 batches | lr 1.00 | ms/batch 153.36 | loss  6.60 | ppl   733.87\n",
      "| epoch   3 | 22000/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.64 | ppl   767.63\n",
      "| epoch   3 | 22200/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.68 | ppl   799.62\n",
      "| epoch   3 | 22400/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.66 | ppl   782.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 22600/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.64 | ppl   764.64\n",
      "| epoch   3 | 22800/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.59 | ppl   728.13\n",
      "| epoch   3 | 23000/25026 batches | lr 1.00 | ms/batch 153.50 | loss  6.56 | ppl   704.94\n",
      "| epoch   3 | 23200/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.66 | ppl   784.18\n",
      "| epoch   3 | 23400/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.33 | ppl   562.86\n",
      "| epoch   3 | 23600/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.50 | ppl   666.52\n",
      "| epoch   3 | 23800/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.61 | ppl   743.67\n",
      "| epoch   3 | 24000/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.50 | ppl   668.29\n",
      "| epoch   3 | 24200/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.28 | ppl   534.37\n",
      "| epoch   3 | 24400/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.60 | ppl   738.66\n",
      "| epoch   3 | 24600/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.69 | ppl   804.91\n",
      "| epoch   3 | 24800/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.77 | ppl   870.03\n",
      "| epoch   3 | 25000/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.71 | ppl   822.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 4060.09s | valid loss  7.03 | valid ppl  1127.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/25026 batches | lr 1.00 | ms/batch 154.11 | loss  6.70 | ppl   810.90\n",
      "| epoch   4 |   400/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.64 | ppl   765.15\n",
      "| epoch   4 |   600/25026 batches | lr 1.00 | ms/batch 153.48 | loss  6.62 | ppl   748.54\n",
      "| epoch   4 |   800/25026 batches | lr 1.00 | ms/batch 153.30 | loss  6.61 | ppl   742.98\n",
      "| epoch   4 |  1000/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.50 | ppl   661.85\n",
      "| epoch   4 |  1200/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.34 | ppl   565.08\n",
      "| epoch   4 |  1400/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.55 | ppl   699.80\n",
      "| epoch   4 |  1600/25026 batches | lr 1.00 | ms/batch 154.25 | loss  6.72 | ppl   827.53\n",
      "| epoch   4 |  1800/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.81 | ppl   909.83\n",
      "| epoch   4 |  2000/25026 batches | lr 1.00 | ms/batch 153.30 | loss  6.94 | ppl  1031.11\n",
      "| epoch   4 |  2200/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.69 | ppl   802.54\n",
      "| epoch   4 |  2400/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.75 | ppl   855.32\n",
      "| epoch   4 |  2600/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.71 | ppl   822.63\n",
      "| epoch   4 |  2800/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.49 | ppl   657.49\n",
      "| epoch   4 |  3000/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.54 | ppl   691.13\n",
      "| epoch   4 |  3200/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.55 | ppl   699.11\n",
      "| epoch   4 |  3400/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.43 | ppl   622.71\n",
      "| epoch   4 |  3600/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.50 | ppl   663.33\n",
      "| epoch   4 |  3800/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.48 | ppl   651.03\n",
      "| epoch   4 |  4000/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.46 | ppl   635.93\n",
      "| epoch   4 |  4200/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.60 | ppl   738.58\n",
      "| epoch   4 |  4400/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.40 | ppl   599.94\n",
      "| epoch   4 |  4600/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.36 | ppl   578.72\n",
      "| epoch   4 |  4800/25026 batches | lr 1.00 | ms/batch 153.14 | loss  6.53 | ppl   682.47\n",
      "| epoch   4 |  5000/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.43 | ppl   622.63\n",
      "| epoch   4 |  5200/25026 batches | lr 1.00 | ms/batch 153.16 | loss  6.40 | ppl   599.77\n",
      "| epoch   4 |  5400/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.43 | ppl   623.06\n",
      "| epoch   4 |  5600/25026 batches | lr 1.00 | ms/batch 153.14 | loss  6.36 | ppl   578.63\n",
      "| epoch   4 |  5800/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.33 | ppl   562.70\n",
      "| epoch   4 |  6000/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.68 | ppl   796.00\n",
      "| epoch   4 |  6200/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.57 | ppl   713.18\n",
      "| epoch   4 |  6400/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.44 | ppl   624.07\n",
      "| epoch   4 |  6600/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.55 | ppl   700.68\n",
      "| epoch   4 |  6800/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.72 | ppl   831.42\n",
      "| epoch   4 |  7000/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.54 | ppl   692.09\n",
      "| epoch   4 |  7200/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.59 | ppl   726.50\n",
      "| epoch   4 |  7400/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.35 | ppl   574.51\n",
      "| epoch   4 |  7600/25026 batches | lr 1.00 | ms/batch 153.14 | loss  6.55 | ppl   697.97\n",
      "| epoch   4 |  7800/25026 batches | lr 1.00 | ms/batch 153.10 | loss  6.46 | ppl   637.17\n",
      "| epoch   4 |  8000/25026 batches | lr 1.00 | ms/batch 153.16 | loss  6.53 | ppl   685.96\n",
      "| epoch   4 |  8200/25026 batches | lr 1.00 | ms/batch 153.10 | loss  6.29 | ppl   540.62\n",
      "| epoch   4 |  8400/25026 batches | lr 1.00 | ms/batch 153.47 | loss  6.37 | ppl   584.55\n",
      "| epoch   4 |  8600/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.43 | ppl   618.15\n",
      "| epoch   4 |  8800/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.59 | ppl   727.47\n",
      "| epoch   4 |  9000/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.50 | ppl   664.91\n",
      "| epoch   4 |  9200/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.51 | ppl   668.96\n",
      "| epoch   4 |  9400/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.49 | ppl   656.87\n",
      "| epoch   4 |  9600/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.40 | ppl   600.01\n",
      "| epoch   4 |  9800/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.57 | ppl   712.10\n",
      "| epoch   4 | 10000/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.47 | ppl   642.27\n",
      "| epoch   4 | 10200/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.43 | ppl   617.27\n",
      "| epoch   4 | 10400/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.32 | ppl   558.14\n",
      "| epoch   4 | 10600/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.45 | ppl   632.88\n",
      "| epoch   4 | 10800/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.43 | ppl   619.16\n",
      "| epoch   4 | 11000/25026 batches | lr 1.00 | ms/batch 153.12 | loss  6.57 | ppl   711.66\n",
      "| epoch   4 | 11200/25026 batches | lr 1.00 | ms/batch 153.16 | loss  6.41 | ppl   610.67\n",
      "| epoch   4 | 11400/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.47 | ppl   647.29\n",
      "| epoch   4 | 11600/25026 batches | lr 1.00 | ms/batch 153.11 | loss  6.32 | ppl   553.90\n",
      "| epoch   4 | 11800/25026 batches | lr 1.00 | ms/batch 153.10 | loss  6.58 | ppl   722.17\n",
      "| epoch   4 | 12000/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.26 | ppl   523.98\n",
      "| epoch   4 | 12200/25026 batches | lr 1.00 | ms/batch 153.14 | loss  6.28 | ppl   535.92\n",
      "| epoch   4 | 12400/25026 batches | lr 1.00 | ms/batch 153.36 | loss  6.46 | ppl   636.22\n",
      "| epoch   4 | 12600/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.55 | ppl   699.03\n",
      "| epoch   4 | 12800/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.53 | ppl   683.74\n",
      "| epoch   4 | 13000/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.57 | ppl   710.73\n",
      "| epoch   4 | 13200/25026 batches | lr 1.00 | ms/batch 153.14 | loss  6.39 | ppl   595.66\n",
      "| epoch   4 | 13400/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.32 | ppl   557.59\n",
      "| epoch   4 | 13600/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.48 | ppl   650.31\n",
      "| epoch   4 | 13800/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.44 | ppl   629.05\n",
      "| epoch   4 | 14000/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.38 | ppl   589.89\n",
      "| epoch   4 | 14200/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.42 | ppl   615.82\n",
      "| epoch   4 | 14400/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.41 | ppl   610.61\n",
      "| epoch   4 | 14600/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.28 | ppl   531.95\n",
      "| epoch   4 | 14800/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.24 | ppl   511.83\n",
      "| epoch   4 | 15000/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.43 | ppl   620.59\n",
      "| epoch   4 | 15200/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.43 | ppl   621.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 15400/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.34 | ppl   568.39\n",
      "| epoch   4 | 15600/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.31 | ppl   549.88\n",
      "| epoch   4 | 15800/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.25 | ppl   516.68\n",
      "| epoch   4 | 16000/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.38 | ppl   590.59\n",
      "| epoch   4 | 16200/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.29 | ppl   539.06\n",
      "| epoch   4 | 16400/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.51 | ppl   670.83\n",
      "| epoch   4 | 16600/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.41 | ppl   607.61\n",
      "| epoch   4 | 16800/25026 batches | lr 1.00 | ms/batch 153.14 | loss  6.39 | ppl   598.68\n",
      "| epoch   4 | 17000/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.41 | ppl   606.29\n",
      "| epoch   4 | 17200/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.40 | ppl   599.27\n",
      "| epoch   4 | 17400/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.35 | ppl   570.70\n",
      "| epoch   4 | 17600/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.38 | ppl   590.60\n",
      "| epoch   4 | 17800/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.47 | ppl   642.30\n",
      "| epoch   4 | 18000/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.39 | ppl   595.92\n",
      "| epoch   4 | 18200/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.53 | ppl   684.23\n",
      "| epoch   4 | 18400/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.27 | ppl   529.59\n",
      "| epoch   4 | 18600/25026 batches | lr 1.00 | ms/batch 153.14 | loss  6.33 | ppl   560.94\n",
      "| epoch   4 | 18800/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.19 | ppl   487.97\n",
      "| epoch   4 | 19000/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.44 | ppl   625.62\n",
      "| epoch   4 | 19200/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.56 | ppl   704.30\n",
      "| epoch   4 | 19400/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.48 | ppl   654.06\n",
      "| epoch   4 | 19600/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.25 | ppl   518.05\n",
      "| epoch   4 | 19800/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.31 | ppl   547.70\n",
      "| epoch   4 | 20000/25026 batches | lr 1.00 | ms/batch 153.16 | loss  6.42 | ppl   613.73\n",
      "| epoch   4 | 20200/25026 batches | lr 1.00 | ms/batch 153.40 | loss  6.55 | ppl   699.75\n",
      "| epoch   4 | 20400/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.34 | ppl   568.80\n",
      "| epoch   4 | 20600/25026 batches | lr 1.00 | ms/batch 154.65 | loss  6.42 | ppl   614.03\n",
      "| epoch   4 | 20800/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.20 | ppl   493.49\n",
      "| epoch   4 | 21000/25026 batches | lr 1.00 | ms/batch 153.14 | loss  6.42 | ppl   611.42\n",
      "| epoch   4 | 21200/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.44 | ppl   627.30\n",
      "| epoch   4 | 21400/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.27 | ppl   525.99\n",
      "| epoch   4 | 21600/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.51 | ppl   668.59\n",
      "| epoch   4 | 21800/25026 batches | lr 1.00 | ms/batch 153.16 | loss  6.37 | ppl   585.68\n",
      "| epoch   4 | 22000/25026 batches | lr 1.00 | ms/batch 153.11 | loss  6.42 | ppl   612.78\n",
      "| epoch   4 | 22200/25026 batches | lr 1.00 | ms/batch 153.14 | loss  6.46 | ppl   642.16\n",
      "| epoch   4 | 22400/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.44 | ppl   628.57\n",
      "| epoch   4 | 22600/25026 batches | lr 1.00 | ms/batch 153.16 | loss  6.41 | ppl   605.20\n",
      "| epoch   4 | 22800/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.36 | ppl   580.42\n",
      "| epoch   4 | 23000/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.35 | ppl   571.39\n",
      "| epoch   4 | 23200/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.45 | ppl   629.95\n",
      "| epoch   4 | 23400/25026 batches | lr 1.00 | ms/batch 153.34 | loss  6.11 | ppl   450.64\n",
      "| epoch   4 | 23600/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.28 | ppl   532.56\n",
      "| epoch   4 | 23800/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.39 | ppl   596.98\n",
      "| epoch   4 | 24000/25026 batches | lr 1.00 | ms/batch 153.36 | loss  6.29 | ppl   541.16\n",
      "| epoch   4 | 24200/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.06 | ppl   429.58\n",
      "| epoch   4 | 24400/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.38 | ppl   592.86\n",
      "| epoch   4 | 24600/25026 batches | lr 1.00 | ms/batch 153.31 | loss  6.48 | ppl   649.10\n",
      "| epoch   4 | 24800/25026 batches | lr 1.00 | ms/batch 153.34 | loss  6.56 | ppl   706.36\n",
      "| epoch   4 | 25000/25026 batches | lr 1.00 | ms/batch 153.36 | loss  6.50 | ppl   662.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 4035.02s | valid loss  6.95 | valid ppl  1039.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/25026 batches | lr 1.00 | ms/batch 154.18 | loss  6.48 | ppl   654.84\n",
      "| epoch   5 |   400/25026 batches | lr 1.00 | ms/batch 153.43 | loss  6.43 | ppl   618.66\n",
      "| epoch   5 |   600/25026 batches | lr 1.00 | ms/batch 153.35 | loss  6.39 | ppl   598.18\n",
      "| epoch   5 |   800/25026 batches | lr 1.00 | ms/batch 153.37 | loss  6.39 | ppl   596.66\n",
      "| epoch   5 |  1000/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.27 | ppl   530.03\n",
      "| epoch   5 |  1200/25026 batches | lr 1.00 | ms/batch 153.40 | loss  6.11 | ppl   451.93\n",
      "| epoch   5 |  1400/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.33 | ppl   563.87\n",
      "| epoch   5 |  1600/25026 batches | lr 1.00 | ms/batch 154.26 | loss  6.50 | ppl   664.87\n",
      "| epoch   5 |  1800/25026 batches | lr 1.00 | ms/batch 153.79 | loss  6.61 | ppl   740.51\n",
      "| epoch   5 |  2000/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.73 | ppl   835.80\n",
      "| epoch   5 |  2200/25026 batches | lr 1.00 | ms/batch 153.39 | loss  6.47 | ppl   647.58\n",
      "| epoch   5 |  2400/25026 batches | lr 1.00 | ms/batch 153.31 | loss  6.55 | ppl   696.16\n",
      "| epoch   5 |  2600/25026 batches | lr 1.00 | ms/batch 153.35 | loss  6.50 | ppl   663.85\n",
      "| epoch   5 |  2800/25026 batches | lr 1.00 | ms/batch 153.41 | loss  6.27 | ppl   529.02\n",
      "| epoch   5 |  3000/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.34 | ppl   564.15\n",
      "| epoch   5 |  3200/25026 batches | lr 1.00 | ms/batch 153.30 | loss  6.34 | ppl   565.35\n",
      "| epoch   5 |  3400/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.22 | ppl   504.64\n",
      "| epoch   5 |  3600/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.28 | ppl   534.12\n",
      "| epoch   5 |  3800/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.26 | ppl   523.28\n",
      "| epoch   5 |  4000/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.24 | ppl   513.89\n",
      "| epoch   5 |  4200/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.41 | ppl   608.52\n",
      "| epoch   5 |  4400/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.20 | ppl   491.21\n",
      "| epoch   5 |  4600/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.16 | ppl   473.04\n",
      "| epoch   5 |  4800/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.33 | ppl   558.85\n",
      "| epoch   5 |  5000/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.23 | ppl   507.16\n",
      "| epoch   5 |  5200/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.19 | ppl   488.93\n",
      "| epoch   5 |  5400/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.23 | ppl   505.45\n",
      "| epoch   5 |  5600/25026 batches | lr 1.00 | ms/batch 153.42 | loss  6.16 | ppl   475.25\n",
      "| epoch   5 |  5800/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.13 | ppl   457.34\n",
      "| epoch   5 |  6000/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.47 | ppl   647.19\n",
      "| epoch   5 |  6200/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.36 | ppl   578.82\n",
      "| epoch   5 |  6400/25026 batches | lr 1.00 | ms/batch 153.31 | loss  6.23 | ppl   506.97\n",
      "| epoch   5 |  6600/25026 batches | lr 1.00 | ms/batch 153.38 | loss  6.34 | ppl   567.60\n",
      "| epoch   5 |  6800/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.52 | ppl   677.42\n",
      "| epoch   5 |  7000/25026 batches | lr 1.00 | ms/batch 153.30 | loss  6.33 | ppl   559.44\n",
      "| epoch   5 |  7200/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.38 | ppl   590.75\n",
      "| epoch   5 |  7400/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.14 | ppl   464.60\n",
      "| epoch   5 |  7600/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.34 | ppl   568.17\n",
      "| epoch   5 |  7800/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.26 | ppl   521.64\n",
      "| epoch   5 |  8000/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.32 | ppl   558.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  8200/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.09 | ppl   443.05\n",
      "| epoch   5 |  8400/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.16 | ppl   474.73\n",
      "| epoch   5 |  8600/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.23 | ppl   506.07\n",
      "| epoch   5 |  8800/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.39 | ppl   597.79\n",
      "| epoch   5 |  9000/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.28 | ppl   535.72\n",
      "| epoch   5 |  9200/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.31 | ppl   547.64\n",
      "| epoch   5 |  9400/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.29 | ppl   537.95\n",
      "| epoch   5 |  9600/25026 batches | lr 1.00 | ms/batch 153.40 | loss  6.19 | ppl   488.69\n",
      "| epoch   5 |  9800/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.36 | ppl   580.73\n",
      "| epoch   5 | 10000/25026 batches | lr 1.00 | ms/batch 154.40 | loss  6.26 | ppl   520.72\n",
      "| epoch   5 | 10200/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.22 | ppl   503.39\n",
      "| epoch   5 | 10400/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.13 | ppl   457.66\n",
      "| epoch   5 | 10600/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.26 | ppl   524.24\n",
      "| epoch   5 | 10800/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.23 | ppl   505.95\n",
      "| epoch   5 | 11000/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.37 | ppl   584.83\n",
      "| epoch   5 | 11200/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.20 | ppl   494.80\n",
      "| epoch   5 | 11400/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.28 | ppl   532.72\n",
      "| epoch   5 | 11600/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.11 | ppl   450.38\n",
      "| epoch   5 | 11800/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.37 | ppl   584.61\n",
      "| epoch   5 | 12000/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.07 | ppl   431.27\n",
      "| epoch   5 | 12200/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.09 | ppl   442.11\n",
      "| epoch   5 | 12400/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.26 | ppl   522.69\n",
      "| epoch   5 | 12600/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.35 | ppl   571.86\n",
      "| epoch   5 | 12800/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.33 | ppl   562.90\n",
      "| epoch   5 | 13000/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.37 | ppl   584.54\n",
      "| epoch   5 | 13200/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.19 | ppl   486.70\n",
      "| epoch   5 | 13400/25026 batches | lr 1.00 | ms/batch 153.40 | loss  6.13 | ppl   457.92\n",
      "| epoch   5 | 13600/25026 batches | lr 1.00 | ms/batch 153.34 | loss  6.27 | ppl   530.53\n",
      "| epoch   5 | 13800/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.23 | ppl   507.85\n",
      "| epoch   5 | 14000/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.18 | ppl   484.50\n",
      "| epoch   5 | 14200/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.23 | ppl   506.93\n",
      "| epoch   5 | 14400/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.23 | ppl   505.76\n",
      "| epoch   5 | 14600/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.08 | ppl   437.28\n",
      "| epoch   5 | 14800/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.05 | ppl   424.56\n",
      "| epoch   5 | 15000/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.24 | ppl   515.41\n",
      "| epoch   5 | 15200/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.24 | ppl   512.00\n",
      "| epoch   5 | 15400/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.15 | ppl   467.78\n",
      "| epoch   5 | 15600/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.11 | ppl   452.17\n",
      "| epoch   5 | 15800/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.05 | ppl   422.81\n",
      "| epoch   5 | 16000/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.19 | ppl   488.57\n",
      "| epoch   5 | 16200/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.10 | ppl   447.29\n",
      "| epoch   5 | 16400/25026 batches | lr 1.00 | ms/batch 153.16 | loss  6.30 | ppl   543.44\n",
      "| epoch   5 | 16600/25026 batches | lr 1.00 | ms/batch 153.13 | loss  6.21 | ppl   499.04\n",
      "| epoch   5 | 16800/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.21 | ppl   496.80\n",
      "| epoch   5 | 17000/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.22 | ppl   502.27\n",
      "| epoch   5 | 17200/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.20 | ppl   494.32\n",
      "| epoch   5 | 17400/25026 batches | lr 1.00 | ms/batch 153.46 | loss  6.15 | ppl   469.13\n",
      "| epoch   5 | 17600/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.20 | ppl   490.61\n",
      "| epoch   5 | 17800/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.27 | ppl   527.79\n",
      "| epoch   5 | 18000/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.19 | ppl   487.72\n",
      "| epoch   5 | 18200/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.34 | ppl   564.99\n",
      "| epoch   5 | 18400/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.09 | ppl   439.84\n",
      "| epoch   5 | 18600/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.13 | ppl   457.68\n",
      "| epoch   5 | 18800/25026 batches | lr 1.00 | ms/batch 153.34 | loss  5.99 | ppl   401.02\n",
      "| epoch   5 | 19000/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.25 | ppl   516.77\n",
      "| epoch   5 | 19200/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.36 | ppl   580.89\n",
      "| epoch   5 | 19400/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.29 | ppl   537.85\n",
      "| epoch   5 | 19600/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.06 | ppl   427.85\n",
      "| epoch   5 | 19800/25026 batches | lr 1.00 | ms/batch 153.37 | loss  6.12 | ppl   453.31\n",
      "| epoch   5 | 20000/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.23 | ppl   507.27\n",
      "| epoch   5 | 20200/25026 batches | lr 1.00 | ms/batch 153.30 | loss  6.36 | ppl   577.15\n",
      "| epoch   5 | 20400/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.16 | ppl   472.83\n",
      "| epoch   5 | 20600/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.23 | ppl   509.34\n",
      "| epoch   5 | 20800/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.02 | ppl   411.91\n",
      "| epoch   5 | 21000/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.23 | ppl   509.91\n",
      "| epoch   5 | 21200/25026 batches | lr 1.00 | ms/batch 153.46 | loss  6.26 | ppl   524.99\n",
      "| epoch   5 | 21400/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.08 | ppl   438.31\n",
      "| epoch   5 | 21600/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.32 | ppl   555.86\n",
      "| epoch   5 | 21800/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.18 | ppl   481.48\n",
      "| epoch   5 | 22000/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.22 | ppl   504.28\n",
      "| epoch   5 | 22200/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.28 | ppl   531.69\n",
      "| epoch   5 | 22400/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.26 | ppl   522.55\n",
      "| epoch   5 | 22600/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.21 | ppl   499.44\n",
      "| epoch   5 | 22800/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.18 | ppl   482.31\n",
      "| epoch   5 | 23000/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.17 | ppl   476.20\n",
      "| epoch   5 | 23200/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.26 | ppl   523.88\n",
      "| epoch   5 | 23400/25026 batches | lr 1.00 | ms/batch 153.22 | loss  5.93 | ppl   375.06\n",
      "| epoch   5 | 23600/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.10 | ppl   444.06\n",
      "| epoch   5 | 23800/25026 batches | lr 1.00 | ms/batch 153.18 | loss  6.21 | ppl   497.16\n",
      "| epoch   5 | 24000/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.11 | ppl   449.81\n",
      "| epoch   5 | 24200/25026 batches | lr 1.00 | ms/batch 153.25 | loss  5.88 | ppl   356.46\n",
      "| epoch   5 | 24400/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.20 | ppl   491.94\n",
      "| epoch   5 | 24600/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.29 | ppl   541.85\n",
      "| epoch   5 | 24800/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.39 | ppl   594.11\n",
      "| epoch   5 | 25000/25026 batches | lr 1.00 | ms/batch 153.16 | loss  6.30 | ppl   546.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 4036.37s | valid loss  6.92 | valid ppl  1011.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/25026 batches | lr 1.00 | ms/batch 154.05 | loss  6.30 | ppl   545.67\n",
      "| epoch   6 |   400/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.25 | ppl   518.05\n",
      "| epoch   6 |   600/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.20 | ppl   490.89\n",
      "| epoch   6 |   800/25026 batches | lr 1.00 | ms/batch 153.12 | loss  6.20 | ppl   492.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |  1000/25026 batches | lr 1.00 | ms/batch 153.12 | loss  6.08 | ppl   435.38\n",
      "| epoch   6 |  1200/25026 batches | lr 1.00 | ms/batch 153.16 | loss  5.93 | ppl   376.59\n",
      "| epoch   6 |  1400/25026 batches | lr 1.00 | ms/batch 153.17 | loss  6.15 | ppl   467.54\n",
      "| epoch   6 |  1600/25026 batches | lr 1.00 | ms/batch 153.12 | loss  6.31 | ppl   552.40\n",
      "| epoch   6 |  1800/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.42 | ppl   615.73\n",
      "| epoch   6 |  2000/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.54 | ppl   689.52\n",
      "| epoch   6 |  2200/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.28 | ppl   532.71\n",
      "| epoch   6 |  2400/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.36 | ppl   576.95\n",
      "| epoch   6 |  2600/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.31 | ppl   550.54\n",
      "| epoch   6 |  2800/25026 batches | lr 1.00 | ms/batch 153.49 | loss  6.08 | ppl   435.99\n",
      "| epoch   6 |  3000/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.16 | ppl   473.77\n",
      "| epoch   6 |  3200/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.15 | ppl   470.77\n",
      "| epoch   6 |  3400/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.04 | ppl   421.47\n",
      "| epoch   6 |  3600/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.09 | ppl   443.21\n",
      "| epoch   6 |  3800/25026 batches | lr 1.00 | ms/batch 154.11 | loss  6.08 | ppl   435.26\n",
      "| epoch   6 |  4000/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.06 | ppl   428.25\n",
      "| epoch   6 |  4200/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.24 | ppl   512.02\n",
      "| epoch   6 |  4400/25026 batches | lr 1.00 | ms/batch 153.31 | loss  6.03 | ppl   413.99\n",
      "| epoch   6 |  4600/25026 batches | lr 1.00 | ms/batch 153.28 | loss  5.97 | ppl   392.67\n",
      "| epoch   6 |  4800/25026 batches | lr 1.00 | ms/batch 153.37 | loss  6.15 | ppl   470.97\n",
      "| epoch   6 |  5000/25026 batches | lr 1.00 | ms/batch 153.35 | loss  6.05 | ppl   423.06\n",
      "| epoch   6 |  5200/25026 batches | lr 1.00 | ms/batch 153.36 | loss  6.01 | ppl   407.98\n",
      "| epoch   6 |  5400/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.05 | ppl   425.58\n",
      "| epoch   6 |  5600/25026 batches | lr 1.00 | ms/batch 153.28 | loss  5.98 | ppl   396.81\n",
      "| epoch   6 |  5800/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.94 | ppl   381.05\n",
      "| epoch   6 |  6000/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.28 | ppl   535.86\n",
      "| epoch   6 |  6200/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.17 | ppl   479.33\n",
      "| epoch   6 |  6400/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.06 | ppl   426.95\n",
      "| epoch   6 |  6600/25026 batches | lr 1.00 | ms/batch 153.38 | loss  6.16 | ppl   471.10\n",
      "| epoch   6 |  6800/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.34 | ppl   568.15\n",
      "| epoch   6 |  7000/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.14 | ppl   462.88\n",
      "| epoch   6 |  7200/25026 batches | lr 1.00 | ms/batch 153.15 | loss  6.20 | ppl   493.29\n",
      "| epoch   6 |  7400/25026 batches | lr 1.00 | ms/batch 153.21 | loss  5.95 | ppl   384.96\n",
      "| epoch   6 |  7600/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.16 | ppl   474.13\n",
      "| epoch   6 |  7800/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.07 | ppl   434.39\n",
      "| epoch   6 |  8000/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.14 | ppl   465.82\n",
      "| epoch   6 |  8200/25026 batches | lr 1.00 | ms/batch 153.28 | loss  5.93 | ppl   374.49\n",
      "| epoch   6 |  8400/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.98 | ppl   396.41\n",
      "| epoch   6 |  8600/25026 batches | lr 1.00 | ms/batch 153.31 | loss  6.05 | ppl   423.02\n",
      "| epoch   6 |  8800/25026 batches | lr 1.00 | ms/batch 153.36 | loss  6.22 | ppl   503.45\n",
      "| epoch   6 |  9000/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.10 | ppl   444.28\n",
      "| epoch   6 |  9200/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.12 | ppl   456.59\n",
      "| epoch   6 |  9400/25026 batches | lr 1.00 | ms/batch 153.34 | loss  6.12 | ppl   455.67\n",
      "| epoch   6 |  9600/25026 batches | lr 1.00 | ms/batch 154.67 | loss  6.02 | ppl   411.36\n",
      "| epoch   6 |  9800/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.19 | ppl   485.69\n",
      "| epoch   6 | 10000/25026 batches | lr 1.00 | ms/batch 153.35 | loss  6.07 | ppl   431.72\n",
      "| epoch   6 | 10200/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.05 | ppl   422.36\n",
      "| epoch   6 | 10400/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.95 | ppl   384.16\n",
      "| epoch   6 | 10600/25026 batches | lr 1.00 | ms/batch 153.40 | loss  6.09 | ppl   442.50\n",
      "| epoch   6 | 10800/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.05 | ppl   422.74\n",
      "| epoch   6 | 11000/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.19 | ppl   488.48\n",
      "| epoch   6 | 11200/25026 batches | lr 1.00 | ms/batch 153.30 | loss  6.03 | ppl   413.74\n",
      "| epoch   6 | 11400/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.10 | ppl   446.07\n",
      "| epoch   6 | 11600/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.93 | ppl   377.63\n",
      "| epoch   6 | 11800/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.20 | ppl   492.50\n",
      "| epoch   6 | 12000/25026 batches | lr 1.00 | ms/batch 153.23 | loss  5.90 | ppl   364.90\n",
      "| epoch   6 | 12200/25026 batches | lr 1.00 | ms/batch 153.20 | loss  5.93 | ppl   374.28\n",
      "| epoch   6 | 12400/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.08 | ppl   438.39\n",
      "| epoch   6 | 12600/25026 batches | lr 1.00 | ms/batch 153.20 | loss  6.18 | ppl   482.01\n",
      "| epoch   6 | 12800/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.15 | ppl   469.89\n",
      "| epoch   6 | 13000/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.20 | ppl   490.42\n",
      "| epoch   6 | 13200/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.00 | ppl   405.34\n",
      "| epoch   6 | 13400/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.95 | ppl   383.44\n",
      "| epoch   6 | 13600/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.10 | ppl   445.88\n",
      "| epoch   6 | 13800/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.05 | ppl   423.89\n",
      "| epoch   6 | 14000/25026 batches | lr 1.00 | ms/batch 153.19 | loss  6.00 | ppl   404.03\n",
      "| epoch   6 | 14200/25026 batches | lr 1.00 | ms/batch 153.21 | loss  6.06 | ppl   427.78\n",
      "| epoch   6 | 14400/25026 batches | lr 1.00 | ms/batch 153.44 | loss  6.06 | ppl   429.46\n",
      "| epoch   6 | 14600/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.91 | ppl   368.58\n",
      "| epoch   6 | 14800/25026 batches | lr 1.00 | ms/batch 153.22 | loss  5.89 | ppl   362.80\n",
      "| epoch   6 | 15000/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.08 | ppl   436.68\n",
      "| epoch   6 | 15200/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.07 | ppl   431.94\n",
      "| epoch   6 | 15400/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.98 | ppl   395.33\n",
      "| epoch   6 | 15600/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.93 | ppl   377.20\n",
      "| epoch   6 | 15800/25026 batches | lr 1.00 | ms/batch 153.36 | loss  5.87 | ppl   353.20\n",
      "| epoch   6 | 16000/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.02 | ppl   412.83\n",
      "| epoch   6 | 16200/25026 batches | lr 1.00 | ms/batch 153.43 | loss  5.93 | ppl   375.69\n",
      "| epoch   6 | 16400/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.12 | ppl   455.15\n",
      "| epoch   6 | 16600/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.04 | ppl   418.34\n",
      "| epoch   6 | 16800/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.04 | ppl   421.22\n",
      "| epoch   6 | 17000/25026 batches | lr 1.00 | ms/batch 153.31 | loss  6.04 | ppl   421.30\n",
      "| epoch   6 | 17200/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.04 | ppl   418.85\n",
      "| epoch   6 | 17400/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.99 | ppl   398.11\n",
      "| epoch   6 | 17600/25026 batches | lr 1.00 | ms/batch 154.50 | loss  6.03 | ppl   413.94\n",
      "| epoch   6 | 17800/25026 batches | lr 1.00 | ms/batch 153.22 | loss  6.10 | ppl   443.96\n",
      "| epoch   6 | 18000/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.01 | ppl   409.27\n",
      "| epoch   6 | 18200/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.16 | ppl   473.33\n",
      "| epoch   6 | 18400/25026 batches | lr 1.00 | ms/batch 153.46 | loss  5.92 | ppl   372.11\n",
      "| epoch   6 | 18600/25026 batches | lr 1.00 | ms/batch 153.34 | loss  5.95 | ppl   383.18\n",
      "| epoch   6 | 18800/25026 batches | lr 1.00 | ms/batch 153.25 | loss  5.82 | ppl   335.79\n",
      "| epoch   6 | 19000/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.07 | ppl   434.66\n",
      "| epoch   6 | 19200/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.19 | ppl   488.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 | 19400/25026 batches | lr 1.00 | ms/batch 153.35 | loss  6.11 | ppl   451.67\n",
      "| epoch   6 | 19600/25026 batches | lr 1.00 | ms/batch 153.41 | loss  5.90 | ppl   363.86\n",
      "| epoch   6 | 19800/25026 batches | lr 1.00 | ms/batch 153.44 | loss  5.95 | ppl   383.53\n",
      "| epoch   6 | 20000/25026 batches | lr 1.00 | ms/batch 153.36 | loss  6.06 | ppl   426.32\n",
      "| epoch   6 | 20200/25026 batches | lr 1.00 | ms/batch 153.41 | loss  6.18 | ppl   483.44\n",
      "| epoch   6 | 20400/25026 batches | lr 1.00 | ms/batch 153.51 | loss  6.00 | ppl   403.32\n",
      "| epoch   6 | 20600/25026 batches | lr 1.00 | ms/batch 153.42 | loss  6.07 | ppl   433.88\n",
      "| epoch   6 | 20800/25026 batches | lr 1.00 | ms/batch 153.41 | loss  5.86 | ppl   350.68\n",
      "| epoch   6 | 21000/25026 batches | lr 1.00 | ms/batch 153.44 | loss  6.08 | ppl   437.91\n",
      "| epoch   6 | 21200/25026 batches | lr 1.00 | ms/batch 153.44 | loss  6.10 | ppl   447.19\n",
      "| epoch   6 | 21400/25026 batches | lr 1.00 | ms/batch 153.47 | loss  5.92 | ppl   373.63\n",
      "| epoch   6 | 21600/25026 batches | lr 1.00 | ms/batch 153.46 | loss  6.15 | ppl   470.97\n",
      "| epoch   6 | 21800/25026 batches | lr 1.00 | ms/batch 153.49 | loss  6.01 | ppl   407.58\n",
      "| epoch   6 | 22000/25026 batches | lr 1.00 | ms/batch 153.45 | loss  6.06 | ppl   426.33\n",
      "| epoch   6 | 22200/25026 batches | lr 1.00 | ms/batch 153.50 | loss  6.11 | ppl   448.87\n",
      "| epoch   6 | 22400/25026 batches | lr 1.00 | ms/batch 153.50 | loss  6.09 | ppl   441.97\n",
      "| epoch   6 | 22600/25026 batches | lr 1.00 | ms/batch 153.36 | loss  6.04 | ppl   419.33\n",
      "| epoch   6 | 22800/25026 batches | lr 1.00 | ms/batch 153.37 | loss  6.01 | ppl   405.51\n",
      "| epoch   6 | 23000/25026 batches | lr 1.00 | ms/batch 153.34 | loss  6.00 | ppl   404.82\n",
      "| epoch   6 | 23200/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.10 | ppl   444.73\n",
      "| epoch   6 | 23400/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.76 | ppl   318.11\n",
      "| epoch   6 | 23600/25026 batches | lr 1.00 | ms/batch 154.47 | loss  5.92 | ppl   374.02\n",
      "| epoch   6 | 23800/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.04 | ppl   420.81\n",
      "| epoch   6 | 24000/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.95 | ppl   383.38\n",
      "| epoch   6 | 24200/25026 batches | lr 1.00 | ms/batch 153.28 | loss  5.71 | ppl   303.31\n",
      "| epoch   6 | 24400/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.03 | ppl   414.17\n",
      "| epoch   6 | 24600/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.12 | ppl   454.55\n",
      "| epoch   6 | 24800/25026 batches | lr 1.00 | ms/batch 153.34 | loss  6.22 | ppl   502.96\n",
      "| epoch   6 | 25000/25026 batches | lr 1.00 | ms/batch 153.26 | loss  6.13 | ppl   460.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 4037.84s | valid loss  6.91 | valid ppl  1004.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/25026 batches | lr 1.00 | ms/batch 154.12 | loss  6.14 | ppl   465.10\n",
      "| epoch   7 |   400/25026 batches | lr 1.00 | ms/batch 153.38 | loss  6.08 | ppl   438.80\n",
      "| epoch   7 |   600/25026 batches | lr 1.00 | ms/batch 153.41 | loss  6.02 | ppl   411.38\n",
      "| epoch   7 |   800/25026 batches | lr 1.00 | ms/batch 153.38 | loss  6.03 | ppl   417.69\n",
      "| epoch   7 |  1000/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.90 | ppl   365.75\n",
      "| epoch   7 |  1200/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.76 | ppl   317.92\n",
      "| epoch   7 |  1400/25026 batches | lr 1.00 | ms/batch 153.34 | loss  5.98 | ppl   396.56\n",
      "| epoch   7 |  1600/25026 batches | lr 1.00 | ms/batch 153.38 | loss  6.15 | ppl   469.44\n",
      "| epoch   7 |  1800/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.25 | ppl   518.71\n",
      "| epoch   7 |  2000/25026 batches | lr 1.00 | ms/batch 153.31 | loss  6.36 | ppl   581.10\n",
      "| epoch   7 |  2200/25026 batches | lr 1.00 | ms/batch 153.30 | loss  6.11 | ppl   450.11\n",
      "| epoch   7 |  2400/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.20 | ppl   490.58\n",
      "| epoch   7 |  2600/25026 batches | lr 1.00 | ms/batch 153.40 | loss  6.14 | ppl   463.95\n",
      "| epoch   7 |  2800/25026 batches | lr 1.00 | ms/batch 153.41 | loss  5.90 | ppl   366.68\n",
      "| epoch   7 |  3000/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.00 | ppl   401.97\n",
      "| epoch   7 |  3200/25026 batches | lr 1.00 | ms/batch 153.28 | loss  5.99 | ppl   399.01\n",
      "| epoch   7 |  3400/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.88 | ppl   357.14\n",
      "| epoch   7 |  3600/25026 batches | lr 1.00 | ms/batch 153.23 | loss  5.92 | ppl   372.71\n",
      "| epoch   7 |  3800/25026 batches | lr 1.00 | ms/batch 154.20 | loss  5.90 | ppl   365.25\n",
      "| epoch   7 |  4000/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.90 | ppl   364.98\n",
      "| epoch   7 |  4200/25026 batches | lr 1.00 | ms/batch 153.23 | loss  6.08 | ppl   438.39\n",
      "| epoch   7 |  4400/25026 batches | lr 1.00 | ms/batch 153.21 | loss  5.87 | ppl   353.41\n",
      "| epoch   7 |  4600/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.82 | ppl   337.02\n",
      "| epoch   7 |  4800/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.99 | ppl   399.86\n",
      "| epoch   7 |  5000/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.89 | ppl   359.80\n",
      "| epoch   7 |  5200/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.85 | ppl   346.65\n",
      "| epoch   7 |  5400/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.90 | ppl   363.98\n",
      "| epoch   7 |  5600/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.83 | ppl   339.72\n",
      "| epoch   7 |  5800/25026 batches | lr 1.00 | ms/batch 153.38 | loss  5.78 | ppl   324.43\n",
      "| epoch   7 |  6000/25026 batches | lr 1.00 | ms/batch 153.34 | loss  6.11 | ppl   452.04\n",
      "| epoch   7 |  6200/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.00 | ppl   404.86\n",
      "| epoch   7 |  6400/25026 batches | lr 1.00 | ms/batch 153.39 | loss  5.90 | ppl   363.25\n",
      "| epoch   7 |  6600/25026 batches | lr 1.00 | ms/batch 153.37 | loss  5.99 | ppl   399.08\n",
      "| epoch   7 |  6800/25026 batches | lr 1.00 | ms/batch 153.43 | loss  6.18 | ppl   481.53\n",
      "| epoch   7 |  7000/25026 batches | lr 1.00 | ms/batch 153.41 | loss  5.96 | ppl   389.08\n",
      "| epoch   7 |  7200/25026 batches | lr 1.00 | ms/batch 153.37 | loss  6.04 | ppl   419.35\n",
      "| epoch   7 |  7400/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.79 | ppl   326.01\n",
      "| epoch   7 |  7600/25026 batches | lr 1.00 | ms/batch 153.40 | loss  6.00 | ppl   402.97\n",
      "| epoch   7 |  7800/25026 batches | lr 1.00 | ms/batch 153.45 | loss  5.92 | ppl   371.04\n",
      "| epoch   7 |  8000/25026 batches | lr 1.00 | ms/batch 153.37 | loss  5.98 | ppl   395.81\n",
      "| epoch   7 |  8200/25026 batches | lr 1.00 | ms/batch 153.50 | loss  5.77 | ppl   320.14\n",
      "| epoch   7 |  8400/25026 batches | lr 1.00 | ms/batch 154.39 | loss  5.82 | ppl   335.99\n",
      "| epoch   7 |  8600/25026 batches | lr 1.00 | ms/batch 153.45 | loss  5.88 | ppl   358.43\n",
      "| epoch   7 |  8800/25026 batches | lr 1.00 | ms/batch 153.40 | loss  6.06 | ppl   429.48\n",
      "| epoch   7 |  9000/25026 batches | lr 1.00 | ms/batch 153.37 | loss  5.92 | ppl   374.14\n",
      "| epoch   7 |  9200/25026 batches | lr 1.00 | ms/batch 153.34 | loss  5.96 | ppl   388.88\n",
      "| epoch   7 |  9400/25026 batches | lr 1.00 | ms/batch 153.37 | loss  5.98 | ppl   393.92\n",
      "| epoch   7 |  9600/25026 batches | lr 1.00 | ms/batch 153.38 | loss  5.85 | ppl   348.38\n",
      "| epoch   7 |  9800/25026 batches | lr 1.00 | ms/batch 153.41 | loss  6.02 | ppl   409.63\n",
      "| epoch   7 | 10000/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.91 | ppl   367.45\n",
      "| epoch   7 | 10200/25026 batches | lr 1.00 | ms/batch 153.39 | loss  5.88 | ppl   359.02\n",
      "| epoch   7 | 10400/25026 batches | lr 1.00 | ms/batch 153.37 | loss  5.79 | ppl   325.92\n",
      "| epoch   7 | 10600/25026 batches | lr 1.00 | ms/batch 153.34 | loss  5.94 | ppl   380.28\n",
      "| epoch   7 | 10800/25026 batches | lr 1.00 | ms/batch 153.34 | loss  5.88 | ppl   358.99\n",
      "| epoch   7 | 11000/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.03 | ppl   414.30\n",
      "| epoch   7 | 11200/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.87 | ppl   352.76\n",
      "| epoch   7 | 11400/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.94 | ppl   379.79\n",
      "| epoch   7 | 11600/25026 batches | lr 1.00 | ms/batch 153.60 | loss  5.76 | ppl   318.05\n",
      "| epoch   7 | 11800/25026 batches | lr 1.00 | ms/batch 153.28 | loss  6.03 | ppl   415.71\n",
      "| epoch   7 | 12000/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.74 | ppl   311.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 | 12200/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.77 | ppl   322.09\n",
      "| epoch   7 | 12400/25026 batches | lr 1.00 | ms/batch 153.25 | loss  5.92 | ppl   372.02\n",
      "| epoch   7 | 12600/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.02 | ppl   411.66\n",
      "| epoch   7 | 12800/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.99 | ppl   400.95\n",
      "| epoch   7 | 13000/25026 batches | lr 1.00 | ms/batch 153.27 | loss  6.04 | ppl   418.00\n",
      "| epoch   7 | 13200/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.85 | ppl   345.53\n",
      "| epoch   7 | 13400/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.79 | ppl   328.23\n",
      "| epoch   7 | 13600/25026 batches | lr 1.00 | ms/batch 153.23 | loss  5.94 | ppl   380.79\n",
      "| epoch   7 | 13800/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.87 | ppl   354.09\n",
      "| epoch   7 | 14000/25026 batches | lr 1.00 | ms/batch 153.20 | loss  5.84 | ppl   344.85\n",
      "| epoch   7 | 14200/25026 batches | lr 1.00 | ms/batch 153.18 | loss  5.90 | ppl   365.94\n",
      "| epoch   7 | 14400/25026 batches | lr 1.00 | ms/batch 153.20 | loss  5.91 | ppl   368.88\n",
      "| epoch   7 | 14600/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.75 | ppl   315.37\n",
      "| epoch   7 | 14800/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.74 | ppl   312.03\n",
      "| epoch   7 | 15000/25026 batches | lr 1.00 | ms/batch 153.28 | loss  5.93 | ppl   374.30\n",
      "| epoch   7 | 15200/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.90 | ppl   366.19\n",
      "| epoch   7 | 15400/25026 batches | lr 1.00 | ms/batch 153.25 | loss  5.82 | ppl   337.09\n",
      "| epoch   7 | 15600/25026 batches | lr 1.00 | ms/batch 153.34 | loss  5.77 | ppl   321.83\n",
      "| epoch   7 | 15800/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.70 | ppl   300.17\n",
      "| epoch   7 | 16000/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.87 | ppl   354.31\n",
      "| epoch   7 | 16200/25026 batches | lr 1.00 | ms/batch 154.72 | loss  5.77 | ppl   319.64\n",
      "| epoch   7 | 16400/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.96 | ppl   386.82\n",
      "| epoch   7 | 16600/25026 batches | lr 1.00 | ms/batch 154.16 | loss  5.88 | ppl   358.52\n",
      "| epoch   7 | 16800/25026 batches | lr 1.00 | ms/batch 153.34 | loss  5.89 | ppl   360.33\n",
      "| epoch   7 | 17000/25026 batches | lr 1.00 | ms/batch 153.28 | loss  5.88 | ppl   358.37\n",
      "| epoch   7 | 17200/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.88 | ppl   358.82\n",
      "| epoch   7 | 17400/25026 batches | lr 1.00 | ms/batch 153.38 | loss  5.83 | ppl   339.81\n",
      "| epoch   7 | 17600/25026 batches | lr 1.00 | ms/batch 153.42 | loss  5.88 | ppl   357.30\n",
      "| epoch   7 | 17800/25026 batches | lr 1.00 | ms/batch 153.37 | loss  5.93 | ppl   376.64\n",
      "| epoch   7 | 18000/25026 batches | lr 1.00 | ms/batch 154.09 | loss  5.86 | ppl   349.83\n",
      "| epoch   7 | 18200/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.00 | ppl   402.93\n",
      "| epoch   7 | 18400/25026 batches | lr 1.00 | ms/batch 153.33 | loss  5.76 | ppl   318.58\n",
      "| epoch   7 | 18600/25026 batches | lr 1.00 | ms/batch 153.43 | loss  5.78 | ppl   322.75\n",
      "| epoch   7 | 18800/25026 batches | lr 1.00 | ms/batch 153.37 | loss  5.65 | ppl   285.10\n",
      "| epoch   7 | 19000/25026 batches | lr 1.00 | ms/batch 153.28 | loss  5.91 | ppl   369.42\n",
      "| epoch   7 | 19200/25026 batches | lr 1.00 | ms/batch 153.46 | loss  6.02 | ppl   412.62\n",
      "| epoch   7 | 19400/25026 batches | lr 1.00 | ms/batch 153.81 | loss  5.95 | ppl   383.43\n",
      "| epoch   7 | 19600/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.74 | ppl   311.42\n",
      "| epoch   7 | 19800/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.80 | ppl   329.08\n",
      "| epoch   7 | 20000/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.90 | ppl   365.56\n",
      "| epoch   7 | 20200/25026 batches | lr 1.00 | ms/batch 153.30 | loss  6.02 | ppl   410.98\n",
      "| epoch   7 | 20400/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.84 | ppl   344.97\n",
      "| epoch   7 | 20600/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.92 | ppl   371.75\n",
      "| epoch   7 | 20800/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.71 | ppl   302.39\n",
      "| epoch   7 | 21000/25026 batches | lr 1.00 | ms/batch 154.41 | loss  5.94 | ppl   378.14\n",
      "| epoch   7 | 21200/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.96 | ppl   387.67\n",
      "| epoch   7 | 21400/25026 batches | lr 1.00 | ms/batch 153.25 | loss  5.77 | ppl   322.08\n",
      "| epoch   7 | 21600/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.99 | ppl   401.28\n",
      "| epoch   7 | 21800/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.85 | ppl   347.06\n",
      "| epoch   7 | 22000/25026 batches | lr 1.00 | ms/batch 153.38 | loss  5.89 | ppl   362.17\n",
      "| epoch   7 | 22200/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.96 | ppl   385.79\n",
      "| epoch   7 | 22400/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.94 | ppl   378.40\n",
      "| epoch   7 | 22600/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.87 | ppl   355.62\n",
      "| epoch   7 | 22800/25026 batches | lr 1.00 | ms/batch 153.36 | loss  5.85 | ppl   345.75\n",
      "| epoch   7 | 23000/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.86 | ppl   349.30\n",
      "| epoch   7 | 23200/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.94 | ppl   378.99\n",
      "| epoch   7 | 23400/25026 batches | lr 1.00 | ms/batch 153.55 | loss  5.61 | ppl   272.47\n",
      "| epoch   7 | 23600/25026 batches | lr 1.00 | ms/batch 153.22 | loss  5.77 | ppl   319.73\n",
      "| epoch   7 | 23800/25026 batches | lr 1.00 | ms/batch 153.23 | loss  5.89 | ppl   361.45\n",
      "| epoch   7 | 24000/25026 batches | lr 1.00 | ms/batch 153.25 | loss  5.80 | ppl   330.72\n",
      "| epoch   7 | 24200/25026 batches | lr 1.00 | ms/batch 153.28 | loss  5.56 | ppl   260.38\n",
      "| epoch   7 | 24400/25026 batches | lr 1.00 | ms/batch 154.66 | loss  5.88 | ppl   357.00\n",
      "| epoch   7 | 24600/25026 batches | lr 1.00 | ms/batch 153.22 | loss  5.97 | ppl   390.82\n",
      "| epoch   7 | 24800/25026 batches | lr 1.00 | ms/batch 153.24 | loss  6.07 | ppl   431.85\n",
      "| epoch   7 | 25000/25026 batches | lr 1.00 | ms/batch 153.19 | loss  5.97 | ppl   391.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 4039.42s | valid loss  6.93 | valid ppl  1023.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/25026 batches | lr 1.00 | ms/batch 154.15 | loss  5.98 | ppl   396.68\n",
      "| epoch   8 |   400/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.93 | ppl   376.24\n",
      "| epoch   8 |   600/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.86 | ppl   349.48\n",
      "| epoch   8 |   800/25026 batches | lr 1.00 | ms/batch 153.23 | loss  5.87 | ppl   354.70\n",
      "| epoch   8 |  1000/25026 batches | lr 1.00 | ms/batch 153.49 | loss  5.74 | ppl   310.83\n",
      "| epoch   8 |  1200/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.61 | ppl   271.88\n",
      "| epoch   8 |  1400/25026 batches | lr 1.00 | ms/batch 153.34 | loss  5.83 | ppl   339.71\n",
      "| epoch   8 |  1600/25026 batches | lr 1.00 | ms/batch 153.34 | loss  5.99 | ppl   400.82\n",
      "| epoch   8 |  1800/25026 batches | lr 1.00 | ms/batch 153.33 | loss  6.09 | ppl   440.30\n",
      "| epoch   8 |  2000/25026 batches | lr 1.00 | ms/batch 153.32 | loss  6.20 | ppl   493.56\n",
      "| epoch   8 |  2200/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.95 | ppl   384.81\n",
      "| epoch   8 |  2400/25026 batches | lr 1.00 | ms/batch 153.25 | loss  6.04 | ppl   418.65\n",
      "| epoch   8 |  2600/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.97 | ppl   392.93\n",
      "| epoch   8 |  2800/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.75 | ppl   314.11\n",
      "| epoch   8 |  3000/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.84 | ppl   345.35\n",
      "| epoch   8 |  3200/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.83 | ppl   341.97\n",
      "| epoch   8 |  3400/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.72 | ppl   306.10\n",
      "| epoch   8 |  3600/25026 batches | lr 1.00 | ms/batch 153.36 | loss  5.75 | ppl   314.58\n",
      "| epoch   8 |  3800/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.74 | ppl   310.90\n",
      "| epoch   8 |  4000/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.75 | ppl   314.10\n",
      "| epoch   8 |  4200/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.93 | ppl   376.75\n",
      "| epoch   8 |  4400/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.72 | ppl   304.62\n",
      "| epoch   8 |  4600/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.67 | ppl   290.70\n",
      "| epoch   8 |  4800/25026 batches | lr 1.00 | ms/batch 153.45 | loss  5.84 | ppl   343.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |  5000/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.74 | ppl   309.91\n",
      "| epoch   8 |  5200/25026 batches | lr 1.00 | ms/batch 153.36 | loss  5.70 | ppl   298.07\n",
      "| epoch   8 |  5400/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.75 | ppl   313.45\n",
      "| epoch   8 |  5600/25026 batches | lr 1.00 | ms/batch 153.33 | loss  5.68 | ppl   292.05\n",
      "| epoch   8 |  5800/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.63 | ppl   277.90\n",
      "| epoch   8 |  6000/25026 batches | lr 1.00 | ms/batch 153.33 | loss  5.96 | ppl   386.45\n",
      "| epoch   8 |  6200/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.85 | ppl   346.98\n",
      "| epoch   8 |  6400/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.75 | ppl   313.43\n",
      "| epoch   8 |  6600/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.83 | ppl   341.48\n",
      "| epoch   8 |  6800/25026 batches | lr 1.00 | ms/batch 153.29 | loss  6.02 | ppl   410.39\n",
      "| epoch   8 |  7000/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.80 | ppl   331.48\n",
      "| epoch   8 |  7200/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.89 | ppl   359.64\n",
      "| epoch   8 |  7400/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.63 | ppl   278.98\n",
      "| epoch   8 |  7600/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.84 | ppl   344.26\n",
      "| epoch   8 |  7800/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.76 | ppl   317.77\n",
      "| epoch   8 |  8000/25026 batches | lr 1.00 | ms/batch 153.23 | loss  5.82 | ppl   338.02\n",
      "| epoch   8 |  8200/25026 batches | lr 1.00 | ms/batch 153.23 | loss  5.62 | ppl   276.92\n",
      "| epoch   8 |  8400/25026 batches | lr 1.00 | ms/batch 153.19 | loss  5.67 | ppl   288.98\n",
      "| epoch   8 |  8600/25026 batches | lr 1.00 | ms/batch 153.22 | loss  5.74 | ppl   310.09\n",
      "| epoch   8 |  8800/25026 batches | lr 1.00 | ms/batch 153.42 | loss  5.91 | ppl   370.54\n",
      "| epoch   8 |  9000/25026 batches | lr 1.00 | ms/batch 153.28 | loss  5.76 | ppl   318.32\n",
      "| epoch   8 |  9200/25026 batches | lr 1.00 | ms/batch 153.23 | loss  5.81 | ppl   333.42\n",
      "| epoch   8 |  9400/25026 batches | lr 1.00 | ms/batch 153.17 | loss  5.83 | ppl   341.05\n",
      "| epoch   8 |  9600/25026 batches | lr 1.00 | ms/batch 153.25 | loss  5.70 | ppl   298.83\n",
      "| epoch   8 |  9800/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.87 | ppl   352.87\n",
      "| epoch   8 | 10000/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.75 | ppl   312.88\n",
      "| epoch   8 | 10200/25026 batches | lr 1.00 | ms/batch 153.18 | loss  5.73 | ppl   308.61\n",
      "| epoch   8 | 10400/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.64 | ppl   281.94\n",
      "| epoch   8 | 10600/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.80 | ppl   328.68\n",
      "| epoch   8 | 10800/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.73 | ppl   306.65\n",
      "| epoch   8 | 11000/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.87 | ppl   355.12\n",
      "| epoch   8 | 11200/25026 batches | lr 1.00 | ms/batch 153.22 | loss  5.72 | ppl   303.47\n",
      "| epoch   8 | 11400/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.79 | ppl   325.48\n",
      "| epoch   8 | 11600/25026 batches | lr 1.00 | ms/batch 153.23 | loss  5.61 | ppl   272.17\n",
      "| epoch   8 | 11800/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.88 | ppl   356.65\n",
      "| epoch   8 | 12000/25026 batches | lr 1.00 | ms/batch 153.25 | loss  5.59 | ppl   268.60\n",
      "| epoch   8 | 12200/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.63 | ppl   278.72\n",
      "| epoch   8 | 12400/25026 batches | lr 1.00 | ms/batch 153.33 | loss  5.76 | ppl   318.91\n",
      "| epoch   8 | 12600/25026 batches | lr 1.00 | ms/batch 153.43 | loss  5.87 | ppl   355.58\n",
      "| epoch   8 | 12800/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.84 | ppl   344.41\n",
      "| epoch   8 | 13000/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.88 | ppl   358.60\n",
      "| epoch   8 | 13200/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.69 | ppl   294.89\n",
      "| epoch   8 | 13400/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.64 | ppl   281.03\n",
      "| epoch   8 | 13600/25026 batches | lr 1.00 | ms/batch 153.33 | loss  5.79 | ppl   327.44\n",
      "| epoch   8 | 13800/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.71 | ppl   301.57\n",
      "| epoch   8 | 14000/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.69 | ppl   294.69\n",
      "| epoch   8 | 14200/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.76 | ppl   316.49\n",
      "| epoch   8 | 14400/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.76 | ppl   318.42\n",
      "| epoch   8 | 14600/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.60 | ppl   270.11\n",
      "| epoch   8 | 14800/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.60 | ppl   270.45\n",
      "| epoch   8 | 15000/25026 batches | lr 1.00 | ms/batch 153.25 | loss  5.78 | ppl   324.74\n",
      "| epoch   8 | 15200/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.76 | ppl   316.17\n",
      "| epoch   8 | 15400/25026 batches | lr 1.00 | ms/batch 153.24 | loss  5.66 | ppl   288.47\n",
      "| epoch   8 | 15600/25026 batches | lr 1.00 | ms/batch 153.36 | loss  5.62 | ppl   274.82\n",
      "| epoch   8 | 15800/25026 batches | lr 1.00 | ms/batch 153.28 | loss  5.55 | ppl   256.41\n",
      "| epoch   8 | 16000/25026 batches | lr 1.00 | ms/batch 153.33 | loss  5.72 | ppl   305.57\n",
      "| epoch   8 | 16200/25026 batches | lr 1.00 | ms/batch 153.34 | loss  5.62 | ppl   275.30\n",
      "| epoch   8 | 16400/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.80 | ppl   330.22\n",
      "| epoch   8 | 16600/25026 batches | lr 1.00 | ms/batch 154.48 | loss  5.73 | ppl   307.97\n",
      "| epoch   8 | 16800/25026 batches | lr 1.00 | ms/batch 153.33 | loss  5.73 | ppl   309.33\n",
      "| epoch   8 | 17000/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.73 | ppl   308.93\n",
      "| epoch   8 | 17200/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.74 | ppl   309.89\n",
      "| epoch   8 | 17400/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.68 | ppl   293.14\n",
      "| epoch   8 | 17600/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.74 | ppl   310.13\n",
      "| epoch   8 | 17800/25026 batches | lr 1.00 | ms/batch 153.30 | loss  5.78 | ppl   322.83\n",
      "| epoch   8 | 18000/25026 batches | lr 1.00 | ms/batch 153.32 | loss  5.70 | ppl   297.70\n",
      "| epoch   8 | 18200/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.85 | ppl   347.89\n",
      "| epoch   8 | 18400/25026 batches | lr 1.00 | ms/batch 153.26 | loss  5.62 | ppl   275.02\n",
      "| epoch   8 | 18600/25026 batches | lr 1.00 | ms/batch 153.22 | loss  5.62 | ppl   277.23\n",
      "| epoch   8 | 18800/25026 batches | lr 1.00 | ms/batch 153.23 | loss  5.50 | ppl   245.84\n",
      "| epoch   8 | 19000/25026 batches | lr 1.00 | ms/batch 153.25 | loss  5.76 | ppl   317.00\n",
      "| epoch   8 | 19200/25026 batches | lr 1.00 | ms/batch 153.27 | loss  5.87 | ppl   353.66\n",
      "| epoch   8 | 19400/25026 batches | lr 1.00 | ms/batch 153.25 | loss  5.79 | ppl   326.17\n",
      "| epoch   8 | 19600/25026 batches | lr 1.00 | ms/batch 153.36 | loss  5.60 | ppl   270.21\n",
      "| epoch   8 | 19800/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.65 | ppl   284.48\n",
      "| epoch   8 | 20000/25026 batches | lr 1.00 | ms/batch 153.31 | loss  5.75 | ppl   314.49\n",
      "| epoch   8 | 20200/25026 batches | lr 1.00 | ms/batch 153.29 | loss  5.86 | ppl   352.28\n",
      "| epoch   8 | 20400/25026 batches | lr 1.00 | ms/batch 153.37 | loss  5.70 | ppl   297.97\n",
      "| epoch   8 | 20600/25026 batches | lr 1.00 | ms/batch 153.48 | loss  5.76 | ppl   318.92\n",
      "| epoch   8 | 20800/25026 batches | lr 1.00 | ms/batch 153.38 | loss  5.57 | ppl   262.99\n",
      "| epoch   8 | 21000/25026 batches | lr 1.00 | ms/batch 153.39 | loss  5.80 | ppl   329.02\n",
      "| epoch   8 | 21200/25026 batches | lr 1.00 | ms/batch 153.38 | loss  5.82 | ppl   337.26\n",
      "| epoch   8 | 21400/25026 batches | lr 1.00 | ms/batch 153.47 | loss  5.64 | ppl   280.43\n",
      "| epoch   8 | 21600/25026 batches | lr 1.00 | ms/batch 153.40 | loss  5.85 | ppl   346.30\n",
      "| epoch   8 | 21800/25026 batches | lr 1.00 | ms/batch 153.38 | loss  5.70 | ppl   299.85\n",
      "| epoch   8 | 22000/25026 batches | lr 1.00 | ms/batch 153.35 | loss  5.74 | ppl   309.85\n",
      "| epoch   8 | 22200/25026 batches | lr 1.00 | ms/batch 153.45 | loss  5.81 | ppl   332.02\n",
      "| epoch   8 | 22400/25026 batches | lr 1.00 | ms/batch 153.55 | loss  5.79 | ppl   326.77\n",
      "| epoch   8 | 22600/25026 batches | lr 1.00 | ms/batch 153.48 | loss  5.72 | ppl   304.84\n",
      "| epoch   8 | 22800/25026 batches | lr 1.00 | ms/batch 153.42 | loss  5.70 | ppl   299.41\n",
      "| epoch   8 | 23000/25026 batches | lr 1.00 | ms/batch 153.39 | loss  5.72 | ppl   304.84\n",
      "| epoch   8 | 23200/25026 batches | lr 1.00 | ms/batch 153.44 | loss  5.79 | ppl   327.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 | 23400/25026 batches | lr 1.00 | ms/batch 153.56 | loss  5.46 | ppl   235.22\n",
      "| epoch   8 | 23600/25026 batches | lr 1.00 | ms/batch 153.44 | loss  5.62 | ppl   276.74\n",
      "| epoch   8 | 23800/25026 batches | lr 1.00 | ms/batch 153.43 | loss  5.74 | ppl   312.39\n",
      "| epoch   8 | 24000/25026 batches | lr 1.00 | ms/batch 153.44 | loss  5.66 | ppl   287.05\n",
      "| epoch   8 | 24200/25026 batches | lr 1.00 | ms/batch 153.46 | loss  5.42 | ppl   226.68\n",
      "| epoch   8 | 24400/25026 batches | lr 1.00 | ms/batch 153.67 | loss  5.73 | ppl   307.74\n",
      "| epoch   8 | 24600/25026 batches | lr 1.00 | ms/batch 153.46 | loss  5.82 | ppl   337.36\n",
      "| epoch   8 | 24800/25026 batches | lr 1.00 | ms/batch 153.49 | loss  5.93 | ppl   375.01\n",
      "| epoch   8 | 25000/25026 batches | lr 1.00 | ms/batch 153.43 | loss  5.82 | ppl   336.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 4038.02s | valid loss  6.98 | valid ppl  1072.59\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs. Save the model if the validation loss is the best\n",
    "# we've seen so far. Adjust the learning rate after each epoch.\n",
    "import copy\n",
    "best_val_loss = float('inf')\n",
    "epochs = 8\n",
    "best_model = None\n",
    "#print(len(vocab.get_stoi()))\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    eval_loss = evaluate(model, test_data)\n",
    "    eval_ppl = math.exp(eval_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "         f'valid loss {eval_loss:5.2f} | valid ppl {eval_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "    #best_model = copy.deepcopy(model)\n",
    "    if eval_loss < best_val_loss:\n",
    "       best_val_loss = eval_loss\n",
    "       best_model = copy.deepcopy(model)\n",
    "\n",
    "#     scheduler.step()\n",
    "#save model\n",
    "torch.save(best_model.state_dict(),'best_model_3bigx3.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa4c44",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c2c2453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generator(model: nn.Module, gen_data: Tensor, no_words = 10):\n",
    "    model.eval()\n",
    "    temp_text = text\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    for i in range(no_words):\n",
    "        print('i:', i)\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        else:\n",
    "            src_mask_ = src_mask[:,:]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "        indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "        #print(indices[0],indices[1])\n",
    "        #for j in range(batch_size):\n",
    "        print('next word: ', [vocab.lookup_tokens(list(index))\n",
    "                                  for index in indices][0][-1])\n",
    "        print(i,\"Gen_data: \",gen_data,\"Pred_data: \",indices)\n",
    "        pred_text.append([vocab.lookup_tokens(list(index))\n",
    "                                  for index in indices][0][-1])\n",
    "        if(batch_size < 16):\n",
    "            gen_data = torch.cat((gen_data[:,:],indices.t()[-1:][:]),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:,:],indices.t()[-1:][:]),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "            \n",
    "    return pred_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cebb7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load saved model\n",
    "\n",
    "# model.load_state_dict(torch.load('best_model_3bigx3.pt'))\n",
    "# model.to(device)\n",
    "# best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8bf265b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357]], device='cuda:0')\n",
      "tensor([[ 2086,   357],\n",
      "        [ 5694,   465],\n",
      "        [  568,   410],\n",
      "        [    0,  6548],\n",
      "        [  897,   293],\n",
      "        [28361,     0],\n",
      "        [    0,   357]], device='cuda:0')\n",
      "i: 0\n",
      "next word:  राज्य\n",
      "0 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357]], device='cuda:0') Pred_data:  tensor([[   53,   665,  5676, 13856,  1978,     3,   465]], device='cuda:0')\n",
      "i: 1\n",
      "next word:  अमेरिका\n",
      "1 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465]], device='cuda:0') Pred_data:  tensor([[   53,   665,  5676, 13856,  1978,     3,   465,   665]],\n",
      "       device='cuda:0')\n",
      "i: 2\n",
      "next word:  र\n",
      "2 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665]], device='cuda:0') Pred_data:  tensor([[   53,   665,  5676, 13856,  1978,     3,   465,   665,     3]],\n",
      "       device='cuda:0')\n",
      "i: 3\n",
      "next word:  उत्तर\n",
      "3 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   3]], device='cuda:0') Pred_data:  tensor([[   53,   665,  5676, 13856,  1978,     3,   465,   665,     3,   634]],\n",
      "       device='cuda:0')\n",
      "i: 4\n",
      "next word:  कोरियाली\n",
      "4 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   3],\n",
      "        [ 634]], device='cuda:0') Pred_data:  tensor([[   53,   665,  5676, 13856,  1978,     3,   465,   665,     3,   634,\n",
      "          2500]], device='cuda:0')\n",
      "i: 5\n",
      "next word:  नेता\n",
      "5 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   3],\n",
      "        [ 634],\n",
      "        [2500]], device='cuda:0') Pred_data:  tensor([[   53,   665,  5676, 13856,  1978,     3,   465,   665,     3,   634,\n",
      "          2500,   196]], device='cuda:0')\n",
      "i: 6\n",
      "next word:  किम\n",
      "6 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   3],\n",
      "        [ 634],\n",
      "        [2500],\n",
      "        [ 196]], device='cuda:0') Pred_data:  tensor([[   53,   665,  5676, 13856,  1978,     3,   465,   665,     3,   634,\n",
      "          2500,   196,  4906]], device='cuda:0')\n",
      "i: 7\n",
      "next word:  जोङ\n",
      "7 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   3],\n",
      "        [ 634],\n",
      "        [2500],\n",
      "        [ 196],\n",
      "        [4906]], device='cuda:0') Pred_data:  tensor([[   53,   665,  5676, 13856,  1978,     3,   465,   665,     3,   634,\n",
      "          2500,   196,  4906,  9974]], device='cuda:0')\n",
      "i: 8\n",
      "next word:  उन\n",
      "8 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   3],\n",
      "        [ 634],\n",
      "        [2500],\n",
      "        [ 196],\n",
      "        [4906],\n",
      "        [9974]], device='cuda:0') Pred_data:  tensor([[   53,   665,  5676, 13856,  1978,     3,   465,   665,     3,   634,\n",
      "          2500,   196,  4906,  9974, 13913]], device='cuda:0')\n",
      "i: 9\n",
      "next word:  र\n",
      "9 Gen_data:  tensor([[  357],\n",
      "        [  465],\n",
      "        [  410],\n",
      "        [ 6548],\n",
      "        [  293],\n",
      "        [    0],\n",
      "        [  357],\n",
      "        [  465],\n",
      "        [  665],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913]], device='cuda:0') Pred_data:  tensor([[   53,   665,  5676, 13856,  1978,     3,   465,   665,     3,   634,\n",
      "          2500,   196,  4906,  9974, 13913,     3]], device='cuda:0')\n",
      "i: 10\n",
      "next word:  उत्तर\n",
      "10 Gen_data:  tensor([[  465],\n",
      "        [  410],\n",
      "        [ 6548],\n",
      "        [  293],\n",
      "        [    0],\n",
      "        [  357],\n",
      "        [  465],\n",
      "        [  665],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3]], device='cuda:0') Pred_data:  tensor([[    2,  1083, 13856,  1978,     3,   465,   665,     2,  3154,  2500,\n",
      "           196,  4906,  9974, 13913,     3,   634]], device='cuda:0')\n",
      "i: 11\n",
      "next word:  कोरियाली\n",
      "11 Gen_data:  tensor([[  410],\n",
      "        [ 6548],\n",
      "        [  293],\n",
      "        [    0],\n",
      "        [  357],\n",
      "        [  465],\n",
      "        [  665],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634]], device='cuda:0') Pred_data:  tensor([[ 1083, 13856,  1978,     3,   465,   665,     3,  3154,  2500,   196,\n",
      "          4906,  9974, 13913,     3,   634,  2500]], device='cuda:0')\n",
      "i: 12\n",
      "next word:  नेता\n",
      "12 Gen_data:  tensor([[ 6548],\n",
      "        [  293],\n",
      "        [    0],\n",
      "        [  357],\n",
      "        [  465],\n",
      "        [  665],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500]], device='cuda:0') Pred_data:  tensor([[13856,  5177,     1,   465,   665,     2,   100,   109,   196,  4906,\n",
      "          9974, 13913,     3,   634,  2500,   196]], device='cuda:0')\n",
      "i: 13\n",
      "next word:  किम\n",
      "13 Gen_data:  tensor([[  293],\n",
      "        [    0],\n",
      "        [  357],\n",
      "        [  465],\n",
      "        [  665],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196]], device='cuda:0') Pred_data:  tensor([[ 4481,     1,   465,   665,     2,  1277,   109,   196,  4906,  9974,\n",
      "         13913,     3,   634,  2500,   196,  4906]], device='cuda:0')\n",
      "i: 14\n",
      "next word:  जोङ\n",
      "14 Gen_data:  tensor([[    0],\n",
      "        [  357],\n",
      "        [  465],\n",
      "        [  665],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906]], device='cuda:0') Pred_data:  tensor([[    1,   465,   665,     2,  1277,  2500,   196,  4906,  9974, 32247,\n",
      "             3,   634,  2500,   196,  4906,  9974]], device='cuda:0')\n",
      "i: 15\n",
      "next word:  उन\n",
      "15 Gen_data:  tensor([[  357],\n",
      "        [  465],\n",
      "        [  665],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974]], device='cuda:0') Pred_data:  tensor([[   53,   665,     2,  1277,  2500,   196,  4906,  9974, 32247,     3,\n",
      "           634,  2500,   196,  4906,  9974, 13913]], device='cuda:0')\n",
      "i: 16\n",
      "next word:  र\n",
      "16 Gen_data:  tensor([[  465],\n",
      "        [  665],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913]], device='cuda:0') Pred_data:  tensor([[    2,     3,   100,  2500,   196,  4906,  9974, 32247,    34,   634,\n",
      "          2500,   196,  4906,  9974, 13913,     3]], device='cuda:0')\n",
      "i: 17\n",
      "next word:  उत्तर\n",
      "17 Gen_data:  tensor([[  665],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3]], device='cuda:0') Pred_data:  tensor([[    3,   100,  2500,   196,  4906,  9974, 32247,     3,   634,  2500,\n",
      "           196,  4906,  9974, 13913,     3,   634]], device='cuda:0')\n",
      "i: 18\n",
      "next word:  कोरियाली\n",
      "18 Gen_data:  tensor([[    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634]], device='cuda:0') Pred_data:  tensor([[   15,  1042,   196,  4906,  9974, 13913,     3,   634,  2500,   196,\n",
      "          4906,  9974, 13913,     3,   634,  2500]], device='cuda:0')\n",
      "i: 19\n",
      "next word:  नेता\n",
      "19 Gen_data:  tensor([[  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500]], device='cuda:0') Pred_data:  tensor([[  485,   196,  4906,  9974, 13913,     3,   634,  2500,   196,  4906,\n",
      "          9974, 13913,     3,   634,  2500,   196]], device='cuda:0')\n",
      "i: 20\n",
      "next word:  किम\n",
      "20 Gen_data:  tensor([[ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196]], device='cuda:0') Pred_data:  tensor([[  196,  4906,  9974, 13913,     3,   634,  2500,   196,  4906,  9974,\n",
      "         13913,     3,   634,  2500,   196,  4906]], device='cuda:0')\n",
      "i: 21\n",
      "next word:  जोङ\n",
      "21 Gen_data:  tensor([[  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906]], device='cuda:0') Pred_data:  tensor([[    3,  9974, 13913,     3,  3968,  2500,   196,  4906,  9974, 13913,\n",
      "             3,   634,  2500,   196,  4906,  9974]], device='cuda:0')\n",
      "i: 22\n",
      "next word:  उन\n",
      "22 Gen_data:  tensor([[ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974]], device='cuda:0') Pred_data:  tensor([[ 9974, 13913,     3,  3968,  2500,   196,  4906,  9974, 32247,     3,\n",
      "           634,  2500,   196,  4906,  9974, 13913]], device='cuda:0')\n",
      "i: 23\n",
      "next word:  र\n",
      "23 Gen_data:  tensor([[ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913]], device='cuda:0') Pred_data:  tensor([[   29,     3,  1223,  2500,   196,  4906,  9974, 13913,     3,   634,\n",
      "          2500,   196,  4906,  9974, 13913,     3]], device='cuda:0')\n",
      "i: 24\n",
      "next word:  उत्तर\n",
      "24 Gen_data:  tensor([[13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3]], device='cuda:0') Pred_data:  tensor([[    3,  1223,  2500,   196,  4906,  9974, 32247,     3,   634,  2500,\n",
      "           196,  4906,  9974, 13913,     3,   634]], device='cuda:0')\n",
      "i: 25\n",
      "next word:  कोरियाली\n",
      "25 Gen_data:  tensor([[    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634]], device='cuda:0') Pred_data:  tensor([[   15,  1042,   196,  4906,  9974, 13913,     3,   634,  2500,   196,\n",
      "          4906,  9974, 13913,     3,   634,  2500]], device='cuda:0')\n",
      "i: 26\n",
      "next word:  नेता\n",
      "26 Gen_data:  tensor([[  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500]], device='cuda:0') Pred_data:  tensor([[  485,   196,  4906,  9974, 13913,     3,   634,  2500,   196,  4906,\n",
      "          9974, 13913,     3,   634,  2500,   196]], device='cuda:0')\n",
      "i: 27\n",
      "next word:  किम\n",
      "27 Gen_data:  tensor([[ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196]], device='cuda:0') Pred_data:  tensor([[  196,  4906,  9974, 13913,     3,   634,  2500,   196,  4906,  9974,\n",
      "         13913,     3,   634,  2500,   196,  4906]], device='cuda:0')\n",
      "i: 28\n",
      "next word:  जोङ\n",
      "28 Gen_data:  tensor([[  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906]], device='cuda:0') Pred_data:  tensor([[    3,  9974, 13913,     3,  3968,  2500,   196,  4906,  9974, 13913,\n",
      "             3,   634,  2500,   196,  4906,  9974]], device='cuda:0')\n",
      "i: 29\n",
      "next word:  उन\n",
      "29 Gen_data:  tensor([[ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974]], device='cuda:0') Pred_data:  tensor([[ 9974, 13913,     3,  3968,  2500,   196,  4906,  9974, 32247,     3,\n",
      "           634,  2500,   196,  4906,  9974, 13913]], device='cuda:0')\n",
      "i: 30\n",
      "next word:  र\n",
      "30 Gen_data:  tensor([[ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913]], device='cuda:0') Pred_data:  tensor([[   29,     3,  1223,  2500,   196,  4906,  9974, 13913,     3,   634,\n",
      "          2500,   196,  4906,  9974, 13913,     3]], device='cuda:0')\n",
      "i: 31\n",
      "next word:  उत्तर\n",
      "31 Gen_data:  tensor([[13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3]], device='cuda:0') Pred_data:  tensor([[    3,  1223,  2500,   196,  4906,  9974, 32247,     3,   634,  2500,\n",
      "           196,  4906,  9974, 13913,     3,   634]], device='cuda:0')\n",
      "i: 32\n",
      "next word:  कोरियाली\n",
      "32 Gen_data:  tensor([[    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634]], device='cuda:0') Pred_data:  tensor([[   15,  1042,   196,  4906,  9974, 13913,     3,   634,  2500,   196,\n",
      "          4906,  9974, 13913,     3,   634,  2500]], device='cuda:0')\n",
      "i: 33\n",
      "next word:  नेता\n",
      "33 Gen_data:  tensor([[  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500]], device='cuda:0') Pred_data:  tensor([[  485,   196,  4906,  9974, 13913,     3,   634,  2500,   196,  4906,\n",
      "          9974, 13913,     3,   634,  2500,   196]], device='cuda:0')\n",
      "i: 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word:  किम\n",
      "34 Gen_data:  tensor([[ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196]], device='cuda:0') Pred_data:  tensor([[  196,  4906,  9974, 13913,     3,   634,  2500,   196,  4906,  9974,\n",
      "         13913,     3,   634,  2500,   196,  4906]], device='cuda:0')\n",
      "i: 35\n",
      "next word:  जोङ\n",
      "35 Gen_data:  tensor([[  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906]], device='cuda:0') Pred_data:  tensor([[    3,  9974, 13913,     3,  3968,  2500,   196,  4906,  9974, 13913,\n",
      "             3,   634,  2500,   196,  4906,  9974]], device='cuda:0')\n",
      "i: 36\n",
      "next word:  उन\n",
      "36 Gen_data:  tensor([[ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974]], device='cuda:0') Pred_data:  tensor([[ 9974, 13913,     3,  3968,  2500,   196,  4906,  9974, 32247,     3,\n",
      "           634,  2500,   196,  4906,  9974, 13913]], device='cuda:0')\n",
      "i: 37\n",
      "next word:  र\n",
      "37 Gen_data:  tensor([[ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913]], device='cuda:0') Pred_data:  tensor([[   29,     3,  1223,  2500,   196,  4906,  9974, 13913,     3,   634,\n",
      "          2500,   196,  4906,  9974, 13913,     3]], device='cuda:0')\n",
      "i: 38\n",
      "next word:  उत्तर\n",
      "38 Gen_data:  tensor([[13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3]], device='cuda:0') Pred_data:  tensor([[    3,  1223,  2500,   196,  4906,  9974, 32247,     3,   634,  2500,\n",
      "           196,  4906,  9974, 13913,     3,   634]], device='cuda:0')\n",
      "i: 39\n",
      "next word:  कोरियाली\n",
      "39 Gen_data:  tensor([[    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634]], device='cuda:0') Pred_data:  tensor([[   15,  1042,   196,  4906,  9974, 13913,     3,   634,  2500,   196,\n",
      "          4906,  9974, 13913,     3,   634,  2500]], device='cuda:0')\n",
      "i: 40\n",
      "next word:  नेता\n",
      "40 Gen_data:  tensor([[  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500]], device='cuda:0') Pred_data:  tensor([[  485,   196,  4906,  9974, 13913,     3,   634,  2500,   196,  4906,\n",
      "          9974, 13913,     3,   634,  2500,   196]], device='cuda:0')\n",
      "i: 41\n",
      "next word:  किम\n",
      "41 Gen_data:  tensor([[ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196]], device='cuda:0') Pred_data:  tensor([[  196,  4906,  9974, 13913,     3,   634,  2500,   196,  4906,  9974,\n",
      "         13913,     3,   634,  2500,   196,  4906]], device='cuda:0')\n",
      "i: 42\n",
      "next word:  जोङ\n",
      "42 Gen_data:  tensor([[  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906]], device='cuda:0') Pred_data:  tensor([[    3,  9974, 13913,     3,  3968,  2500,   196,  4906,  9974, 13913,\n",
      "             3,   634,  2500,   196,  4906,  9974]], device='cuda:0')\n",
      "i: 43\n",
      "next word:  उन\n",
      "43 Gen_data:  tensor([[ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974]], device='cuda:0') Pred_data:  tensor([[ 9974, 13913,     3,  3968,  2500,   196,  4906,  9974, 32247,     3,\n",
      "           634,  2500,   196,  4906,  9974, 13913]], device='cuda:0')\n",
      "i: 44\n",
      "next word:  र\n",
      "44 Gen_data:  tensor([[ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913]], device='cuda:0') Pred_data:  tensor([[   29,     3,  1223,  2500,   196,  4906,  9974, 13913,     3,   634,\n",
      "          2500,   196,  4906,  9974, 13913,     3]], device='cuda:0')\n",
      "i: 45\n",
      "next word:  उत्तर\n",
      "45 Gen_data:  tensor([[13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3]], device='cuda:0') Pred_data:  tensor([[    3,  1223,  2500,   196,  4906,  9974, 32247,     3,   634,  2500,\n",
      "           196,  4906,  9974, 13913,     3,   634]], device='cuda:0')\n",
      "i: 46\n",
      "next word:  कोरियाली\n",
      "46 Gen_data:  tensor([[    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634]], device='cuda:0') Pred_data:  tensor([[   15,  1042,   196,  4906,  9974, 13913,     3,   634,  2500,   196,\n",
      "          4906,  9974, 13913,     3,   634,  2500]], device='cuda:0')\n",
      "i: 47\n",
      "next word:  नेता\n",
      "47 Gen_data:  tensor([[  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500]], device='cuda:0') Pred_data:  tensor([[  485,   196,  4906,  9974, 13913,     3,   634,  2500,   196,  4906,\n",
      "          9974, 13913,     3,   634,  2500,   196]], device='cuda:0')\n",
      "i: 48\n",
      "next word:  किम\n",
      "48 Gen_data:  tensor([[ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196]], device='cuda:0') Pred_data:  tensor([[  196,  4906,  9974, 13913,     3,   634,  2500,   196,  4906,  9974,\n",
      "         13913,     3,   634,  2500,   196,  4906]], device='cuda:0')\n",
      "i: 49\n",
      "next word:  जोङ\n",
      "49 Gen_data:  tensor([[  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906],\n",
      "        [ 9974],\n",
      "        [13913],\n",
      "        [    3],\n",
      "        [  634],\n",
      "        [ 2500],\n",
      "        [  196],\n",
      "        [ 4906]], device='cuda:0') Pred_data:  tensor([[    3,  9974, 13913,     3,  3968,  2500,   196,  4906,  9974, 13913,\n",
      "             3,   634,  2500,   196,  4906,  9974]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sample_data[:,-1].unsqueeze(1))\n",
    "print(sample_data)\n",
    "z = generator(best_model, sample_data[:,-1].unsqueeze(1),no_words = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ddefdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य अमेरिका र उत्तर कोरियाली नेता किम जोङ उन र उत्तर कोरियाली नेता किम जोङ उन र उत्तर कोरियाली नेता किम जोङ उन र उत्तर कोरियाली नेता किम जोङ उन र उत्तर कोरियाली नेता किम जोङ उन र उत्तर कोरियाली नेता किम जोङ उन र उत्तर कोरियाली नेता किम जोङ'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त ' +' '.join(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d23f62b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.805072784423828"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(best_model, sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "343b249f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nआधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , \\nसंयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त\\n\\nविकास गर्न प्रोत्साहित गरी सहकारी क्षेत्रले आर्थिक दृष्टिले सक्रिय\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , \n",
    "संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त\n",
    "\n",
    "विकास गर्न प्रोत्साहित गरी सहकारी क्षेत्रले आर्थिक दृष्टिले सक्रिय\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ef802714",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ['गर्ने']\n",
    "st_i = data_process(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "28fe199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_i = st_i.unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0be960e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "next word:  ?\n",
      "0 Gen_data:  tensor([[13]], device='cuda:0') Pred_data:  tensor([[2]], device='cuda:0')\n",
      "i: 1\n",
      "next word:  सबै\n",
      "1 Gen_data:  tensor([[13],\n",
      "        [ 2]], device='cuda:0') Pred_data:  tensor([[ 2, 46]], device='cuda:0')\n",
      "i: 2\n",
      "next word:  ठाउँमा\n",
      "2 Gen_data:  tensor([[13],\n",
      "        [ 2],\n",
      "        [46]], device='cuda:0') Pred_data:  tensor([[  2,  46, 333]], device='cuda:0')\n",
      "i: 3\n",
      "next word:  पहिरो\n",
      "3 Gen_data:  tensor([[ 13],\n",
      "        [  2],\n",
      "        [ 46],\n",
      "        [333]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503]], device='cuda:0')\n",
      "i: 4\n",
      "next word:  जाँदा\n",
      "4 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831]], device='cuda:0')\n",
      "i: 5\n",
      "next word:  पहिरो\n",
      "5 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831, 1503]], device='cuda:0')\n",
      "i: 6\n",
      "next word:  जाने\n",
      "6 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831, 1503,  237]], device='cuda:0')\n",
      "i: 7\n",
      "next word:  र\n",
      "7 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831, 1503,  237,    3]], device='cuda:0')\n",
      "i: 8\n",
      "next word:  पहिरो\n",
      "8 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831, 1503,  237,    3, 1503]],\n",
      "       device='cuda:0')\n",
      "i: 9\n",
      "next word:  जाने\n",
      "9 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831, 1503,  237,    3, 1503,  237]],\n",
      "       device='cuda:0')\n",
      "i: 10\n",
      "next word:  क्रम\n",
      "10 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831, 1503,  237,    3, 1503,  237, 1160]],\n",
      "       device='cuda:0')\n",
      "i: 11\n",
      "next word:  बढेपछि\n",
      "11 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831, 1503,  237,    3, 1503,  237, 1160, 4126]],\n",
      "       device='cuda:0')\n",
      "i: 12\n",
      "next word:  बाटो\n",
      "12 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831, 1503,  237,    3, 1503,  237, 1160, 4126,\n",
      "          435]], device='cuda:0')\n",
      "i: 13\n",
      "next word:  अवरुद्ध\n",
      "13 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831, 1503,  237,    3, 1503,  237, 1160, 4126,\n",
      "          435, 2009]], device='cuda:0')\n",
      "i: 14\n",
      "next word:  भएको\n",
      "14 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831, 1503,  237,    3, 1503,  237, 1160, 4126,\n",
      "          435, 2009,    6]], device='cuda:0')\n",
      "i: 15\n",
      "next word:  छ\n",
      "15 Gen_data:  tensor([[  13],\n",
      "        [   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6]], device='cuda:0') Pred_data:  tensor([[   2,   46,  333, 1503,  831, 1503,  237,    3, 1503,  237, 1160, 4126,\n",
      "          435, 2009,    6,    4]], device='cuda:0')\n",
      "i: 16\n",
      "next word:  ।\n",
      "16 Gen_data:  tensor([[   2],\n",
      "        [  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4]], device='cuda:0') Pred_data:  tensor([[  15,   45, 1503,  831, 1503,  237, 1160, 1503,  237, 1160, 4126,  435,\n",
      "         2009,    6,    4,    1]], device='cuda:0')\n",
      "i: 17\n",
      "next word:  नारायणगढमुग्लिन\n",
      "17 Gen_data:  tensor([[  46],\n",
      "        [ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1]], device='cuda:0') Pred_data:  tensor([[  45, 1503,  831, 1503,  831, 1160, 1503,  237, 1160, 4126, 2009, 2009,\n",
      "            6,    4,    1, 8321]], device='cuda:0')\n",
      "i: 18\n",
      "next word:  सडक\n",
      "18 Gen_data:  tensor([[ 333],\n",
      "        [1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321]], device='cuda:0') Pred_data:  tensor([[   5,  831, 1503,  831, 1160, 1503,  237, 1160, 4126, 2009, 2009,    6,\n",
      "            4,    1, 8321,  189]], device='cuda:0')\n",
      "i: 19\n",
      "next word:  आयोजनाका\n",
      "19 Gen_data:  tensor([[1503],\n",
      "        [ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189]], device='cuda:0') Pred_data:  tensor([[ 831, 1503,  237, 1160, 1503,  237, 1160, 4126, 2009, 2009,    6,    4,\n",
      "            1, 8321, 8565, 2936]], device='cuda:0')\n",
      "i: 20\n",
      "next word:  प्रमुख\n",
      "20 Gen_data:  tensor([[ 831],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936]], device='cuda:0') Pred_data:  tensor([[   5,  237, 1160, 1503,  237, 1160, 4126,  435, 2009,    6,    4,    1,\n",
      "         8321, 8565, 2936,   79]], device='cuda:0')\n",
      "i: 21\n",
      "next word:  एवं\n",
      "21 Gen_data:  tensor([[1503],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79]], device='cuda:0') Pred_data:  tensor([[ 831, 1160, 1503,  237, 1160, 4126, 2009, 2009,    6,    4,    1, 8321,\n",
      "         8565, 2936,   79,  268]], device='cuda:0')\n",
      "i: 22\n",
      "next word:  प्रवक्ता\n",
      "22 Gen_data:  tensor([[ 237],\n",
      "        [   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268]], device='cuda:0') Pred_data:  tensor([[   2,   46,  237, 1160, 4126,  435,  336,    6,    4,    1, 8321, 8668,\n",
      "         2936,   79,  268,  788]], device='cuda:0')\n",
      "i: 23\n",
      "next word:  सुरेश\n",
      "23 Gen_data:  tensor([[   3],\n",
      "        [1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788]], device='cuda:0') Pred_data:  tensor([[  15,  831, 1160, 1373,   15,  336,    6,    4,    1, 8321, 8668, 2936,\n",
      "           79,  268,  788, 4567]], device='cuda:0')\n",
      "i: 24\n",
      "next word:  अधिकारीले\n",
      "24 Gen_data:  tensor([[1503],\n",
      "        [ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567]], device='cuda:0') Pred_data:  tensor([[ 831, 1160, 1373,   15,  336,    6,    4,    1, 8321, 8668, 2936,   79,\n",
      "          268,  788, 4567,  849]], device='cuda:0')\n",
      "i: 25\n",
      "next word:  भने\n",
      "25 Gen_data:  tensor([[ 237],\n",
      "        [1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849]], device='cuda:0') Pred_data:  tensor([[   2,  325,   15,  336,    6,    4,    1,   15, 8668, 2936,   79, 2045,\n",
      "          788, 4567,  849,   12]], device='cuda:0')\n",
      "i: 26\n",
      "next word:  ?\n",
      "26 Gen_data:  tensor([[1160],\n",
      "        [4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12]], device='cuda:0') Pred_data:  tensor([[ 325,   15,  336,    6,    4,    1,   15,  189, 2936,   79, 2045,  788,\n",
      "         4567, 4024,   12,    2]], device='cuda:0')\n",
      "i: 27\n",
      "next word:  अब\n",
      "27 Gen_data:  tensor([[4126],\n",
      "        [ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12],\n",
      "        [   2]], device='cuda:0') Pred_data:  tensor([[  15,  336,    6,    4,    1,   15, 8668, 2936,   79, 2045,  788, 4567,\n",
      "         4024,   12,    2,  102]], device='cuda:0')\n",
      "i: 28\n",
      "next word:  काम\n",
      "28 Gen_data:  tensor([[ 435],\n",
      "        [2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12],\n",
      "        [   2],\n",
      "        [ 102]], device='cuda:0') Pred_data:  tensor([[   8,    6,    4,    1,   15, 8668, 2936,    9,  268,  788, 4567, 4024,\n",
      "           12,    2,  143,   26]], device='cuda:0')\n",
      "i: 29\n",
      "next word:  अघि\n",
      "29 Gen_data:  tensor([[2009],\n",
      "        [   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12],\n",
      "        [   2],\n",
      "        [ 102],\n",
      "        [  26]], device='cuda:0') Pred_data:  tensor([[ 189,    4,    1,   15, 8668, 2936,    9,  268,  788, 3580, 4024,   12,\n",
      "            2,  143,   26,  136]], device='cuda:0')\n",
      "i: 30\n",
      "next word:  बढेको\n",
      "30 Gen_data:  tensor([[   6],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12],\n",
      "        [   2],\n",
      "        [ 102],\n",
      "        [  26],\n",
      "        [ 136]], device='cuda:0') Pred_data:  tensor([[   4,    1,   15,  189, 2936,    9,  268,  788, 2045,  849,   12,    2,\n",
      "          102,   26,  136,  325]], device='cuda:0')\n",
      "i: 31\n",
      "next word:  छ\n",
      "31 Gen_data:  tensor([[   4],\n",
      "        [   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12],\n",
      "        [   2],\n",
      "        [ 102],\n",
      "        [  26],\n",
      "        [ 136],\n",
      "        [ 325]], device='cuda:0') Pred_data:  tensor([[   1,   15,  189, 2936,    9,  268,  788, 4567,  849,   12,    2,  143,\n",
      "           26,  136,  325,    4]], device='cuda:0')\n",
      "i: 32\n",
      "next word:  ।\n",
      "32 Gen_data:  tensor([[   1],\n",
      "        [8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12],\n",
      "        [   2],\n",
      "        [ 102],\n",
      "        [  26],\n",
      "        [ 136],\n",
      "        [ 325],\n",
      "        [   4]], device='cuda:0') Pred_data:  tensor([[  19,  189, 2936,    9,  268,  788, 4567,  849,   12,    2,  143,   26,\n",
      "          136,  325,    4,    1]], device='cuda:0')\n",
      "i: 33\n",
      "next word:  यो\n",
      "33 Gen_data:  tensor([[8321],\n",
      "        [ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12],\n",
      "        [   2],\n",
      "        [ 102],\n",
      "        [  26],\n",
      "        [ 136],\n",
      "        [ 325],\n",
      "        [   4],\n",
      "        [   1]], device='cuda:0') Pred_data:  tensor([[ 189, 2936,    9, 3544,  788,  207,  482,   65,    2,  143,   26,  136,\n",
      "          325,    4,    1,   15]], device='cuda:0')\n",
      "i: 34\n",
      "next word:  काम\n",
      "34 Gen_data:  tensor([[ 189],\n",
      "        [2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12],\n",
      "        [   2],\n",
      "        [ 102],\n",
      "        [  26],\n",
      "        [ 136],\n",
      "        [ 325],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  15]], device='cuda:0') Pred_data:  tensor([[ 336,    9, 3301,  788,  207, 4024,   65,    2,  143,   26,  136,  325,\n",
      "            4,    1,   15,   26]], device='cuda:0')\n",
      "i: 35\n",
      "next word:  हामीले\n",
      "35 Gen_data:  tensor([[2936],\n",
      "        [  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12],\n",
      "        [   2],\n",
      "        [ 102],\n",
      "        [  26],\n",
      "        [ 136],\n",
      "        [ 325],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  15],\n",
      "        [  26]], device='cuda:0') Pred_data:  tensor([[   9,  741,  788,  207, 4024,   65,    2,  143,   26,  136,  325,    4,\n",
      "            1,   19,   26,  143]], device='cuda:0')\n",
      "i: 36\n",
      "next word:  गर्न\n",
      "36 Gen_data:  tensor([[  79],\n",
      "        [ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12],\n",
      "        [   2],\n",
      "        [ 102],\n",
      "        [  26],\n",
      "        [ 136],\n",
      "        [ 325],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  15],\n",
      "        [  26],\n",
      "        [ 143]], device='cuda:0') Pred_data:  tensor([[  74,   74,  207, 4024,   65,    2,  143,  143,  136,  325,    4,    1,\n",
      "           70,   26,  143,   10]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 37\n",
      "next word:  सकेनौं\n",
      "37 Gen_data:  tensor([[ 268],\n",
      "        [ 788],\n",
      "        [4567],\n",
      "        [ 849],\n",
      "        [  12],\n",
      "        [   2],\n",
      "        [ 102],\n",
      "        [  26],\n",
      "        [ 136],\n",
      "        [ 325],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  15],\n",
      "        [  26],\n",
      "        [ 143],\n",
      "        [  10]], device='cuda:0') Pred_data:  tensor([[  100, 17643,  4024,    65,     2,   143,    15,   136,  2866,     4,\n",
      "             1,    19,    26,   143,    10, 10039]], device='cuda:0')\n",
      "i: 38\n",
      "next word:  ।\n",
      "38 Gen_data:  tensor([[  788],\n",
      "        [ 4567],\n",
      "        [  849],\n",
      "        [   12],\n",
      "        [    2],\n",
      "        [  102],\n",
      "        [   26],\n",
      "        [  136],\n",
      "        [  325],\n",
      "        [    4],\n",
      "        [    1],\n",
      "        [   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039]], device='cuda:0') Pred_data:  tensor([[17643,  4024,    65,     2,   143,    15,   136,  2866,     4,     1,\n",
      "            19,    26,   143,    10, 10039,     1]], device='cuda:0')\n",
      "i: 39\n",
      "next word:  तर\n",
      "39 Gen_data:  tensor([[ 4567],\n",
      "        [  849],\n",
      "        [   12],\n",
      "        [    2],\n",
      "        [  102],\n",
      "        [   26],\n",
      "        [  136],\n",
      "        [  325],\n",
      "        [    4],\n",
      "        [    1],\n",
      "        [   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1]], device='cuda:0') Pred_data:  tensor([[ 1816,    65,     2,   143,    15,   136,  2866,     4,     1,    19,\n",
      "            26,   143,    10, 10039,     1,    19]], device='cuda:0')\n",
      "i: 40\n",
      "next word:  ?\n",
      "40 Gen_data:  tensor([[  849],\n",
      "        [   12],\n",
      "        [    2],\n",
      "        [  102],\n",
      "        [   26],\n",
      "        [  136],\n",
      "        [  325],\n",
      "        [    4],\n",
      "        [    1],\n",
      "        [   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [   19]], device='cuda:0') Pred_data:  tensor([[   12,     2,   143,    15,   136,  2866,     4,     1,    19,    26,\n",
      "           143,    10, 10039,     1,    19,     2]], device='cuda:0')\n",
      "i: 41\n",
      "next word:  हामीले\n",
      "41 Gen_data:  tensor([[   12],\n",
      "        [    2],\n",
      "        [  102],\n",
      "        [   26],\n",
      "        [  136],\n",
      "        [  325],\n",
      "        [    4],\n",
      "        [    1],\n",
      "        [   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [   19],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,    15,    15,   136,  2866,     4,     1,    19,    26,   143,\n",
      "            10, 10039,     1,    19,     2,   143]], device='cuda:0')\n",
      "i: 42\n",
      "next word:  काम\n",
      "42 Gen_data:  tensor([[    2],\n",
      "        [  102],\n",
      "        [   26],\n",
      "        [  136],\n",
      "        [  325],\n",
      "        [    4],\n",
      "        [    1],\n",
      "        [   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [   19],\n",
      "        [    2],\n",
      "        [  143]], device='cuda:0') Pred_data:  tensor([[   15,    38,    13,  2866,     4,     1,    19,    26,   143,    10,\n",
      "         10039,     1,    19,     2,   143,    26]], device='cuda:0')\n",
      "i: 43\n",
      "next word:  गर्न\n",
      "43 Gen_data:  tensor([[  102],\n",
      "        [   26],\n",
      "        [  136],\n",
      "        [  325],\n",
      "        [    4],\n",
      "        [    1],\n",
      "        [   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [   19],\n",
      "        [    2],\n",
      "        [  143],\n",
      "        [   26]], device='cuda:0') Pred_data:  tensor([[   15,    13,  2866,     4,     1,    19,    26,   143,    10, 10039,\n",
      "             1,    19,     2,   143,    26,    10]], device='cuda:0')\n",
      "i: 44\n",
      "next word:  सकेनौं\n",
      "44 Gen_data:  tensor([[   26],\n",
      "        [  136],\n",
      "        [  325],\n",
      "        [    4],\n",
      "        [    1],\n",
      "        [   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [   19],\n",
      "        [    2],\n",
      "        [  143],\n",
      "        [   26],\n",
      "        [   10]], device='cuda:0') Pred_data:  tensor([[   10,   325,     4,     1,    19,    77,   143,    10, 10039,     1,\n",
      "            19,     2,   143,   111,    10, 10039]], device='cuda:0')\n",
      "i: 45\n",
      "next word:  ।\n",
      "45 Gen_data:  tensor([[  136],\n",
      "        [  325],\n",
      "        [    4],\n",
      "        [    1],\n",
      "        [   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [   19],\n",
      "        [    2],\n",
      "        [  143],\n",
      "        [   26],\n",
      "        [   10],\n",
      "        [10039]], device='cuda:0') Pred_data:  tensor([[  325,     4,     1,    15,   106,   143,    10, 10039,     1,    19,\n",
      "             2,   143,   111,    10, 10039,     1]], device='cuda:0')\n",
      "i: 46\n",
      "next word:  हामीले\n",
      "46 Gen_data:  tensor([[  325],\n",
      "        [    4],\n",
      "        [    1],\n",
      "        [   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [   19],\n",
      "        [    2],\n",
      "        [  143],\n",
      "        [   26],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1]], device='cuda:0') Pred_data:  tensor([[    4,     1,    15,   164,   143,    10, 10039,     1,    19,     2,\n",
      "           143,    51,    10, 10039,     1,   143]], device='cuda:0')\n",
      "i: 47\n",
      "next word:  हाम्रो\n",
      "47 Gen_data:  tensor([[    4],\n",
      "        [    1],\n",
      "        [   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [   19],\n",
      "        [    2],\n",
      "        [  143],\n",
      "        [   26],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [  143]], device='cuda:0') Pred_data:  tensor([[    1,    15,    45,   143,    10, 10039,     1,    19,     2,   143,\n",
      "            51,    10, 10039,     1,   143,   111]], device='cuda:0')\n",
      "i: 48\n",
      "next word:  शिक्षा\n",
      "48 Gen_data:  tensor([[    1],\n",
      "        [   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [   19],\n",
      "        [    2],\n",
      "        [  143],\n",
      "        [   26],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [  143],\n",
      "        [  111]], device='cuda:0') Pred_data:  tensor([[   19,    45,   143,    10,  3576,     1,    19,     2,   143,    51,\n",
      "            10, 10039,     1,   143,   111,   222]], device='cuda:0')\n",
      "i: 49\n",
      "next word:  ?\n",
      "49 Gen_data:  tensor([[   15],\n",
      "        [   26],\n",
      "        [  143],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [   19],\n",
      "        [    2],\n",
      "        [  143],\n",
      "        [   26],\n",
      "        [   10],\n",
      "        [10039],\n",
      "        [    1],\n",
      "        [  143],\n",
      "        [  111],\n",
      "        [  222]], device='cuda:0') Pred_data:  tensor([[   45,     5,    10,  3576,     1,    19,     2,   143,    51,    10,\n",
      "         10039,     1,   143,   111,   222,     2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "z_ = generator(best_model, st_i,no_words =50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "945c8b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? सबै ठाउँमा पहिरो जाँदा पहिरो जाने र पहिरो जाने क्रम बढेपछि बाटो अवरुद्ध भएको छ । नारायणगढमुग्लिन सडक आयोजनाका प्रमुख एवं प्रवक्ता सुरेश अधिकारीले भने ? अब काम अघि बढेको छ । यो काम हामीले गर्न सकेनौं । तर ? हामीले काम गर्न सकेनौं । हामीले हाम्रो शिक्षा ?'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(z_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a4a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
