{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2epAl_2WFoVG",
    "outputId": "2c9ef53b-120d-40b3-9d68-ca771059472d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V6XQUNmFFsUG"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7XksTER7vls"
   },
   "outputs": [],
   "source": [
    "class preprocessText:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  \n",
    "  def count_words(self, tokenized_sentences):\n",
    "    \"\"\"\n",
    "    Count the number of word appearances in the tokenized sentences\n",
    "\n",
    "    Input :\n",
    "      tokenized_sentences : list of tokenized sentences\n",
    "    \n",
    "    Output : \n",
    "      word_counts : dictionary with key = word and value = frequency of word in the list of tokenized sentences\n",
    "    \"\"\"\n",
    "\n",
    "    word_counts = {}\n",
    "    for sentence in tokenized_sentences:\n",
    "      for token in sentence:\n",
    "        if token not in word_counts.keys():\n",
    "          word_counts[token] = 1\n",
    "        else:\n",
    "          word_counts[token] += 1\n",
    "\n",
    "    return word_counts\n",
    "  \n",
    "  def get_words_with_nplus_frequency(self, tokenized_sentences, count_threshold):\n",
    "    \"\"\"\n",
    "    Create the vocabulary such that the words are of certain minimum frequency in the training dataset\n",
    "    \n",
    "    Input :\n",
    "      tokenized_sentences : list of tokenized sentences\n",
    "      count_threshold : minimum frequency for a word to be added in vocabulary\n",
    "    \n",
    "    Ouput : \n",
    "      closed_vocab : list of words that has more that the minimum frequency\n",
    "    \"\"\"\n",
    "    closed_vocab = []\n",
    "\n",
    "    word_counts = self.count_words(tokenized_sentences)\n",
    "\n",
    "    for word, cnt in word_counts.items():\n",
    "      if cnt >= count_threshold:\n",
    "        closed_vocab.append(word)\n",
    "    \n",
    "    return closed_vocab\n",
    "\n",
    "  def replace_oov_words_by_unk(self, tokenized_sentences, vocabulary, unknown_token = \"<unk>\"):\n",
    "    \"\"\"\n",
    "    Replaced all the words in tokenized_sentences not in vocabulary by the unknown_token\n",
    "\n",
    "    Input :\n",
    "      tokenized_sentences : list of tokenized sentences\n",
    "      vocabulary : list of words => output from get_words_with_nplus_frequency()\n",
    "      unknown_token : symbol to replace the words absent in vocabulary\n",
    "    \n",
    "    Output : \n",
    "      replaced_tokenized_sentences :  tokenized_sentences with words absent in vocabulary replaced by the \"unknown_token\"\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary = set(vocabulary)\n",
    "\n",
    "    replaced_tokenized_sentences = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "      replaced_sentence = []\n",
    "\n",
    "      for token in sentence:\n",
    "        if token in vocabulary:\n",
    "          replaced_sentence.append(token)\n",
    "        else:\n",
    "          replaced_sentence.append(unknown_token)\n",
    "      replaced_tokenized_sentences.append(replaced_sentence)\n",
    "    \n",
    "    return replaced_tokenized_sentences\n",
    "  \n",
    "  def preprocess_data(self, train_data, test_data, count_threshold, unknown_token = \"<unk>\"):\n",
    "    \"\"\"\n",
    "    Preprocess the training and test data by replacing the words not in vocabulary by unknown_token\n",
    "\n",
    "    Input : \n",
    "      train_data : list of tokenized sentences of train_data\n",
    "      test_data : list of tokenized sentences of test_data\n",
    "      count_threshold : minimum frequency for a word to be added in vocabulary\n",
    "    \n",
    "    Output :\n",
    "      train_data_replaced : preprocessed training data\n",
    "      test_data_replaced : preprocessed testing data\n",
    "      vocabulary : list of words => output from get_words_with_nplus_frequency()    \n",
    "    \"\"\"\n",
    "    vocabulary = self.get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "\n",
    "    train_data_replaced = self.replace_oov_words_by_unk(train_data, vocabulary, unknown_token = unknown_token)\n",
    "\n",
    "    test_data_replaced = self.replace_oov_words_by_unk(test_data, vocabulary, unknown_token = unknown_token)\n",
    "\n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtXTRWoFFQHj"
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = joblib.load(\"/content/drive/MyDrive/Colab Notebooks/NLP/Language Modelling/Datasets/Nepali_Corpus/tokenized_sentences.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RQSa8o5Fl0A"
   },
   "outputs": [],
   "source": [
    "tokenized_data = tokenized_sentences\n",
    "random.seed(101)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.9)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXX30Vp-F4hG",
    "outputId": "4884f4bb-97d3-4ac4-b1c9-7075bba161f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198000 are split into 178200 train and 19800 test set\n",
      "First Training sample: \n",
      "['ऐलानी', 'जग्गामा', 'बस्ने', 'उनीहरूसँग', 'लालपुर्जा', 'छैन']\n",
      "First Test Sample: \n",
      "['ना', 'ख', 'नम्बरको', 'बाह्र', 'चक्के', 'ट्रकले', 'पैदल', 'हिँड्दै', 'गरेका', 'सोही', 'ठाउँमा', 'वर्षीय', 'चन्द्रमान', 'श्रेष्ठलाई', 'ठक्कर', 'दिँदा', 'उनको', 'घटनास्थलमै', 'ज्यान', 'गएको', 'ट्राफिक', 'प्रमुख', 'दयाराम', 'पौडेलले', 'बताए']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(tokenized_data)} are split into {len(train_data)} train and {len(test_data)} test set\")\n",
    "\n",
    "print(\"First Training sample: \")\n",
    "print(train_data[0])\n",
    "print(\"First Test Sample: \")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_t4r8DlsGAMH"
   },
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "preProcessModel = preprocessText()\n",
    "train_data_processed, test_data_processed, vocabulary = preProcessModel.preprocess_data(train_data, test_data, minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uko0uzyLGPTn",
    "outputId": "9176e1df-14c5-44bb-f0d4-5b6d91998a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample:\n",
      "Original Sentence :  ['ऐलानी', 'जग्गामा', 'बस्ने', 'उनीहरूसँग', 'लालपुर्जा', 'छैन']\n",
      "['ऐलानी', 'जग्गामा', 'बस्ने', 'उनीहरूसँग', 'लालपुर्जा', 'छैन']\n",
      "\n",
      "First preprocessed test sample:\n",
      "Original Sentence :  ['ना', 'ख', 'नम्बरको', 'बाह्र', 'चक्के', 'ट्रकले', 'पैदल', 'हिँड्दै', 'गरेका', 'सोही', 'ठाउँमा', 'वर्षीय', 'चन्द्रमान', 'श्रेष्ठलाई', 'ठक्कर', 'दिँदा', 'उनको', 'घटनास्थलमै', 'ज्यान', 'गएको', 'ट्राफिक', 'प्रमुख', 'दयाराम', 'पौडेलले', 'बताए']\n",
      "['ना', 'ख', 'नम्बरको', 'बाह्र', '<unk>', 'ट्रकले', 'पैदल', 'हिँड्दै', 'गरेका', 'सोही', 'ठाउँमा', 'वर्षीय', 'चन्द्रमान', 'श्रेष्ठलाई', 'ठक्कर', 'दिँदा', 'उनको', 'घटनास्थलमै', 'ज्यान', 'गएको', 'ट्राफिक', 'प्रमुख', 'दयाराम', 'पौडेलले', 'बताए']\n",
      "\n",
      "First 10 vocabulary:\n",
      "['ऐलानी', 'जग्गामा', 'बस्ने', 'उनीहरूसँग', 'लालपुर्जा', 'छैन', 'धेरै', 'कारोबार', 'कार्डबाट', 'हुने']\n",
      "\n",
      "Size of vocabulary: 80552\n"
     ]
    }
   ],
   "source": [
    "print(\"First preprocessed training sample:\")\n",
    "print(\"Original Sentence : \", train_data[0])\n",
    "print(train_data_processed[0])\n",
    "print()\n",
    "print(\"First preprocessed test sample:\")\n",
    "print(\"Original Sentence : \", test_data[0])\n",
    "print(test_data_processed[0])\n",
    "print()\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(vocabulary[0:10])\n",
    "print()\n",
    "print(\"Size of vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "jrDPrEwg2Flt"
   },
   "outputs": [],
   "source": [
    "class nGramLangugageModel:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  \n",
    "  def count_n_grams(self, data, n, start_token = \"<s>\", end_token = \"</s>\"):\n",
    "    \"\"\"\n",
    "    Count all the n-grams of the data\n",
    "\n",
    "    Input : \n",
    "      data = list of tokenized sentences\n",
    "      n = 1 for unigram, 2 for bigram and 3 for trigram and so on......\n",
    "\n",
    "    Output :\n",
    "      n_grams : dictionary with key of n-gram and value of the count of the corresponding n-gram\n",
    "    \"\"\"\n",
    "\n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in data:\n",
    "      # prepend start token n times and append end token one time\n",
    "      sentence = [start_token] * n + sentence + [end_token]\n",
    "\n",
    "      # convert list to tuple so that the sequence of words can be used as a key in the dictionary\n",
    "      sentence = tuple(sentence)\n",
    "\n",
    "      # use value of m to denote the number of n grams in the current sentence\n",
    "      m = len(sentence) if n==1 else len(sentence) - 1\n",
    "\n",
    "      for i in range(m):\n",
    "        n_gram = sentence[i: i + n]\n",
    "\n",
    "        # check if the n-gram is in the dictionary\n",
    "        # if present, increase the value else se the value of n-gram to 1\n",
    "        if n_gram in n_grams.keys():\n",
    "          n_grams[n_gram] += 1\n",
    "        else:\n",
    "          n_grams[n_gram] = 1\n",
    "\n",
    "    return n_grams\n",
    "\n",
    "  def estimate_probability(self, word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k = 1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of the next word using the n-gram counts with k-smoothing\n",
    "\n",
    "    Input : \n",
    "      word : next word to be predicted\n",
    "      previous_n_gram : given input words by the user of n-gram\n",
    "      n_gram_counts : output of the count_n_gram functions for n-gram\n",
    "      n_plus1_gram_counts : output of the count_n_gram functions for (n+1)-gram\n",
    "      vocabulary_size : length of the vocabulary\n",
    "      k : constant for k-smoothing\n",
    "    \n",
    "    Output:\n",
    "      probability : probability that the \"word\" appear after the \"previous_n_gram\"\n",
    "    \"\"\"\n",
    "\n",
    "    # convert the list to tuple to use it as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "    # set the denominator\n",
    "    # if the previous n-gram exists in the dictionary of n-gram counts, get its coun\n",
    "    # else set the count to zero\n",
    "    # use the dictionary that has counts for n-grams\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "\n",
    "    # calculate the denominator useing the count fo the previous n gram and apply k-smoothing\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "    # define n plus 1 gram as the previous n-gram plus the current word as a tuple\n",
    "    n_plus1_gram = previous_n_gram + (word, )\n",
    "\n",
    "    # set the count to the count in the dictionary\n",
    "    # 0 if not in the dictionary\n",
    "    # use the dictionary that has counts for the n-gram plus current word\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n",
    "\n",
    "    # define the numerator using the counf of the n-gram plus current word and apply smoothing\n",
    "    numerator = n_plus1_gram_count + k\n",
    "\n",
    "    # calculate the probability as the numberator divided by the denominator\n",
    "    probability = numerator / denominator\n",
    "\n",
    "    return probability\n",
    "\n",
    "  def estimate_probabilities(self, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k = 1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n",
    "\n",
    "    Input : \n",
    "      previous_n_gram : given input words by the user of n-gram\n",
    "      n_gram_counts : output of the count_n_gram functions for n-gram\n",
    "      n_plus1_gram_counts : output of the count_n_gram functions for (n+1)-gram\n",
    "      vocabulary : list of unique words in the training datasets\n",
    "      k : constant for k-smoothing\n",
    "    \n",
    "    Output:\n",
    "      probabilities : dictionary of probability that the \"word\" in \"vocabulary\" appear after the \"previous_n_gram\"\n",
    "    \"\"\"\n",
    "    # convert the list to tuple to use it as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "    # add </s> and <unk> to the vocabulary\n",
    "    # <s> is not needed as it should not appear as the next word\n",
    "    vocabulary = vocabulary + [\"</s>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "      probability = self.estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k = k)\n",
    "      probabilities[word] = probability\n",
    "    \n",
    "    return probabilities\n",
    "  \n",
    "  def suggest_a_word(self, previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k = 1.0, start_with = None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "\n",
    "    Input : \n",
    "      previous_n_gram : given input words by the user of n-gram\n",
    "      n_gram_counts : output of the count_n_gram functions for n-gram\n",
    "      n_plus1_gram_counts : output of the count_n_gram functions for (n+1)-gram\n",
    "      vocabulary : list of unique words in the training datasets\n",
    "      k : constant for k-smoothing\n",
    "      start_with : starting letters of the word to be suggested\n",
    "\n",
    "    Output :\n",
    "      suggestion : word in vocabulary with the highest probability\n",
    "      max_prob : corresponding probabilirt of the suggested word after the given n-gram\n",
    "    \"\"\"\n",
    "\n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "\n",
    "    # From the words that the user already typed, get the most recent 'n' words as the previous n-gram\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "\n",
    "    # Estimate the probabilities that each word in the vocabular is the next word\n",
    "    probabilities = self.estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k = k)\n",
    "    \n",
    "    # Words with highest probability will be set to suggestion\n",
    "    suggestion = None\n",
    "\n",
    "    # Initialie the value for maximum probability\n",
    "    max_prob = 0\n",
    "\n",
    "    # For each word and its probability in the probabilities dictionary\n",
    "    for word, prob in probabilities.items():\n",
    "      # if the optional start with string is set\n",
    "      if start_with != None:\n",
    "        # Check if the beginning of word does not match with the letters in 'start_with'\n",
    "        if not word.startswith(start_with):\n",
    "          # if they don't match, skip this word and move onto the next word\n",
    "          continue\n",
    "\n",
    "      # Check if this word's probability is greater than the current maximum probability\n",
    "      if prob > max_prob:\n",
    "        # if so, save this word for the best suggestion\n",
    "        suggestion = word\n",
    "        max_prob = prob\n",
    "    \n",
    "    return suggestion, max_prob\n",
    "\n",
    "  def suggest_words(self, previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k = 1.0, start_with = None, no_suggestions = 3):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "\n",
    "    Input : \n",
    "      previous_n_gram : given input words by the user of n-gram\n",
    "      n_gram_counts : output of the count_n_gram functions for n-gram\n",
    "      n_plus1_gram_counts : output of the count_n_gram functions for (n+1)-gram\n",
    "      vocabulary : list of unique words in the training datasets\n",
    "      k : constant for k-smoothing\n",
    "      start_with : starting letters of the word to be suggested\n",
    "      no_suggestions : No. of suggestions provided for the next word\n",
    "\n",
    "    Output :\n",
    "      suggestions : dictionary of (no_suggestions) of \"word\" in \"vocabulary\" with the highest probability\n",
    "    \"\"\"\n",
    "\n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "\n",
    "    # From the words that the user already typed, get the most recent 'n' words as the previous n-gram\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "\n",
    "    # Estimate the probabilities that each word in the vocabular is the next word\n",
    "    probabilities = self.estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k = k)\n",
    "    \n",
    "    # Words with highest probability will be set to suggestion\n",
    "    suggestions = {}\n",
    "\n",
    "    # For each word and its probability in the probabilities dictionary\n",
    "    for word, prob in probabilities.items():\n",
    "      # if the optional start with string is set\n",
    "      if start_with != None:\n",
    "        # Check if the beginning of word does not match with the letters in 'start_with'\n",
    "        if not word.startswith(start_with):\n",
    "          # if they don't match, skip this word and move onto the next word\n",
    "          continue\n",
    "\n",
    "      if len(suggestions) < no_suggestions:\n",
    "        suggestions[word] = prob\n",
    "      else:\n",
    "        # find the suggestions (key, value) with the smallest probability\n",
    "        suggestions_list = list(suggestions.items())\n",
    "        suggest_key = suggestions_list[0][0]\n",
    "        suggest_prob = suggestions_list[0][1]\n",
    "        for i in range(1, no_suggestions):\n",
    "          if suggest_prob > suggestions_list[i][1]:\n",
    "            suggest_key = suggestions_list[i][0]\n",
    "            suggest_prob = suggestions_list[i][1]\n",
    "        # replace if the smallest probability is smaller than the probability of the current word\n",
    "        if(suggest_prob < prob):\n",
    "          suggestions.pop(suggest_key)\n",
    "          suggestions[word] = prob\n",
    "    \n",
    "    # sort the suggestions in descending order\n",
    "    suggestions = sorted(suggestions.items(), key = lambda x:x[1], reverse = True)\n",
    "\n",
    "    return suggestions\n",
    "  \n",
    "  def display_suggestions(self, previous_tokens, vocabulary, n_gram_counts_list, n_gram_index, k = 1.0, start_with = None, no_suggestions = 3):\n",
    "    \"\"\"\n",
    "    Display suggestion for the next word\n",
    "\n",
    "    Input : \n",
    "      previous_tokens : given input words by the user\n",
    "      vocabulary : list of unique words in the training datasets\n",
    "      n_gram_counts_list : list of output of the count_n_gram functions for n-gram where n = 1, 2, 3, 4, 5\n",
    "      n_gram_index : index for the n-gram, where = 1 for unigram, = 2 for bigram and so on.... \n",
    "      k : constant for k-smoothing\n",
    "      start_with : starting letters of the word to be suggested\n",
    "      no_suggestions : No. of suggestions provided for the next word\n",
    "\n",
    "    Output :\n",
    "      Display the suggested words\n",
    "    \"\"\"\n",
    "    print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "\n",
    "    for index in n_gram_index:\n",
    "      tmp_suggest = self.suggest_words(previous_tokens, n_gram_counts_list[index], n_gram_counts_list[index + 1], vocabulary, k = k, start_with = start_with, no_suggestions = no_suggestions)\n",
    "      print(f'n-gram-index = {index}')\n",
    "      print(\"=========================\")\n",
    "      display(tmp_suggest)\n",
    "      print(\"=========================\")\n",
    "\n",
    "  def return_suggestions(self, previous_tokens, vocabulary, n_gram_counts_list, n_gram_index, k = 1.0, start_with = None, no_suggestions = 3):\n",
    "    \"\"\"\n",
    "    Display suggestion for the next word\n",
    "\n",
    "    Input :\n",
    "      previous_tokens : given input words by the user\n",
    "      vocabulary : list of unique words in the training datasets\n",
    "      n_gram_counts_list : list of output of the count_n_gram functions for n-gram where n = 1, 2, 3, 4, 5\n",
    "      n_gram_index : index for the n-gram, where = 1 for unigram, = 2 for bigram and so on....\n",
    "      k : constant for k-smoothing\n",
    "      start_with : starting letters of the word to be suggested\n",
    "      no_suggestions : No. of suggestions provided for the next word\n",
    "\n",
    "    Output :\n",
    "      suggestions : no_suggestions * 2 suggest words list\n",
    "    \"\"\"\n",
    "    suggestions = []\n",
    "    for index in n_gram_index:\n",
    "      tmp_suggest = self.suggest_words(previous_tokens, n_gram_counts_list[index], n_gram_counts_list[index + 1], vocabulary, k = k, start_with = start_with, no_suggestions = no_suggestions)\n",
    "      for suggest in tmp_suggest:\n",
    "        suggestions.append(suggest)\n",
    "    \n",
    "    suggestions.sort(key = lambda x : x[1], reverse = True)\n",
    "\n",
    "    return suggestions[0:no_suggestions - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "_FggU5Bk3rIA"
   },
   "outputs": [],
   "source": [
    "vocabulary = joblib.load('/content/drive/MyDrive/Colab Notebooks/NLP/Language Modelling/Datasets/Nepali_Corpus/vocabulary.pkl')\n",
    "n_gram_counts_list = joblib.load('/content/drive/MyDrive/Colab Notebooks/NLP/Language Modelling/Datasets/Nepali_Corpus/n_gram_counts_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kq1aVRJUA34M",
    "outputId": "501a58e6-d5d7-4ecb-cf75-e22ea75d1fc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_gram_counts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "TNf3fAonDvjy"
   },
   "outputs": [],
   "source": [
    "model = nGramLangugageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m08AZ5Q26cKw",
    "outputId": "b7ce83e5-9909-4314-849b-05e4ad925558"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('जलस्रोत', 0.000371089642888067),\n",
       " ('र', 0.00016080551191816238),\n",
       " ('क्षेत्रमा', 0.00016080551191816238),\n",
       " ('संकट', 0.00013606620239229122)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_tokens = ['ऊर्जा']\n",
    "model.return_suggestions(previous_tokens, vocabulary, n_gram_counts_list, n_gram_index = [0, 1], no_suggestions = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "jYa4ItfXBZ5B",
    "outputId": "4793ffcc-d93a-4123-a9a2-a5344403168b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['त्यो', 'त'], the suggestions are:\n",
      "n-gram-index = 0\n",
      "=========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('</s>', 0.0063799731369552125),\n",
       " ('<unk>', 0.0032768283080913344),\n",
       " ('होइन', 0.0009378907878282618),\n",
       " ('यो', 0.0008105229030614608),\n",
       " ('के', 0.0008105229030614608)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "n-gram-index = 1\n",
      "=========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<unk>', 7.44130669345537e-05),\n",
       " ('एउटा', 4.9608711289702475e-05),\n",
       " ('सबै', 3.720653346727685e-05),\n",
       " ('झन्', 3.720653346727685e-05),\n",
       " ('थाहा', 3.720653346727685e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n"
     ]
    }
   ],
   "source": [
    "previous_tokens = ['त्यो', 'त']\n",
    "model.display_suggestions(previous_tokens, vocabulary, n_gram_counts_list, n_gram_index = [0, 1], no_suggestions = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "Qpn06k-HD3l9",
    "outputId": "06c1bf4f-be12-4477-b04c-f77a59471cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['ऊर्जा'], the suggestions are:\n",
      "n-gram-index = 0\n",
      "=========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('जलस्रोत', 0.000371089642888067),\n",
       " ('र', 0.00016080551191816238),\n",
       " ('क्षेत्रमा', 0.00016080551191816238),\n",
       " ('संकट', 0.00013606620239229122),\n",
       " ('<unk>', 9.895723810348453e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "n-gram-index = 1\n",
      "=========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('</s>', 2.4828065645405568e-05),\n",
       " ('जग्गामा', 1.2414032822702784e-05),\n",
       " ('बस्ने', 1.2414032822702784e-05),\n",
       " ('उनीहरूसँग', 1.2414032822702784e-05),\n",
       " ('लालपुर्जा', 1.2414032822702784e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n"
     ]
    }
   ],
   "source": [
    "previous_tokens = ['ऊर्जा']\n",
    "model.display_suggestions(previous_tokens, vocabulary, n_gram_counts_list, n_gram_index = [0, 1], no_suggestions = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmAJkcxbFF0b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
